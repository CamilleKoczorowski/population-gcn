{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CamilleKoczorowski/population-gcn/blob/master/Reproductibilite_Article_PARISOT_FINALE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Projet Geometric Data Analysis - Spectral Graph Convolutions for Population-based Disease Prediction\n",
        "**Sarah Parisot⋆, Sofia Ira Ktena, Enzo Ferrante, Matthew Lee,\n",
        "Ricardo Guerrerro Moreno, Ben Glocker, and Daniel Rueckert**\n",
        "*Biomedical Image Analysis Group, Imperial College London, UK*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pw6tAmCt7MT3"
      },
      "source": [
        "____"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XOatE5hB-t0"
      },
      "source": [
        "## Chargement des données depuis le dossier data du Github"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/Users/camillekoczo/Desktop/MVA/Semestre_1/GEOMETRIC DATA ANALYSIS/population-gcn/notebook\n"
          ]
        }
      ],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/Users/camillekoczo/Desktop/MVA/Semestre_1/GEOMETRIC DATA ANALYSIS/population-gcn\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'/Users/camillekoczo/Desktop/MVA/Semestre_1/GEOMETRIC DATA ANALYSIS/population-gcn'"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%cd ..\n",
        "%pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Root set to: /Users/camillekoczo/Desktop/MVA/Semestre_1/GEOMETRIC DATA ANALYSIS/population-gcn\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "# Trouver le dossier du notebook\n",
        "root = Path(os.getcwd()).resolve()\n",
        "\n",
        "print(\"Root set to:\", root)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ILg2709CBi0",
        "outputId": "b968b377-51ce-4f52-9de6-91950341d855"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Nb dossiers sujets : 873\n",
            "['50128', '51203', '50325', '50117', '50573', '50741', '51009', '50574', '51036', '51204']\n"
          ]
        }
      ],
      "source": [
        "#root = \"/content/drive/MyDrive/MVA_ABIDE_article/ABIDE_pcp/cpac/filt_noglobal\"\n",
        "folder = root / \"data/ABIDE_pcp/cpac/filt_noglobal\"\n",
        "print(\"Nb dossiers sujets :\", len(os.listdir(folder)))\n",
        "print(os.listdir(folder)[:10])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myJZo6HFCBrs"
      },
      "source": [
        "____"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "___"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUlDD-i2L81r"
      },
      "source": [
        "## Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "OKmxpxjy_snq"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /opt/homebrew/Caskroom/miniconda/base/envs/gcn-env/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n",
            "/Users/camillekoczo/Desktop/MVA/Semestre_1/GEOMETRIC DATA ANALYSIS/population-gcn/ABIDEParser.py:228: RuntimeWarning: divide by zero encountered in arctanh\n",
            "  norm_networks = [np.arctanh(mat) for mat in all_networks]\n",
            "WARNING:tensorflow:From /opt/homebrew/Caskroom/miniconda/base/envs/gcn-env/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n",
            "WARNING:tensorflow:From /opt/homebrew/Caskroom/miniconda/base/envs/gcn-env/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n",
            "WARNING:tensorflow:From /opt/homebrew/Caskroom/miniconda/base/envs/gcn-env/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n",
            "WARNING:tensorflow:From /opt/homebrew/Caskroom/miniconda/base/envs/gcn-env/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n",
            "WARNING:tensorflow:From /opt/homebrew/Caskroom/miniconda/base/envs/gcn-env/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n",
            "WARNING:tensorflow:From /opt/homebrew/Caskroom/miniconda/base/envs/gcn-env/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n",
            "WARNING:tensorflow:From /opt/homebrew/Caskroom/miniconda/base/envs/gcn-env/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n",
            "WARNING:tensorflow:From /opt/homebrew/Caskroom/miniconda/base/envs/gcn-env/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n",
            "WARNING:tensorflow:From /opt/homebrew/Caskroom/miniconda/base/envs/gcn-env/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n",
            "WARNING:tensorflow:From /opt/homebrew/Caskroom/miniconda/base/envs/gcn-env/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n",
            "784\n",
            "784\n",
            "784\n",
            "784\n",
            "784\n",
            "784\n",
            "784\n",
            "783\n",
            "784\n",
            "784\n",
            "Fitting estimator with 6105 features.\n",
            "Fitting estimator with 6105 features.\n",
            "Fitting estimator with 6105 features.\n",
            "Fitting estimator with 6105 features.\n",
            "Fitting estimator with 6105 features.\n",
            "Fitting estimator with 6105 features.\n",
            "Fitting estimator with 6105 features.\n",
            "Fitting estimator with 6105 features.\n",
            "Fitting estimator with 6105 features.\n",
            "Fitting estimator with 6105 features.\n",
            "Fitting estimator with 6005 features.\n",
            "Fitting estimator with 6005 features.\n",
            "Fitting estimator with 6005 features.\n",
            "Fitting estimator with 6005 features.\n",
            "Fitting estimator with 6005 features.\n",
            "Fitting estimator with 6005 features.\n",
            "Fitting estimator with 6005 features.\n",
            "Fitting estimator with 6005 features.\n",
            "Fitting estimator with 6005 features.\n",
            "Fitting estimator with 6005 features.\n",
            "Fitting estimator with 5905 features.\n",
            "Fitting estimator with 5905 features.\n",
            "Fitting estimator with 5905 features.\n",
            "Fitting estimator with 5905 features.\n",
            "Fitting estimator with 5905 features.\n",
            "Fitting estimator with 5905 features.\n",
            "Fitting estimator with 5905 features.\n",
            "Fitting estimator with 5905 features.\n",
            "Fitting estimator with 5905 features.\n",
            "Fitting estimator with 5905 features.\n",
            "Fitting estimator with 5805 features.\n",
            "Fitting estimator with 5805 features.\n",
            "Fitting estimator with 5805 features.\n",
            "Fitting estimator with 5805 features.\n",
            "Fitting estimator with 5805 features.\n",
            "Fitting estimator with 5805 features.\n",
            "Fitting estimator with 5805 features.\n",
            "Fitting estimator with 5805 features.\n",
            "Fitting estimator with 5805 features.\n",
            "Fitting estimator with 5805 features.\n",
            "Fitting estimator with 5705 features.\n",
            "Fitting estimator with 5705 features.\n",
            "Fitting estimator with 5705 features.\n",
            "Fitting estimator with 5705 features.\n",
            "Fitting estimator with 5705 features.\n",
            "Fitting estimator with 5705 features.\n",
            "Fitting estimator with 5705 features.\n",
            "Fitting estimator with 5705 features.\n",
            "Fitting estimator with 5705 features.\n",
            "Fitting estimator with 5705 features.\n",
            "Fitting estimator with 5605 features.\n",
            "Fitting estimator with 5605 features.\n",
            "Fitting estimator with 5605 features.\n",
            "Fitting estimator with 5605 features.\n",
            "Fitting estimator with 5605 features.\n",
            "Fitting estimator with 5605 features.\n",
            "Fitting estimator with 5605 features.\n",
            "Fitting estimator with 5605 features.\n",
            "Fitting estimator with 5605 features.\n",
            "Fitting estimator with 5605 features.\n",
            "Fitting estimator with 5505 features.\n",
            "Fitting estimator with 5505 features.\n",
            "Fitting estimator with 5505 features.\n",
            "Fitting estimator with 5505 features.\n",
            "Fitting estimator with 5505 features.\n",
            "Fitting estimator with 5505 features.\n",
            "Fitting estimator with 5505 features.\n",
            "Fitting estimator with 5505 features.\n",
            "Fitting estimator with 5505 features.\n",
            "Fitting estimator with 5505 features.\n",
            "Fitting estimator with 5405 features.\n",
            "Fitting estimator with 5405 features.\n",
            "Fitting estimator with 5405 features.\n",
            "Fitting estimator with 5405 features.\n",
            "Fitting estimator with 5405 features.\n",
            "Fitting estimator with 5405 features.\n",
            "Fitting estimator with 5405 features.\n",
            "Fitting estimator with 5405 features.\n",
            "Fitting estimator with 5405 features.\n",
            "Fitting estimator with 5405 features.\n",
            "Fitting estimator with 5305 features.\n",
            "Fitting estimator with 5305 features.\n",
            "Fitting estimator with 5305 features.\n",
            "Fitting estimator with 5305 features.\n",
            "Fitting estimator with 5305 features.\n",
            "Fitting estimator with 5305 features.\n",
            "Fitting estimator with 5305 features.\n",
            "Fitting estimator with 5305 features.\n",
            "Fitting estimator with 5305 features.\n",
            "Fitting estimator with 5305 features.\n",
            "Fitting estimator with 5205 features.\n",
            "Fitting estimator with 5205 features.\n",
            "Fitting estimator with 5205 features.\n",
            "Fitting estimator with 5205 features.\n",
            "Fitting estimator with 5205 features.\n",
            "Fitting estimator with 5205 features.\n",
            "Fitting estimator with 5205 features.\n",
            "Fitting estimator with 5205 features.\n",
            "Fitting estimator with 5205 features.\n",
            "Fitting estimator with 5205 features.\n",
            "Fitting estimator with 5105 features.\n",
            "Fitting estimator with 5105 features.\n",
            "Fitting estimator with 5105 features.\n",
            "Fitting estimator with 5105 features.\n",
            "Fitting estimator with 5105 features.\n",
            "Fitting estimator with 5105 features.\n",
            "Fitting estimator with 5105 features.\n",
            "Fitting estimator with 5105 features.\n",
            "Fitting estimator with 5105 features.\n",
            "Fitting estimator with 5105 features.\n",
            "Fitting estimator with 5005 features.\n",
            "Fitting estimator with 5005 features.\n",
            "Fitting estimator with 5005 features.\n",
            "Fitting estimator with 5005 features.\n",
            "Fitting estimator with 5005 features.\n",
            "Fitting estimator with 5005 features.\n",
            "Fitting estimator with 5005 features.\n",
            "Fitting estimator with 5005 features.\n",
            "Fitting estimator with 5005 features.\n",
            "Fitting estimator with 4905 features.\n",
            "Fitting estimator with 5005 features.\n",
            "Fitting estimator with 4905 features.\n",
            "Fitting estimator with 4905 features.\n",
            "Fitting estimator with 4905 features.\n",
            "Fitting estimator with 4905 features.\n",
            "Fitting estimator with 4905 features.\n",
            "Fitting estimator with 4905 features.\n",
            "Fitting estimator with 4905 features.\n",
            "Fitting estimator with 4905 features.\n",
            "Fitting estimator with 4805 features.\n",
            "Fitting estimator with 4905 features.\n",
            "Fitting estimator with 4805 features.\n",
            "Fitting estimator with 4805 features.\n",
            "Fitting estimator with 4805 features.\n",
            "Fitting estimator with 4805 features.\n",
            "Fitting estimator with 4805 features.\n",
            "Fitting estimator with 4805 features.\n",
            "Fitting estimator with 4805 features.\n",
            "Fitting estimator with 4805 features.\n",
            "Fitting estimator with 4705 features.\n",
            "Fitting estimator with 4705 features.\n",
            "Fitting estimator with 4805 features.\n",
            "Fitting estimator with 4705 features.\n",
            "Fitting estimator with 4705 features.\n",
            "Fitting estimator with 4705 features.\n",
            "Fitting estimator with 4705 features.\n",
            "Fitting estimator with 4705 features.\n",
            "Fitting estimator with 4705 features.\n",
            "Fitting estimator with 4705 features.\n",
            "Fitting estimator with 4605 features.\n",
            "Fitting estimator with 4605 features.\n",
            "Fitting estimator with 4705 features.\n",
            "Fitting estimator with 4605 features.\n",
            "Fitting estimator with 4605 features.\n",
            "Fitting estimator with 4605 features.\n",
            "Fitting estimator with 4605 features.\n",
            "Fitting estimator with 4605 features.\n",
            "Fitting estimator with 4605 features.\n",
            "Fitting estimator with 4605 features.\n",
            "Fitting estimator with 4505 features.\n",
            "Fitting estimator with 4505 features.\n",
            "Fitting estimator with 4605 features.\n",
            "Fitting estimator with 4505 features.\n",
            "Fitting estimator with 4505 features.\n",
            "Fitting estimator with 4505 features.\n",
            "Fitting estimator with 4505 features.\n",
            "Fitting estimator with 4505 features.\n",
            "Fitting estimator with 4505 features.\n",
            "Fitting estimator with 4405 features.\n",
            "Fitting estimator with 4505 features.\n",
            "Fitting estimator with 4405 features.\n",
            "Fitting estimator with 4505 features.\n",
            "Fitting estimator with 4405 features.\n",
            "Fitting estimator with 4405 features.\n",
            "Fitting estimator with 4405 features.\n",
            "Fitting estimator with 4405 features.\n",
            "Fitting estimator with 4405 features.\n",
            "Fitting estimator with 4405 features.\n",
            "Fitting estimator with 4405 features.\n",
            "Fitting estimator with 4305 features.\n",
            "Fitting estimator with 4305 features.\n",
            "Fitting estimator with 4305 features.\n",
            "Fitting estimator with 4405 features.\n",
            "Fitting estimator with 4305 features.\n",
            "Fitting estimator with 4305 features.\n",
            "Fitting estimator with 4305 features.\n",
            "Fitting estimator with 4305 features.\n",
            "Fitting estimator with 4305 features.\n",
            "Fitting estimator with 4205 features.\n",
            "Fitting estimator with 4305 features.\n",
            "Fitting estimator with 4205 features.\n",
            "Fitting estimator with 4205 features.\n",
            "Fitting estimator with 4205 features.\n",
            "Fitting estimator with 4305 features.\n",
            "Fitting estimator with 4205 features.\n",
            "Fitting estimator with 4205 features.\n",
            "Fitting estimator with 4205 features.\n",
            "Fitting estimator with 4205 features.\n",
            "Fitting estimator with 4205 features.\n",
            "Fitting estimator with 4105 features.\n",
            "Fitting estimator with 4105 features.\n",
            "Fitting estimator with 4105 features.\n",
            "Fitting estimator with 4105 features.\n",
            "Fitting estimator with 4205 features.\n",
            "Fitting estimator with 4105 features.\n",
            "Fitting estimator with 4105 features.\n",
            "Fitting estimator with 4105 features.\n",
            "Fitting estimator with 4005 features.\n",
            "Fitting estimator with 4105 features.\n",
            "Fitting estimator with 4105 features.\n",
            "Fitting estimator with 4005 features.\n",
            "Fitting estimator with 4005 features.\n",
            "Fitting estimator with 4005 features.\n",
            "Fitting estimator with 4105 features.\n",
            "Fitting estimator with 4005 features.\n",
            "Fitting estimator with 4005 features.\n",
            "Fitting estimator with 4005 features.\n",
            "Fitting estimator with 3905 features.\n",
            "Fitting estimator with 4005 features.\n",
            "Fitting estimator with 4005 features.\n",
            "Fitting estimator with 3905 features.\n",
            "Fitting estimator with 3905 features.\n",
            "Fitting estimator with 3905 features.\n",
            "Fitting estimator with 4005 features.\n",
            "Fitting estimator with 3905 features.\n",
            "Fitting estimator with 3905 features.\n",
            "Fitting estimator with 3905 features.\n",
            "Fitting estimator with 3905 features.\n",
            "Fitting estimator with 3805 features.\n",
            "Fitting estimator with 3905 features.\n",
            "Fitting estimator with 3805 features.\n",
            "Fitting estimator with 3805 features.\n",
            "Fitting estimator with 3805 features.\n",
            "Fitting estimator with 3905 features.\n",
            "Fitting estimator with 3805 features.\n",
            "Fitting estimator with 3805 features.\n",
            "Fitting estimator with 3705 features.\n",
            "Fitting estimator with 3805 features.\n",
            "Fitting estimator with 3805 features.\n",
            "Fitting estimator with 3805 features.\n",
            "Fitting estimator with 3705 features.\n",
            "Fitting estimator with 3705 features.\n",
            "Fitting estimator with 3805 features.\n",
            "Fitting estimator with 3705 features.\n",
            "Fitting estimator with 3705 features.\n",
            "Fitting estimator with 3705 features.\n",
            "Fitting estimator with 3605 features.\n",
            "Fitting estimator with 3705 features.\n",
            "Fitting estimator with 3705 features.\n",
            "Fitting estimator with 3705 features.\n",
            "Fitting estimator with 3605 features.\n",
            "Fitting estimator with 3605 features.\n",
            "Fitting estimator with 3605 features.\n",
            "Fitting estimator with 3705 features.\n",
            "Fitting estimator with 3605 features.\n",
            "Fitting estimator with 3605 features.\n",
            "Fitting estimator with 3505 features.\n",
            "Fitting estimator with 3605 features.\n",
            "Fitting estimator with 3605 features.\n",
            "Fitting estimator with 3505 features.\n",
            "Fitting estimator with 3605 features.\n",
            "Fitting estimator with 3505 features.\n",
            "Fitting estimator with 3505 features.\n",
            "Fitting estimator with 3505 features.\n",
            "Fitting estimator with 3605 features.\n",
            "Fitting estimator with 3505 features.\n",
            "Fitting estimator with 3405 features.\n",
            "Fitting estimator with 3505 features.\n",
            "Fitting estimator with 3505 features.\n",
            "Fitting estimator with 3405 features.\n",
            "Fitting estimator with 3505 features.\n",
            "Fitting estimator with 3405 features.\n",
            "Fitting estimator with 3405 features.\n",
            "Fitting estimator with 3405 features.\n",
            "Fitting estimator with 3505 features.\n",
            "Fitting estimator with 3405 features.\n",
            "Fitting estimator with 3305 features.\n",
            "Fitting estimator with 3405 features.\n",
            "Fitting estimator with 3405 features.\n",
            "Fitting estimator with 3305 features.\n",
            "Fitting estimator with 3405 features.\n",
            "Fitting estimator with 3305 features.\n",
            "Fitting estimator with 3305 features.\n",
            "Fitting estimator with 3305 features.\n",
            "Fitting estimator with 3405 features.\n",
            "Fitting estimator with 3305 features.\n",
            "Fitting estimator with 3205 features.\n",
            "Fitting estimator with 3305 features.\n",
            "Fitting estimator with 3305 features.\n",
            "Fitting estimator with 3205 features.\n",
            "Fitting estimator with 3305 features.\n",
            "Fitting estimator with 3205 features.\n",
            "Fitting estimator with 3205 features.\n",
            "Fitting estimator with 3205 features.\n",
            "Fitting estimator with 3305 features.\n",
            "Fitting estimator with 3205 features.\n",
            "Fitting estimator with 3105 features.\n",
            "Fitting estimator with 3205 features.\n",
            "Fitting estimator with 3105 features.\n",
            "Fitting estimator with 3205 features.\n",
            "Fitting estimator with 3205 features.\n",
            "Fitting estimator with 3105 features.\n",
            "Fitting estimator with 3105 features.\n",
            "Fitting estimator with 3105 features.\n",
            "Fitting estimator with 3205 features.\n",
            "Fitting estimator with 3105 features.\n",
            "Fitting estimator with 3105 features.\n",
            "Fitting estimator with 3005 features.\n",
            "Fitting estimator with 3105 features.\n",
            "Fitting estimator with 3005 features.\n",
            "Fitting estimator with 3105 features.\n",
            "Fitting estimator with 3005 features.\n",
            "Fitting estimator with 3005 features.\n",
            "Fitting estimator with 3005 features.\n",
            "Fitting estimator with 3005 features.\n",
            "Fitting estimator with 3105 features.\n",
            "Fitting estimator with 2905 features.\n",
            "Fitting estimator with 3005 features.\n",
            "Fitting estimator with 3005 features.\n",
            "Fitting estimator with 2905 features.\n",
            "Fitting estimator with 2905 features.\n",
            "Fitting estimator with 2905 features.\n",
            "Fitting estimator with 3005 features.\n",
            "Fitting estimator with 2905 features.\n",
            "Fitting estimator with 3005 features.\n",
            "Fitting estimator with 2905 features.\n",
            "Fitting estimator with 2805 features.\n",
            "Fitting estimator with 2805 features.\n",
            "Fitting estimator with 2905 features.\n",
            "Fitting estimator with 2905 features.\n",
            "Fitting estimator with 2805 features.\n",
            "Fitting estimator with 2805 features.\n",
            "Fitting estimator with 2805 features.\n",
            "Fitting estimator with 2905 features.\n",
            "Fitting estimator with 2805 features.\n",
            "Fitting estimator with 2905 features.\n",
            "Fitting estimator with 2705 features.\n",
            "Fitting estimator with 2705 features.\n",
            "Fitting estimator with 2805 features.Fitting estimator with 2705 features.\n",
            "\n",
            "Fitting estimator with 2705 features.\n",
            "Fitting estimator with 2805 features.\n",
            "Fitting estimator with 2705 features.\n",
            "Fitting estimator with 2805 features.\n",
            "Fitting estimator with 2605 features.\n",
            "Fitting estimator with 2805 features.\n",
            "Fitting estimator with 2705 features.\n",
            "Fitting estimator with 2605 features.\n",
            "Fitting estimator with 2605 features.\n",
            "Fitting estimator with 2705 features.\n",
            "Fitting estimator with 2605 features.\n",
            "Fitting estimator with 2705 features.\n",
            "Fitting estimator with 2705 features.\n",
            "Fitting estimator with 2605 features.\n",
            "Fitting estimator with 2605 features.\n",
            "Fitting estimator with 2505 features.\n",
            "Fitting estimator with 2705 features.\n",
            "Fitting estimator with 2505 features.\n",
            "Fitting estimator with 2505 features.\n",
            "Fitting estimator with 2605 features.\n",
            "Fitting estimator with 2505 features.\n",
            "Fitting estimator with 2605 features.\n",
            "Fitting estimator with 2505 features.\n",
            "Fitting estimator with 2605 features.\n",
            "Fitting estimator with 2405 features.\n",
            "Fitting estimator with 2505 features.\n",
            "Fitting estimator with 2405 features.\n",
            "Fitting estimator with 2605 features.\n",
            "Fitting estimator with 2405 features.\n",
            "Fitting estimator with 2505 features.\n",
            "Fitting estimator with 2405 features.\n",
            "Fitting estimator with 2505 features.\n",
            "Fitting estimator with 2405 features.\n",
            "Fitting estimator with 2505 features.\n",
            "Fitting estimator with 2405 features.\n",
            "Fitting estimator with 2305 features.\n",
            "Fitting estimator with 2305 features.\n",
            "Fitting estimator with 2505 features.\n",
            "Fitting estimator with 2305 features.\n",
            "Fitting estimator with 2305 features.\n",
            "Fitting estimator with 2405 features.\n",
            "Fitting estimator with 2405 features.\n",
            "Fitting estimator with 2305 features.\n",
            "Fitting estimator with 2405 features.\n",
            "Fitting estimator with 2305 features.\n",
            "Fitting estimator with 2205 features.\n",
            "Fitting estimator with 2205 features.\n",
            "Fitting estimator with 2405 features.\n",
            "Fitting estimator with 2205 features.\n",
            "Fitting estimator with 2205 features.\n",
            "Fitting estimator with 2305 features.\n",
            "Fitting estimator with 2305 features.\n",
            "Fitting estimator with 2205 features.\n",
            "Fitting estimator with 2305 features.\n",
            "Fitting estimator with 2205 features.\n",
            "Fitting estimator with 2105 features.\n",
            "Fitting estimator with 2105 features.\n",
            "Fitting estimator with 2305 features.\n",
            "Fitting estimator with 2105 features.\n",
            "Fitting estimator with 2105 features.\n",
            "Fitting estimator with 2205 features.\n",
            "Fitting estimator with 2205 features.\n",
            "Fitting estimator with 2105 features.\n",
            "Fitting estimator with 2205 features.\n",
            "Fitting estimator with 2105 features.\n",
            "Fitting estimator with 2005 features.\n",
            "Fitting estimator with 2005 features.\n",
            "Fitting estimator with 2205 features.\n",
            "Fitting estimator with 2005 features.\n",
            "Fitting estimator with 2005 features.\n",
            "Fitting estimator with 2105 features.\n",
            "Fitting estimator with 2105 features.\n",
            "Fitting estimator with 2005 features.\n",
            "Fitting estimator with 2105 features.\n",
            "Fitting estimator with 2005 features.\n",
            "Fitting estimator with 2105 features.\n",
            "Fitting estimator with 2005 features.\n",
            "Fitting estimator with 2005 features.\n",
            "Fitting estimator with 2005 features.\n",
            "Number of labeled samples 784\n",
            "Number of features selected 2000\n",
            "Number of labeled samples 784\n",
            "Number of features selected 2000\n",
            "Fitting estimator with 2005 features.\n",
            "Number of labeled samples 784\n",
            "Number of features selected 2000\n",
            "Number of labeled samples 784\n",
            "Number of features selected 2000\n",
            "Number of labeled samples 784\n",
            "Number of features selected 2000\n",
            "Number of labeled samples 784\n",
            "Number of features selected 2000\n",
            "Number of labeled samples 784\n",
            "Number of features selected 2000\n",
            "Number of labeled samples 784\n",
            "Number of features selected 2000\n",
            "Number of labeled samples 784\n",
            "Number of features selected 2000\n",
            "Number of labeled samples 783\n",
            "Number of features selected 2000\n",
            "Linear Accuracy: 0.6666666666666666\n",
            "Linear Accuracy: 0.6896551724137931\n",
            "Linear Accuracy: 0.632183908045977\n",
            "Linear Accuracy: 0.6781609195402298\n",
            "Linear Accuracy: 0.6781609195402298\n",
            "Linear Accuracy: 0.6436781609195402\n",
            "Linear Accuracy: 0.7931034482758621\n",
            "Linear Accuracy: 0.5977011494252874\n",
            "Linear Accuracy: 0.5862068965517241\n",
            "Linear Accuracy: 0.6477272727272727\n",
            "Calculating Chebyshev polynomials up to order 3...\n",
            "Calculating Chebyshev polynomials up to order 3...\n",
            "Calculating Chebyshev polynomials up to order 3...\n",
            "Calculating Chebyshev polynomials up to order 3...\n",
            "Calculating Chebyshev polynomials up to order 3...\n",
            "Calculating Chebyshev polynomials up to order 3...\n",
            "Calculating Chebyshev polynomials up to order 3...\n",
            "Calculating Chebyshev polynomials up to order 3...\n",
            "Calculating Chebyshev polynomials up to order 3...\n",
            "Calculating Chebyshev polynomials up to order 3...\n",
            "WARNING:tensorflow:From /opt/homebrew/Caskroom/miniconda/base/envs/gcn-env/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:1176: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /opt/homebrew/Caskroom/miniconda/base/envs/gcn-env/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:1176: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n",
            "WARNING:tensorflow:From /opt/homebrew/Caskroom/miniconda/base/envs/gcn-env/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:1176: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /opt/homebrew/Caskroom/miniconda/base/envs/gcn-env/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:1176: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /opt/homebrew/Caskroom/miniconda/base/envs/gcn-env/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:1176: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n",
            "WARNING:tensorflow:From /opt/homebrew/Caskroom/miniconda/base/envs/gcn-env/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:1176: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n",
            "2025-12-04 10:44:03.292054: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:375] MLIR V1 optimization pass is not enabled\n",
            "WARNING:tensorflow:From /opt/homebrew/Caskroom/miniconda/base/envs/gcn-env/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:1176: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /opt/homebrew/Caskroom/miniconda/base/envs/gcn-env/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:1176: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /opt/homebrew/Caskroom/miniconda/base/envs/gcn-env/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:1176: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n",
            "WARNING:tensorflow:From /opt/homebrew/Caskroom/miniconda/base/envs/gcn-env/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:1176: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n",
            "WARNING:tensorflow:From /opt/homebrew/Caskroom/miniconda/base/envs/gcn-env/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:1176: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /opt/homebrew/Caskroom/miniconda/base/envs/gcn-env/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:1176: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n",
            "WARNING:tensorflow:From /opt/homebrew/Caskroom/miniconda/base/envs/gcn-env/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:1176: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /opt/homebrew/Caskroom/miniconda/base/envs/gcn-env/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:1176: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /opt/homebrew/Caskroom/miniconda/base/envs/gcn-env/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:1176: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /opt/homebrew/Caskroom/miniconda/base/envs/gcn-env/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:1176: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n",
            "WARNING:tensorflow:From /opt/homebrew/Caskroom/miniconda/base/envs/gcn-env/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:1176: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n",
            "WARNING:tensorflow:From /opt/homebrew/Caskroom/miniconda/base/envs/gcn-env/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:1176: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /opt/homebrew/Caskroom/miniconda/base/envs/gcn-env/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:1176: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n",
            "WARNING:tensorflow:From /opt/homebrew/Caskroom/miniconda/base/envs/gcn-env/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:1176: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n",
            "2025-12-04 10:44:03.342465: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:375] MLIR V1 optimization pass is not enabled\n",
            "2025-12-04 10:44:03.345612: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:375] MLIR V1 optimization pass is not enabled\n",
            "2025-12-04 10:44:03.381541: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:375] MLIR V1 optimization pass is not enabled\n",
            "2025-12-04 10:44:03.388062: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:375] MLIR V1 optimization pass is not enabled\n",
            "2025-12-04 10:44:03.400871: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:375] MLIR V1 optimization pass is not enabled\n",
            "2025-12-04 10:44:03.410725: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:375] MLIR V1 optimization pass is not enabled\n",
            "2025-12-04 10:44:03.415187: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:375] MLIR V1 optimization pass is not enabled\n",
            "2025-12-04 10:44:03.415264: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:375] MLIR V1 optimization pass is not enabled\n",
            "2025-12-04 10:44:03.419116: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:375] MLIR V1 optimization pass is not enabled\n",
            "Epoch: 0001 train_loss= 0.72475 train_acc= 0.52168 train_auc= 0.53158 val_loss= 0.71745 val_acc= 0.52874 val_auc= 0.50954 time= 0.59872\n",
            "Epoch: 0001 train_loss= 0.72477 train_acc= 0.52296 train_auc= 0.51033 val_loss= 0.71639 val_acc= 0.54023 val_auc= 0.50000 time= 0.62223\n",
            "Epoch: 0001 train_loss= 0.72491 train_acc= 0.51658 train_auc= 0.44146 val_loss= 0.71630 val_acc= 0.54023 val_auc= 0.61277 time= 0.63837\n",
            "Epoch: 0001 train_loss= 0.72483 train_acc= 0.52363 train_auc= 0.46091 val_loss= 0.71643 val_acc= 0.53409 val_auc= 0.59159 time= 0.76497\n",
            "Epoch: 0001 train_loss= 0.72472 train_acc= 0.53827 train_auc= 0.49603 val_loss= 0.71631 val_acc= 0.54023 val_auc= 0.59947 time= 0.72217\n",
            "Epoch: 0001 train_loss= 0.72483 train_acc= 0.50638 train_auc= 0.48552 val_loss= 0.71605 val_acc= 0.54023 val_auc= 0.56436 time= 0.80711\n",
            "Epoch: 0001 train_loss= 0.72483 train_acc= 0.52806 train_auc= 0.48928 val_loss= 0.71681 val_acc= 0.54023 val_auc= 0.51223 time= 0.79349\n",
            "Epoch: 0001 train_loss= 0.72469 train_acc= 0.51020 train_auc= 0.50170 val_loss= 0.71806 val_acc= 0.52874 val_auc= 0.46501 time= 0.81500\n",
            "Epoch: 0001 train_loss= 0.72491 train_acc= 0.50255 train_auc= 0.48380 val_loss= 0.71648 val_acc= 0.54023 val_auc= 0.56170 time= 0.82197\n",
            "Epoch: 0002 train_loss= 0.71622 train_acc= 0.53699 train_auc= 0.53275 val_loss= 0.70965 val_acc= 0.54023 val_auc= 0.51436 time= 0.46765\n",
            "Epoch: 0002 train_loss= 0.71646 train_acc= 0.53699 train_auc= 0.52494 val_loss= 0.70964 val_acc= 0.54023 val_auc= 0.59934 time= 0.46600\n",
            "Epoch: 0002 train_loss= 0.71610 train_acc= 0.53827 train_auc= 0.54947 val_loss= 0.71184 val_acc= 0.52874 val_auc= 0.55408 time= 0.54116\n",
            "Epoch: 0001 train_loss= 0.72482 train_acc= 0.51658 train_auc= 0.49041 val_loss= 0.71751 val_acc= 0.54023 val_auc= 0.43777 time= 0.91701\n",
            "Epoch: 0002 train_loss= 0.71622 train_acc= 0.53768 train_auc= 0.52646 val_loss= 0.70984 val_acc= 0.53409 val_auc= 0.59626 time= 0.44480\n",
            "Epoch: 0002 train_loss= 0.71629 train_acc= 0.53699 train_auc= 0.52989 val_loss= 0.70975 val_acc= 0.54023 val_auc= 0.64043 time= 0.45110\n",
            "Epoch: 0002 train_loss= 0.71644 train_acc= 0.53699 train_auc= 0.53469 val_loss= 0.70883 val_acc= 0.54023 val_auc= 0.59096 time= 0.42783\n",
            "Epoch: 0002 train_loss= 0.71596 train_acc= 0.53827 train_auc= 0.54277 val_loss= 0.71307 val_acc= 0.52874 val_auc= 0.49735 time= 0.39230\n",
            "Epoch: 0003 train_loss= 0.71000 train_acc= 0.53699 train_auc= 0.50257 val_loss= 0.70442 val_acc= 0.54023 val_auc= 0.53032 time= 0.38042\n",
            "Epoch: 0002 train_loss= 0.71649 train_acc= 0.53699 train_auc= 0.53216 val_loss= 0.71021 val_acc= 0.54023 val_auc= 0.52340 time= 0.49304\n",
            "Epoch: 0002 train_loss= 0.71653 train_acc= 0.53699 train_auc= 0.52152 val_loss= 0.70952 val_acc= 0.54023 val_auc= 0.58723 time= 0.45930\n",
            "Epoch: 0002 train_loss= 0.71618 train_acc= 0.53699 train_auc= 0.55096 val_loss= 0.71168 val_acc= 0.54023 val_auc= 0.44521 time= 0.36755\n",
            "Epoch: 0003 train_loss= 0.71003 train_acc= 0.53699 train_auc= 0.50256 val_loss= 0.70456 val_acc= 0.54023 val_auc= 0.62128 time= 0.41700\n",
            "Epoch: 0003 train_loss= 0.70980 train_acc= 0.53827 train_auc= 0.51879 val_loss= 0.70749 val_acc= 0.52874 val_auc= 0.56681 time= 0.46594\n",
            "Epoch: 0003 train_loss= 0.71006 train_acc= 0.53768 train_auc= 0.49867 val_loss= 0.70483 val_acc= 0.53409 val_auc= 0.59938 time= 0.37909\n",
            "Epoch: 0003 train_loss= 0.71009 train_acc= 0.53699 train_auc= 0.50184 val_loss= 0.70462 val_acc= 0.54023 val_auc= 0.67606 time= 0.40654\n",
            "Epoch: 0003 train_loss= 0.70994 train_acc= 0.53827 train_auc= 0.51160 val_loss= 0.70878 val_acc= 0.52874 val_auc= 0.50583 time= 0.36156\n",
            "Epoch: 0003 train_loss= 0.71001 train_acc= 0.53699 train_auc= 0.50384 val_loss= 0.70333 val_acc= 0.54023 val_auc= 0.61543 time= 0.36452\n",
            "Epoch: 0004 train_loss= 0.70467 train_acc= 0.53699 train_auc= 0.52411 val_loss= 0.70039 val_acc= 0.54023 val_auc= 0.56237 time= 0.36905\n",
            "Epoch: 0003 train_loss= 0.71055 train_acc= 0.53699 train_auc= 0.48565 val_loss= 0.70512 val_acc= 0.54023 val_auc= 0.53564 time= 0.38246\n",
            "Epoch: 0003 train_loss= 0.70996 train_acc= 0.53699 train_auc= 0.50700 val_loss= 0.70429 val_acc= 0.54023 val_auc= 0.60479 time= 0.38015\n",
            "Epoch: 0003 train_loss= 0.71017 train_acc= 0.53699 train_auc= 0.51254 val_loss= 0.70697 val_acc= 0.54023 val_auc= 0.45638 time= 0.38082\n",
            "Epoch: 0004 train_loss= 0.70490 train_acc= 0.53699 train_auc= 0.51534 val_loss= 0.70048 val_acc= 0.54023 val_auc= 0.63564 time= 0.38686\n",
            "Epoch: 0004 train_loss= 0.70476 train_acc= 0.53827 train_auc= 0.52366 val_loss= 0.70373 val_acc= 0.52874 val_auc= 0.58855 time= 0.33119\n",
            "Epoch: 0004 train_loss= 0.70491 train_acc= 0.53768 train_auc= 0.51628 val_loss= 0.70080 val_acc= 0.53409 val_auc= 0.62740 time= 0.41841\n",
            "Epoch: 0004 train_loss= 0.70463 train_acc= 0.53699 train_auc= 0.53192 val_loss= 0.70050 val_acc= 0.54023 val_auc= 0.70213 time= 0.36290\n",
            "Epoch: 0005 train_loss= 0.70043 train_acc= 0.53699 train_auc= 0.53328 val_loss= 0.69743 val_acc= 0.54023 val_auc= 0.58936 time= 0.31593\n",
            "Epoch: 0004 train_loss= 0.70459 train_acc= 0.53827 train_auc= 0.53102 val_loss= 0.70485 val_acc= 0.52874 val_auc= 0.51750 time= 0.37102\n",
            "Epoch: 0004 train_loss= 0.70450 train_acc= 0.53699 train_auc= 0.53275 val_loss= 0.69936 val_acc= 0.54023 val_auc= 0.62926 time= 0.36550\n",
            "Epoch: 0004 train_loss= 0.70477 train_acc= 0.53699 train_auc= 0.52151 val_loss= 0.70022 val_acc= 0.54023 val_auc= 0.62287 time= 0.35753\n",
            "Epoch: 0004 train_loss= 0.70512 train_acc= 0.53699 train_auc= 0.51790 val_loss= 0.70108 val_acc= 0.54023 val_auc= 0.55745 time= 0.38692\n",
            "Epoch: 0005 train_loss= 0.70044 train_acc= 0.53699 train_auc= 0.53241 val_loss= 0.69724 val_acc= 0.54023 val_auc= 0.67021 time= 0.36116\n",
            "Epoch: 0004 train_loss= 0.70478 train_acc= 0.53699 train_auc= 0.54152 val_loss= 0.70302 val_acc= 0.54023 val_auc= 0.46223 time= 0.38617\n",
            "Epoch: 0005 train_loss= 0.70028 train_acc= 0.53827 train_auc= 0.54191 val_loss= 0.70050 val_acc= 0.52874 val_auc= 0.59703 time= 0.37705\n",
            "Epoch: 0005 train_loss= 0.70047 train_acc= 0.53768 train_auc= 0.53445 val_loss= 0.69775 val_acc= 0.53409 val_auc= 0.65283 time= 0.34761\n",
            "Epoch: 0005 train_loss= 0.70046 train_acc= 0.53699 train_auc= 0.54107 val_loss= 0.69713 val_acc= 0.54023 val_auc= 0.70904 time= 0.40818\n",
            "Epoch: 0006 train_loss= 0.69607 train_acc= 0.53699 train_auc= 0.59025 val_loss= 0.69538 val_acc= 0.54023 val_auc= 0.60745 time= 0.38507\n",
            "Epoch: 0005 train_loss= 0.70043 train_acc= 0.53827 train_auc= 0.53990 val_loss= 0.70134 val_acc= 0.52874 val_auc= 0.52439 time= 0.38037\n",
            "Epoch: 0005 train_loss= 0.70039 train_acc= 0.53699 train_auc= 0.53742 val_loss= 0.69656 val_acc= 0.54023 val_auc= 0.64309 time= 0.40260\n",
            "Epoch: 0005 train_loss= 0.70070 train_acc= 0.53699 train_auc= 0.52626 val_loss= 0.69708 val_acc= 0.54023 val_auc= 0.63723 time= 0.33267\n",
            "Epoch: 0006 train_loss= 0.69645 train_acc= 0.53699 train_auc= 0.58265 val_loss= 0.69490 val_acc= 0.54023 val_auc= 0.70053 time= 0.36740\n",
            "Epoch: 0005 train_loss= 0.70087 train_acc= 0.53699 train_auc= 0.52893 val_loss= 0.69785 val_acc= 0.54023 val_auc= 0.57819 time= 0.40372\n",
            "Epoch: 0006 train_loss= 0.69641 train_acc= 0.53827 train_auc= 0.57644 val_loss= 0.69787 val_acc= 0.52874 val_auc= 0.60233 time= 0.36586\n",
            "Epoch: 0005 train_loss= 0.70086 train_acc= 0.53699 train_auc= 0.53635 val_loss= 0.69969 val_acc= 0.54023 val_auc= 0.46862 time= 0.39194\n",
            "Epoch: 0006 train_loss= 0.69683 train_acc= 0.53768 train_auc= 0.57340 val_loss= 0.69562 val_acc= 0.53409 val_auc= 0.66217 time= 0.36827\n",
            "Epoch: 0006 train_loss= 0.69689 train_acc= 0.53699 train_auc= 0.57823 val_loss= 0.69456 val_acc= 0.54023 val_auc= 0.73032 time= 0.35938\n",
            "Epoch: 0006 train_loss= 0.69639 train_acc= 0.53827 train_auc= 0.58098 val_loss= 0.69847 val_acc= 0.52874 val_auc= 0.52598 time= 0.35996\n",
            "Epoch: 0007 train_loss= 0.69460 train_acc= 0.53699 train_auc= 0.56632 val_loss= 0.69398 val_acc= 0.54023 val_auc= 0.62500 time= 0.41412\n",
            "Epoch: 0006 train_loss= 0.69700 train_acc= 0.53699 train_auc= 0.56693 val_loss= 0.69480 val_acc= 0.54023 val_auc= 0.64628 time= 0.40292\n",
            "Epoch: 0006 train_loss= 0.69628 train_acc= 0.53699 train_auc= 0.58865 val_loss= 0.69487 val_acc= 0.54023 val_auc= 0.65426 time= 0.41443\n",
            "Epoch: 0007 train_loss= 0.69455 train_acc= 0.53954 train_auc= 0.57274 val_loss= 0.69599 val_acc= 0.52874 val_auc= 0.60392 time= 0.35683\n",
            "Epoch: 0006 train_loss= 0.69676 train_acc= 0.53699 train_auc= 0.58516 val_loss= 0.69708 val_acc= 0.54023 val_auc= 0.49149 time= 0.33809\n",
            "Epoch: 0007 train_loss= 0.69466 train_acc= 0.53699 train_auc= 0.56844 val_loss= 0.69314 val_acc= 0.54023 val_auc= 0.72713 time= 0.40449\n",
            "Epoch: 0006 train_loss= 0.69757 train_acc= 0.53699 train_auc= 0.54795 val_loss= 0.69547 val_acc= 0.54023 val_auc= 0.59202 time= 0.46166\n",
            "Epoch: 0007 train_loss= 0.69473 train_acc= 0.53768 train_auc= 0.57058 val_loss= 0.69426 val_acc= 0.53409 val_auc= 0.69382 time= 0.35793\n",
            "Epoch: 0007 train_loss= 0.69493 train_acc= 0.53699 train_auc= 0.57393 val_loss= 0.69280 val_acc= 0.54023 val_auc= 0.76862 time= 0.37331\n",
            "Epoch: 0007 train_loss= 0.69476 train_acc= 0.53699 train_auc= 0.56953 val_loss= 0.69328 val_acc= 0.54023 val_auc= 0.67181 time= 0.36840\n",
            "Epoch: 0007 train_loss= 0.69469 train_acc= 0.53827 train_auc= 0.56389 val_loss= 0.69635 val_acc= 0.52874 val_auc= 0.53924 time= 0.43499\n",
            "Epoch: 0008 train_loss= 0.69285 train_acc= 0.54337 train_auc= 0.58031 val_loss= 0.69308 val_acc= 0.54023 val_auc= 0.64681 time= 0.39586\n",
            "Epoch: 0007 train_loss= 0.69474 train_acc= 0.53699 train_auc= 0.57251 val_loss= 0.69515 val_acc= 0.54023 val_auc= 0.51702 time= 0.36896\n",
            "Epoch: 0007 train_loss= 0.69479 train_acc= 0.53827 train_auc= 0.56526 val_loss= 0.69340 val_acc= 0.54023 val_auc= 0.68457 time= 0.43005\n",
            "Epoch: 0008 train_loss= 0.69217 train_acc= 0.53954 train_auc= 0.60079 val_loss= 0.69206 val_acc= 0.54023 val_auc= 0.75106 time= 0.38353\n",
            "Epoch: 0008 train_loss= 0.69233 train_acc= 0.54082 train_auc= 0.59159 val_loss= 0.69469 val_acc= 0.52874 val_auc= 0.61824 time= 0.42613\n",
            "Epoch: 0008 train_loss= 0.69302 train_acc= 0.53768 train_auc= 0.59099 val_loss= 0.69315 val_acc= 0.53409 val_auc= 0.73210 time= 0.36367\n",
            "Epoch: 0007 train_loss= 0.69520 train_acc= 0.53699 train_auc= 0.54991 val_loss= 0.69383 val_acc= 0.54023 val_auc= 0.61809 time= 0.40455\n",
            "Epoch: 0008 train_loss= 0.69271 train_acc= 0.53699 train_auc= 0.60768 val_loss= 0.69128 val_acc= 0.54023 val_auc= 0.78511 time= 0.37622\n",
            "Epoch: 0008 train_loss= 0.69241 train_acc= 0.53827 train_auc= 0.58684 val_loss= 0.69492 val_acc= 0.52874 val_auc= 0.56575 time= 0.34468\n",
            "Epoch: 0008 train_loss= 0.69253 train_acc= 0.53699 train_auc= 0.61771 val_loss= 0.69225 val_acc= 0.54023 val_auc= 0.69947 time= 0.38957\n",
            "Epoch: 0009 train_loss= 0.69094 train_acc= 0.54337 train_auc= 0.61747 val_loss= 0.69254 val_acc= 0.54023 val_auc= 0.65319 time= 0.39564\n",
            "Epoch: 0008 train_loss= 0.69211 train_acc= 0.54337 train_auc= 0.60651 val_loss= 0.69253 val_acc= 0.54023 val_auc= 0.69681 time= 0.36789\n",
            "Epoch: 0008 train_loss= 0.69235 train_acc= 0.53699 train_auc= 0.60184 val_loss= 0.69386 val_acc= 0.54023 val_auc= 0.54202 time= 0.39385\n",
            "Epoch: 0009 train_loss= 0.69125 train_acc= 0.53699 train_auc= 0.61628 val_loss= 0.69123 val_acc= 0.54023 val_auc= 0.75532 time= 0.39433\n",
            "Epoch: 0009 train_loss= 0.69096 train_acc= 0.54082 train_auc= 0.61311 val_loss= 0.69390 val_acc= 0.52874 val_auc= 0.63043 time= 0.40603\n",
            "Epoch: 0009 train_loss= 0.69126 train_acc= 0.54023 train_auc= 0.62031 val_loss= 0.69258 val_acc= 0.53409 val_auc= 0.71251 time= 0.41066\n",
            "Epoch: 0008 train_loss= 0.69320 train_acc= 0.53699 train_auc= 0.58012 val_loss= 0.69278 val_acc= 0.54023 val_auc= 0.70426 time= 0.45723\n",
            "Epoch: 0009 train_loss= 0.69082 train_acc= 0.53699 train_auc= 0.65456 val_loss= 0.69037 val_acc= 0.54023 val_auc= 0.79415 time= 0.35734\n",
            "Epoch: 0009 train_loss= 0.69026 train_acc= 0.54209 train_auc= 0.63633 val_loss= 0.69407 val_acc= 0.52874 val_auc= 0.60710 time= 0.34294\n",
            "Epoch: 0009 train_loss= 0.69093 train_acc= 0.53699 train_auc= 0.65930 val_loss= 0.69166 val_acc= 0.54023 val_auc= 0.69787 time= 0.37124\n",
            "Epoch: 0010 train_loss= 0.68900 train_acc= 0.54082 train_auc= 0.66020 val_loss= 0.69232 val_acc= 0.54023 val_auc= 0.65532 time= 0.37674\n",
            "Epoch: 0009 train_loss= 0.69132 train_acc= 0.54082 train_auc= 0.61054 val_loss= 0.69205 val_acc= 0.54023 val_auc= 0.69521 time= 0.36196\n",
            "Epoch: 0009 train_loss= 0.69152 train_acc= 0.53699 train_auc= 0.60537 val_loss= 0.69294 val_acc= 0.54023 val_auc= 0.57181 time= 0.40444\n",
            "Epoch: 0010 train_loss= 0.68832 train_acc= 0.54337 train_auc= 0.66850 val_loss= 0.69355 val_acc= 0.52874 val_auc= 0.64051 time= 0.37055\n",
            "Epoch: 0010 train_loss= 0.68960 train_acc= 0.54592 train_auc= 0.63789 val_loss= 0.69074 val_acc= 0.54023 val_auc= 0.75426 time= 0.39879\n",
            "Epoch: 0010 train_loss= 0.68956 train_acc= 0.53895 train_auc= 0.65536 val_loss= 0.69232 val_acc= 0.53409 val_auc= 0.69486 time= 0.35692\n",
            "Epoch: 0009 train_loss= 0.69158 train_acc= 0.53699 train_auc= 0.63195 val_loss= 0.69210 val_acc= 0.54023 val_auc= 0.73191 time= 0.34361\n",
            "Epoch: 0010 train_loss= 0.68950 train_acc= 0.53827 train_auc= 0.66681 val_loss= 0.68982 val_acc= 0.54023 val_auc= 0.79043 time= 0.37589\n",
            "Epoch: 0010 train_loss= 0.68882 train_acc= 0.53954 train_auc= 0.66772 val_loss= 0.69359 val_acc= 0.52874 val_auc= 0.63786 time= 0.42452\n",
            "Epoch: 0010 train_loss= 0.69014 train_acc= 0.53699 train_auc= 0.66887 val_loss= 0.69138 val_acc= 0.54023 val_auc= 0.69681 time= 0.38942\n",
            "Epoch: 0011 train_loss= 0.68955 train_acc= 0.53954 train_auc= 0.64935 val_loss= 0.69240 val_acc= 0.54023 val_auc= 0.65957 time= 0.39285\n",
            "Epoch: 0010 train_loss= 0.68914 train_acc= 0.53827 train_auc= 0.65408 val_loss= 0.69180 val_acc= 0.54023 val_auc= 0.69202 time= 0.38370\n",
            "Epoch: 0010 train_loss= 0.68998 train_acc= 0.53699 train_auc= 0.64850 val_loss= 0.69241 val_acc= 0.54023 val_auc= 0.60266 time= 0.39665\n",
            "Epoch: 0011 train_loss= 0.69111 train_acc= 0.53827 train_auc= 0.59730 val_loss= 0.69041 val_acc= 0.54023 val_auc= 0.75745 time= 0.39064\n",
            "Epoch: 0011 train_loss= 0.68927 train_acc= 0.53954 train_auc= 0.64395 val_loss= 0.69339 val_acc= 0.52874 val_auc= 0.65005 time= 0.41852\n",
            "Epoch: 0010 train_loss= 0.69029 train_acc= 0.53699 train_auc= 0.66635 val_loss= 0.69158 val_acc= 0.54023 val_auc= 0.72979 time= 0.33880\n",
            "Epoch: 0011 train_loss= 0.68987 train_acc= 0.53895 train_auc= 0.64495 val_loss= 0.69223 val_acc= 0.53409 val_auc= 0.69382 time= 0.43026\n",
            "Epoch: 0011 train_loss= 0.69029 train_acc= 0.53954 train_auc= 0.64395 val_loss= 0.68950 val_acc= 0.54023 val_auc= 0.79096 time= 0.35452\n",
            "Epoch: 0011 train_loss= 0.68995 train_acc= 0.54082 train_auc= 0.64213 val_loss= 0.69327 val_acc= 0.52874 val_auc= 0.67603 time= 0.37973\n",
            "Epoch: 0011 train_loss= 0.69041 train_acc= 0.53827 train_auc= 0.65376 val_loss= 0.69134 val_acc= 0.54023 val_auc= 0.69894 time= 0.37276\n",
            "Epoch: 0012 train_loss= 0.68754 train_acc= 0.54337 train_auc= 0.71274 val_loss= 0.69270 val_acc= 0.54023 val_auc= 0.65691 time= 0.41440\n",
            "Epoch: 0011 train_loss= 0.68944 train_acc= 0.53827 train_auc= 0.64251 val_loss= 0.69183 val_acc= 0.54023 val_auc= 0.69255 time= 0.41516\n",
            "Epoch: 0011 train_loss= 0.69042 train_acc= 0.53827 train_auc= 0.63713 val_loss= 0.69211 val_acc= 0.54023 val_auc= 0.63644 time= 0.35595\n",
            "Epoch: 0012 train_loss= 0.68809 train_acc= 0.54464 train_auc= 0.67829 val_loss= 0.69027 val_acc= 0.54023 val_auc= 0.76170 time= 0.37885\n",
            "Epoch: 0012 train_loss= 0.68787 train_acc= 0.54209 train_auc= 0.69834 val_loss= 0.69350 val_acc= 0.52874 val_auc= 0.65589 time= 0.39587\n",
            "Epoch: 0011 train_loss= 0.69078 train_acc= 0.53827 train_auc= 0.63407 val_loss= 0.69138 val_acc= 0.54023 val_auc= 0.73298 time= 0.38178\n",
            "Epoch: 0012 train_loss= 0.68882 train_acc= 0.54592 train_auc= 0.68018 val_loss= 0.68930 val_acc= 0.54023 val_auc= 0.79043 time= 0.38253\n",
            "Epoch: 0012 train_loss= 0.68854 train_acc= 0.54278 train_auc= 0.68997 val_loss= 0.69221 val_acc= 0.53409 val_auc= 0.68915 time= 0.43153\n",
            "Epoch: 0012 train_loss= 0.68785 train_acc= 0.53954 train_auc= 0.71996 val_loss= 0.69326 val_acc= 0.52874 val_auc= 0.69141 time= 0.39375\n",
            "Epoch: 0012 train_loss= 0.68901 train_acc= 0.53827 train_auc= 0.69571 val_loss= 0.69129 val_acc= 0.54023 val_auc= 0.70213 time= 0.41222\n",
            "Epoch: 0013 train_loss= 0.68769 train_acc= 0.54082 train_auc= 0.72882 val_loss= 0.69310 val_acc= 0.54023 val_auc= 0.65372 time= 0.37608\n",
            "Epoch: 0012 train_loss= 0.68797 train_acc= 0.54209 train_auc= 0.68102 val_loss= 0.69197 val_acc= 0.54023 val_auc= 0.69734 time= 0.35214\n",
            "Epoch: 0012 train_loss= 0.68854 train_acc= 0.53827 train_auc= 0.71453 val_loss= 0.69213 val_acc= 0.54023 val_auc= 0.66064 time= 0.37038\n",
            "Epoch: 0013 train_loss= 0.68618 train_acc= 0.54209 train_auc= 0.75304 val_loss= 0.69366 val_acc= 0.52874 val_auc= 0.65960 time= 0.38044\n",
            "Epoch: 0013 train_loss= 0.68728 train_acc= 0.54719 train_auc= 0.71146 val_loss= 0.69024 val_acc= 0.54023 val_auc= 0.76436 time= 0.39553\n",
            "Epoch: 0012 train_loss= 0.68949 train_acc= 0.53699 train_auc= 0.69499 val_loss= 0.69145 val_acc= 0.54023 val_auc= 0.73670 time= 0.40801\n",
            "Epoch: 0013 train_loss= 0.68839 train_acc= 0.54337 train_auc= 0.70611 val_loss= 0.68908 val_acc= 0.54023 val_auc= 0.78723 time= 0.32497\n",
            "Epoch: 0013 train_loss= 0.68798 train_acc= 0.54406 train_auc= 0.70088 val_loss= 0.69236 val_acc= 0.53409 val_auc= 0.68085 time= 0.34842\n",
            "Epoch: 0013 train_loss= 0.68658 train_acc= 0.54337 train_auc= 0.75484 val_loss= 0.69337 val_acc= 0.52874 val_auc= 0.70573 time= 0.33769\n",
            "Epoch: 0013 train_loss= 0.68945 train_acc= 0.53954 train_auc= 0.68530 val_loss= 0.69139 val_acc= 0.54023 val_auc= 0.69415 time= 0.35701\n",
            "Epoch: 0014 train_loss= 0.68670 train_acc= 0.54719 train_auc= 0.74429 val_loss= 0.69342 val_acc= 0.54023 val_auc= 0.65372 time= 0.36488\n",
            "Epoch: 0013 train_loss= 0.68609 train_acc= 0.55230 train_auc= 0.73408 val_loss= 0.69228 val_acc= 0.54023 val_auc= 0.69840 time= 0.39773\n",
            "Epoch: 0013 train_loss= 0.68859 train_acc= 0.53827 train_auc= 0.72769 val_loss= 0.69215 val_acc= 0.54023 val_auc= 0.67660 time= 0.34485\n",
            "Epoch: 0014 train_loss= 0.68693 train_acc= 0.54337 train_auc= 0.73280 val_loss= 0.69399 val_acc= 0.52874 val_auc= 0.66278 time= 0.35395\n",
            "Epoch: 0013 train_loss= 0.68905 train_acc= 0.53699 train_auc= 0.71058 val_loss= 0.69135 val_acc= 0.54023 val_auc= 0.73138 time= 0.35184\n",
            "Epoch: 0014 train_loss= 0.68747 train_acc= 0.54974 train_auc= 0.70944 val_loss= 0.69023 val_acc= 0.54023 val_auc= 0.76383 time= 0.37412\n",
            "Epoch: 0014 train_loss= 0.68782 train_acc= 0.54337 train_auc= 0.71438 val_loss= 0.68891 val_acc= 0.54023 val_auc= 0.78617 time= 0.40264\n",
            "Epoch: 0014 train_loss= 0.68774 train_acc= 0.54662 train_auc= 0.69957 val_loss= 0.69250 val_acc= 0.53409 val_auc= 0.68345 time= 0.37303\n",
            "Epoch: 0014 train_loss= 0.68668 train_acc= 0.54592 train_auc= 0.74261 val_loss= 0.69362 val_acc= 0.52874 val_auc= 0.70520 time= 0.40418\n",
            "Epoch: 0015 train_loss= 0.68530 train_acc= 0.55612 train_auc= 0.77484 val_loss= 0.69379 val_acc= 0.54023 val_auc= 0.65160 time= 0.33130\n",
            "Epoch: 0014 train_loss= 0.68863 train_acc= 0.54464 train_auc= 0.70406 val_loss= 0.69145 val_acc= 0.54023 val_auc= 0.69521 time= 0.39262\n",
            "Epoch: 0014 train_loss= 0.68845 train_acc= 0.53954 train_auc= 0.72543 val_loss= 0.69227 val_acc= 0.54023 val_auc= 0.69096 time= 0.36307\n",
            "Epoch: 0014 train_loss= 0.68638 train_acc= 0.55995 train_auc= 0.72445 val_loss= 0.69255 val_acc= 0.54023 val_auc= 0.70000 time= 0.36653\n",
            "Epoch: 0015 train_loss= 0.68590 train_acc= 0.55230 train_auc= 0.74238 val_loss= 0.69023 val_acc= 0.54023 val_auc= 0.76011 time= 0.32405\n",
            "Epoch: 0015 train_loss= 0.68553 train_acc= 0.54082 train_auc= 0.77381 val_loss= 0.69427 val_acc= 0.52874 val_auc= 0.66384 time= 0.36960\n",
            "Epoch: 0014 train_loss= 0.68830 train_acc= 0.53699 train_auc= 0.73723 val_loss= 0.69139 val_acc= 0.54023 val_auc= 0.73564 time= 0.40498\n",
            "Epoch: 0015 train_loss= 0.68692 train_acc= 0.55230 train_auc= 0.73514 val_loss= 0.68870 val_acc= 0.54023 val_auc= 0.78617 time= 0.36159\n",
            "Epoch: 0015 train_loss= 0.68601 train_acc= 0.55683 train_auc= 0.73372 val_loss= 0.69271 val_acc= 0.53409 val_auc= 0.68241 time= 0.35545\n",
            "Epoch: 0015 train_loss= 0.68523 train_acc= 0.54974 train_auc= 0.77145 val_loss= 0.69361 val_acc= 0.52874 val_auc= 0.72428 time= 0.36586\n",
            "Epoch: 0016 train_loss= 0.68560 train_acc= 0.55867 train_auc= 0.77613 val_loss= 0.69415 val_acc= 0.54023 val_auc= 0.65106 time= 0.33196\n",
            "Epoch: 0015 train_loss= 0.68799 train_acc= 0.54974 train_auc= 0.71519 val_loss= 0.69149 val_acc= 0.54023 val_auc= 0.69309 time= 0.51418\n",
            "Epoch: 0016 train_loss= 0.68506 train_acc= 0.53954 train_auc= 0.80260 val_loss= 0.69446 val_acc= 0.54023 val_auc= 0.66755 time= 0.41358\n",
            "Epoch: 0015 train_loss= 0.68477 train_acc= 0.56633 train_auc= 0.76517 val_loss= 0.69294 val_acc= 0.54023 val_auc= 0.70266 time= 0.48083\n",
            "Epoch: 0015 train_loss= 0.68848 train_acc= 0.54337 train_auc= 0.72681 val_loss= 0.69218 val_acc= 0.54023 val_auc= 0.69521 time= 0.48662\n",
            "Epoch: 0016 train_loss= 0.68660 train_acc= 0.54719 train_auc= 0.74192 val_loss= 0.69047 val_acc= 0.54023 val_auc= 0.76117 time= 0.43667\n",
            "Epoch: 0015 train_loss= 0.68793 train_acc= 0.54082 train_auc= 0.73331 val_loss= 0.69143 val_acc= 0.54023 val_auc= 0.73670 time= 0.47283\n",
            "Epoch: 0016 train_loss= 0.68688 train_acc= 0.54592 train_auc= 0.74105 val_loss= 0.68852 val_acc= 0.54023 val_auc= 0.78670 time= 0.40658\n",
            "Epoch: 0016 train_loss= 0.68635 train_acc= 0.56833 train_auc= 0.73431 val_loss= 0.69292 val_acc= 0.53409 val_auc= 0.68085 time= 0.37094\n",
            "Epoch: 0016 train_loss= 0.68511 train_acc= 0.55485 train_auc= 0.77158 val_loss= 0.69369 val_acc= 0.52874 val_auc= 0.73383 time= 0.41208\n",
            "Epoch: 0017 train_loss= 0.68494 train_acc= 0.57653 train_auc= 0.75638 val_loss= 0.69435 val_acc= 0.54023 val_auc= 0.65319 time= 0.40860\n",
            "Epoch: 0016 train_loss= 0.68729 train_acc= 0.54719 train_auc= 0.73626 val_loss= 0.69159 val_acc= 0.54023 val_auc= 0.69043 time= 0.40265\n",
            "Epoch: 0017 train_loss= 0.68478 train_acc= 0.54974 train_auc= 0.76834 val_loss= 0.69469 val_acc= 0.54023 val_auc= 0.66331 time= 0.33479\n",
            "Epoch: 0016 train_loss= 0.68496 train_acc= 0.56378 train_auc= 0.76342 val_loss= 0.69331 val_acc= 0.54023 val_auc= 0.70585 time= 0.40885\n",
            "Epoch: 0016 train_loss= 0.68769 train_acc= 0.54847 train_auc= 0.73718 val_loss= 0.69207 val_acc= 0.54023 val_auc= 0.70266 time= 0.41613\n",
            "Epoch: 0017 train_loss= 0.68593 train_acc= 0.56888 train_auc= 0.73192 val_loss= 0.69048 val_acc= 0.54023 val_auc= 0.76277 time= 0.45437\n",
            "Epoch: 0016 train_loss= 0.68759 train_acc= 0.54209 train_auc= 0.73764 val_loss= 0.69154 val_acc= 0.54023 val_auc= 0.73032 time= 0.36092\n",
            "Epoch: 0017 train_loss= 0.68529 train_acc= 0.56760 train_auc= 0.74270 val_loss= 0.68823 val_acc= 0.54023 val_auc= 0.78404 time= 0.39415\n",
            "Epoch: 0017 train_loss= 0.68529 train_acc= 0.59004 train_auc= 0.72992 val_loss= 0.69295 val_acc= 0.53409 val_auc= 0.68189 time= 0.40420\n",
            "Epoch: 0017 train_loss= 0.68463 train_acc= 0.59439 train_auc= 0.74456 val_loss= 0.69392 val_acc= 0.52874 val_auc= 0.73171 time= 0.42346\n",
            "Epoch: 0018 train_loss= 0.68420 train_acc= 0.56888 train_auc= 0.76373 val_loss= 0.69453 val_acc= 0.54023 val_auc= 0.65851 time= 0.40106\n",
            "Epoch: 0017 train_loss= 0.68594 train_acc= 0.58163 train_auc= 0.72615 val_loss= 0.69152 val_acc= 0.54023 val_auc= 0.69309 time= 0.35653\n",
            "Epoch: 0018 train_loss= 0.68313 train_acc= 0.55740 train_auc= 0.78991 val_loss= 0.69501 val_acc= 0.54023 val_auc= 0.66331 time= 0.39882\n",
            "Epoch: 0017 train_loss= 0.68707 train_acc= 0.55357 train_auc= 0.72549 val_loss= 0.69195 val_acc= 0.54023 val_auc= 0.70372 time= 0.39848\n",
            "Epoch: 0017 train_loss= 0.68425 train_acc= 0.60459 train_auc= 0.74004 val_loss= 0.69342 val_acc= 0.54023 val_auc= 0.70904 time= 0.42085\n",
            "Epoch: 0018 train_loss= 0.68544 train_acc= 0.56633 train_auc= 0.74217 val_loss= 0.69011 val_acc= 0.54023 val_auc= 0.76277 time= 0.40314\n",
            "Epoch: 0018 train_loss= 0.68526 train_acc= 0.56633 train_auc= 0.75489 val_loss= 0.68795 val_acc= 0.54023 val_auc= 0.77872 time= 0.36918\n",
            "Epoch: 0017 train_loss= 0.68639 train_acc= 0.56760 train_auc= 0.73488 val_loss= 0.69148 val_acc= 0.54023 val_auc= 0.73457 time= 0.37681\n",
            "Epoch: 0018 train_loss= 0.68403 train_acc= 0.58365 train_auc= 0.75707 val_loss= 0.69293 val_acc= 0.53409 val_auc= 0.68708 time= 0.36610\n",
            "Epoch: 0018 train_loss= 0.68282 train_acc= 0.59056 train_auc= 0.76197 val_loss= 0.69432 val_acc= 0.52874 val_auc= 0.72110 time= 0.35560\n",
            "Epoch: 0019 train_loss= 0.68211 train_acc= 0.58418 train_auc= 0.78304 val_loss= 0.69476 val_acc= 0.54023 val_auc= 0.66170 time= 0.36232\n",
            "Epoch: 0018 train_loss= 0.68547 train_acc= 0.56505 train_auc= 0.73181 val_loss= 0.69137 val_acc= 0.54023 val_auc= 0.69628 time= 0.39483\n",
            "Epoch: 0019 train_loss= 0.68281 train_acc= 0.55230 train_auc= 0.78789 val_loss= 0.69523 val_acc= 0.54023 val_auc= 0.66278 time= 0.38926\n",
            "Epoch: 0018 train_loss= 0.68324 train_acc= 0.59439 train_auc= 0.75388 val_loss= 0.69349 val_acc= 0.54023 val_auc= 0.71011 time= 0.35323\n",
            "Epoch: 0019 train_loss= 0.68410 train_acc= 0.58929 train_auc= 0.75458 val_loss= 0.68984 val_acc= 0.54023 val_auc= 0.76489 time= 0.37005\n",
            "Epoch: 0018 train_loss= 0.68604 train_acc= 0.55485 train_auc= 0.74703 val_loss= 0.69179 val_acc= 0.54023 val_auc= 0.70319 time= 0.44598\n",
            "Epoch: 0018 train_loss= 0.68566 train_acc= 0.56505 train_auc= 0.74474 val_loss= 0.69135 val_acc= 0.54023 val_auc= 0.74415 time= 0.39475\n",
            "Epoch: 0019 train_loss= 0.68406 train_acc= 0.57270 train_auc= 0.76367 val_loss= 0.68767 val_acc= 0.54023 val_auc= 0.77660 time= 0.41609\n",
            "Epoch: 0019 train_loss= 0.68364 train_acc= 0.58238 train_auc= 0.75811 val_loss= 0.69287 val_acc= 0.53409 val_auc= 0.69019 time= 0.42063\n",
            "Epoch: 0020 train_loss= 0.68322 train_acc= 0.58036 train_auc= 0.74287 val_loss= 0.69498 val_acc= 0.54023 val_auc= 0.66702 time= 0.37265\n",
            "Epoch: 0019 train_loss= 0.68267 train_acc= 0.60077 train_auc= 0.75638 val_loss= 0.69487 val_acc= 0.52874 val_auc= 0.70838 time= 0.40560\n",
            "Epoch: 0019 train_loss= 0.68416 train_acc= 0.58801 train_auc= 0.74684 val_loss= 0.69115 val_acc= 0.54023 val_auc= 0.70160 time= 0.42231\n",
            "Epoch: 0019 train_loss= 0.68305 train_acc= 0.60077 train_auc= 0.74863 val_loss= 0.69345 val_acc= 0.54023 val_auc= 0.71223 time= 0.40179\n",
            "Epoch: 0020 train_loss= 0.68150 train_acc= 0.57015 train_auc= 0.77641 val_loss= 0.69535 val_acc= 0.54023 val_auc= 0.66490 time= 0.44290\n",
            "Epoch: 0020 train_loss= 0.68283 train_acc= 0.58418 train_auc= 0.76238 val_loss= 0.68961 val_acc= 0.54023 val_auc= 0.76489 time= 0.38693\n",
            "Epoch: 0019 train_loss= 0.68422 train_acc= 0.55612 train_auc= 0.75252 val_loss= 0.69170 val_acc= 0.54023 val_auc= 0.70372 time= 0.38791\n",
            "Epoch: 0020 train_loss= 0.68375 train_acc= 0.58036 train_auc= 0.75534 val_loss= 0.68736 val_acc= 0.54023 val_auc= 0.77500 time= 0.38295\n",
            "Epoch: 0019 train_loss= 0.68524 train_acc= 0.57143 train_auc= 0.73593 val_loss= 0.69119 val_acc= 0.54023 val_auc= 0.75426 time= 0.43146\n",
            "Epoch: 0020 train_loss= 0.68265 train_acc= 0.58110 train_auc= 0.76071 val_loss= 0.69282 val_acc= 0.53409 val_auc= 0.69279 time= 0.37056\n",
            "Epoch: 0020 train_loss= 0.68305 train_acc= 0.58929 train_auc= 0.73312 val_loss= 0.69533 val_acc= 0.52874 val_auc= 0.69724 time= 0.38309\n",
            "Epoch: 0021 train_loss= 0.68100 train_acc= 0.57908 train_auc= 0.76586 val_loss= 0.69524 val_acc= 0.54023 val_auc= 0.66596 time= 0.38712\n",
            "Epoch: 0020 train_loss= 0.68435 train_acc= 0.57270 train_auc= 0.73308 val_loss= 0.69094 val_acc= 0.54023 val_auc= 0.70213 time= 0.39155\n",
            "Epoch: 0020 train_loss= 0.68184 train_acc= 0.60969 train_auc= 0.74928 val_loss= 0.69343 val_acc= 0.54023 val_auc= 0.71596 time= 0.34403\n",
            "Epoch: 0021 train_loss= 0.68093 train_acc= 0.54974 train_auc= 0.77722 val_loss= 0.69535 val_acc= 0.54023 val_auc= 0.66755 time= 0.35770\n",
            "Epoch: 0021 train_loss= 0.68175 train_acc= 0.57781 train_auc= 0.76908 val_loss= 0.68936 val_acc= 0.56322 val_auc= 0.76649 time= 0.39125\n",
            "Epoch: 0020 train_loss= 0.68492 train_acc= 0.56378 train_auc= 0.72804 val_loss= 0.69102 val_acc= 0.54023 val_auc= 0.75798 time= 0.36818\n",
            "Epoch: 0021 train_loss= 0.68301 train_acc= 0.57653 train_auc= 0.76008 val_loss= 0.68701 val_acc= 0.54023 val_auc= 0.77713 time= 0.36620\n",
            "Epoch: 0020 train_loss= 0.68470 train_acc= 0.56378 train_auc= 0.73447 val_loss= 0.69156 val_acc= 0.55172 val_auc= 0.70851 time= 0.40782\n",
            "Epoch: 0021 train_loss= 0.68201 train_acc= 0.57982 train_auc= 0.77097 val_loss= 0.69273 val_acc= 0.53409 val_auc= 0.69538 time= 0.37309\n",
            "Epoch: 0021 train_loss= 0.67941 train_acc= 0.58418 train_auc= 0.77044 val_loss= 0.69561 val_acc= 0.54023 val_auc= 0.69194 time= 0.34868\n",
            "Epoch: 0022 train_loss= 0.68179 train_acc= 0.59439 train_auc= 0.74888 val_loss= 0.69558 val_acc= 0.54023 val_auc= 0.66277 time= 0.39971\n",
            "Epoch: 0021 train_loss= 0.68201 train_acc= 0.57398 train_auc= 0.75434 val_loss= 0.69080 val_acc= 0.55172 val_auc= 0.70319 time= 0.37821\n",
            "Epoch: 0021 train_loss= 0.67971 train_acc= 0.59439 train_auc= 0.77106 val_loss= 0.69348 val_acc= 0.54023 val_auc= 0.71755 time= 0.35268\n",
            "Epoch: 0022 train_loss= 0.68091 train_acc= 0.58418 train_auc= 0.76150 val_loss= 0.69504 val_acc= 0.55172 val_auc= 0.66702 time= 0.40269\n",
            "Epoch: 0022 train_loss= 0.68149 train_acc= 0.58673 train_auc= 0.75327 val_loss= 0.68915 val_acc= 0.57471 val_auc= 0.76755 time= 0.37588\n",
            "Epoch: 0022 train_loss= 0.68187 train_acc= 0.59056 train_auc= 0.76667 val_loss= 0.68655 val_acc= 0.56322 val_auc= 0.77500 time= 0.36790\n",
            "Epoch: 0021 train_loss= 0.68483 train_acc= 0.56505 train_auc= 0.71998 val_loss= 0.69088 val_acc= 0.54023 val_auc= 0.75904 time= 0.38641\n",
            "Epoch: 0022 train_loss= 0.68263 train_acc= 0.58876 train_auc= 0.74782 val_loss= 0.69261 val_acc= 0.55682 val_auc= 0.69590 time= 0.37300\n",
            "Epoch: 0021 train_loss= 0.68307 train_acc= 0.55740 train_auc= 0.75260 val_loss= 0.69138 val_acc= 0.56322 val_auc= 0.71436 time= 0.40993\n",
            "Epoch: 0022 train_loss= 0.67995 train_acc= 0.58163 train_auc= 0.75674 val_loss= 0.69538 val_acc= 0.54023 val_auc= 0.70944 time= 0.35567\n",
            "Epoch: 0023 train_loss= 0.68121 train_acc= 0.60459 train_auc= 0.75232 val_loss= 0.69598 val_acc= 0.56322 val_auc= 0.66117 time= 0.40161\n",
            "Epoch: 0022 train_loss= 0.68084 train_acc= 0.59184 train_auc= 0.75154 val_loss= 0.69376 val_acc= 0.54023 val_auc= 0.71263 time= 0.37808\n",
            "Epoch: 0022 train_loss= 0.68187 train_acc= 0.58801 train_auc= 0.75077 val_loss= 0.69076 val_acc= 0.55172 val_auc= 0.70266 time= 0.37805\n",
            "Epoch: 0023 train_loss= 0.68119 train_acc= 0.59821 train_auc= 0.74533 val_loss= 0.69476 val_acc= 0.55172 val_auc= 0.67550 time= 0.39788\n",
            "Epoch: 0023 train_loss= 0.68143 train_acc= 0.57908 train_auc= 0.74825 val_loss= 0.68895 val_acc= 0.57471 val_auc= 0.77128 time= 0.41909\n",
            "Epoch: 0022 train_loss= 0.68269 train_acc= 0.56122 train_auc= 0.74523 val_loss= 0.69064 val_acc= 0.54023 val_auc= 0.76170 time= 0.37240\n",
            "Epoch: 0023 train_loss= 0.68179 train_acc= 0.60459 train_auc= 0.76129 val_loss= 0.68612 val_acc= 0.57471 val_auc= 0.77606 time= 0.39718\n",
            "Epoch: 0023 train_loss= 0.68091 train_acc= 0.60153 train_auc= 0.76256 val_loss= 0.69256 val_acc= 0.55682 val_auc= 0.69331 time= 0.38104\n",
            "Epoch: 0022 train_loss= 0.68299 train_acc= 0.57270 train_auc= 0.74970 val_loss= 0.69125 val_acc= 0.57471 val_auc= 0.72021 time= 0.40009\n",
            "Epoch: 0023 train_loss= 0.68145 train_acc= 0.58163 train_auc= 0.73916 val_loss= 0.69494 val_acc= 0.55172 val_auc= 0.71898 time= 0.40755\n",
            "Epoch: 0023 train_loss= 0.68291 train_acc= 0.57653 train_auc= 0.73134 val_loss= 0.69082 val_acc= 0.57471 val_auc= 0.70213 time= 0.37613\n",
            "Epoch: 0024 train_loss= 0.68036 train_acc= 0.63903 train_auc= 0.75809 val_loss= 0.69622 val_acc= 0.56322 val_auc= 0.66064 time= 0.43524\n",
            "Epoch: 0024 train_loss= 0.67982 train_acc= 0.60714 train_auc= 0.76254 val_loss= 0.69455 val_acc= 0.54023 val_auc= 0.67603 time= 0.37685\n",
            "Epoch: 0023 train_loss= 0.68051 train_acc= 0.60077 train_auc= 0.74835 val_loss= 0.69419 val_acc= 0.54023 val_auc= 0.71117 time= 0.48730\n",
            "Epoch: 0024 train_loss= 0.67959 train_acc= 0.63393 train_auc= 0.78076 val_loss= 0.68579 val_acc= 0.59770 val_auc= 0.77660 time= 0.37629\n",
            "Epoch: 0024 train_loss= 0.68092 train_acc= 0.60714 train_auc= 0.75048 val_loss= 0.68866 val_acc= 0.63218 val_auc= 0.77234 time= 0.40480\n",
            "Epoch: 0023 train_loss= 0.68331 train_acc= 0.57270 train_auc= 0.72711 val_loss= 0.69042 val_acc= 0.54023 val_auc= 0.75904 time= 0.41508\n",
            "Epoch: 0023 train_loss= 0.68373 train_acc= 0.58418 train_auc= 0.73961 val_loss= 0.69115 val_acc= 0.58621 val_auc= 0.72394 time= 0.37824\n",
            "Epoch: 0024 train_loss= 0.67971 train_acc= 0.61941 train_auc= 0.77378 val_loss= 0.69246 val_acc= 0.56818 val_auc= 0.69486 time= 0.43607\n",
            "Epoch: 0024 train_loss= 0.67865 train_acc= 0.61862 train_auc= 0.76731 val_loss= 0.69464 val_acc= 0.54023 val_auc= 0.72694 time= 0.44433\n",
            "Epoch: 0025 train_loss= 0.68182 train_acc= 0.63903 train_auc= 0.73813 val_loss= 0.69620 val_acc= 0.57471 val_auc= 0.66277 time= 0.37023\n",
            "Epoch: 0024 train_loss= 0.68136 train_acc= 0.62245 train_auc= 0.74152 val_loss= 0.69085 val_acc= 0.60920 val_auc= 0.70106 time= 0.41451\n",
            "Epoch: 0024 train_loss= 0.67889 train_acc= 0.63776 train_auc= 0.76002 val_loss= 0.69457 val_acc= 0.57471 val_auc= 0.71170 time= 0.34383\n",
            "Epoch: 0025 train_loss= 0.68047 train_acc= 0.63903 train_auc= 0.74430 val_loss= 0.69444 val_acc= 0.54023 val_auc= 0.67497 time= 0.41595\n",
            "Epoch: 0025 train_loss= 0.68243 train_acc= 0.62755 train_auc= 0.72948 val_loss= 0.68829 val_acc= 0.64368 val_auc= 0.77181 time= 0.37791\n",
            "Epoch: 0025 train_loss= 0.68119 train_acc= 0.63903 train_auc= 0.76311 val_loss= 0.68544 val_acc= 0.59770 val_auc= 0.77394 time= 0.41212\n",
            "Epoch: 0024 train_loss= 0.68153 train_acc= 0.60459 train_auc= 0.74709 val_loss= 0.69020 val_acc= 0.54023 val_auc= 0.75585 time= 0.37861\n",
            "Epoch: 0024 train_loss= 0.68243 train_acc= 0.61352 train_auc= 0.75169 val_loss= 0.69113 val_acc= 0.58621 val_auc= 0.72340 time= 0.36437\n",
            "Epoch: 0025 train_loss= 0.68163 train_acc= 0.64368 train_auc= 0.75475 val_loss= 0.69233 val_acc= 0.57955 val_auc= 0.69798 time= 0.44797\n",
            "Epoch: 0026 train_loss= 0.68008 train_acc= 0.65051 train_auc= 0.74987 val_loss= 0.69618 val_acc= 0.56322 val_auc= 0.66915 time= 0.34698\n",
            "Epoch: 0025 train_loss= 0.67971 train_acc= 0.64541 train_auc= 0.76451 val_loss= 0.69446 val_acc= 0.54023 val_auc= 0.72641 time= 0.43350\n",
            "Epoch: 0025 train_loss= 0.68161 train_acc= 0.63903 train_auc= 0.74190 val_loss= 0.69073 val_acc= 0.60920 val_auc= 0.70000 time= 0.41246\n",
            "Epoch: 0026 train_loss= 0.68157 train_acc= 0.64158 train_auc= 0.73462 val_loss= 0.69443 val_acc= 0.52874 val_auc= 0.67444 time= 0.36521\n",
            "Epoch: 0025 train_loss= 0.68025 train_acc= 0.65561 train_auc= 0.74713 val_loss= 0.69455 val_acc= 0.58621 val_auc= 0.70904 time= 0.41145\n",
            "Epoch: 0026 train_loss= 0.68155 train_acc= 0.63520 train_auc= 0.72803 val_loss= 0.68794 val_acc= 0.64368 val_auc= 0.77181 time= 0.41730\n",
            "Epoch: 0026 train_loss= 0.68200 train_acc= 0.63010 train_auc= 0.73821 val_loss= 0.68479 val_acc= 0.59770 val_auc= 0.77606 time= 0.43821\n",
            "Epoch: 0025 train_loss= 0.68298 train_acc= 0.62117 train_auc= 0.73499 val_loss= 0.68989 val_acc= 0.55172 val_auc= 0.75798 time= 0.44015\n",
            "Epoch: 0025 train_loss= 0.68180 train_acc= 0.63903 train_auc= 0.75776 val_loss= 0.69108 val_acc= 0.56322 val_auc= 0.71968 time= 0.38972\n",
            "Epoch: 0026 train_loss= 0.67944 train_acc= 0.65390 train_auc= 0.76011 val_loss= 0.69208 val_acc= 0.56818 val_auc= 0.70265 time= 0.38117\n",
            "Epoch: 0026 train_loss= 0.67990 train_acc= 0.66709 train_auc= 0.75458 val_loss= 0.69447 val_acc= 0.54023 val_auc= 0.72428 time= 0.35382\n",
            "Epoch: 0027 train_loss= 0.67761 train_acc= 0.64668 train_auc= 0.77588 val_loss= 0.69613 val_acc= 0.55172 val_auc= 0.66862 time= 0.39295\n",
            "Epoch: 0026 train_loss= 0.68170 train_acc= 0.65434 train_auc= 0.73857 val_loss= 0.69035 val_acc= 0.60920 val_auc= 0.70691 time= 0.36920\n",
            "Epoch: 0026 train_loss= 0.68029 train_acc= 0.64158 train_auc= 0.74317 val_loss= 0.69409 val_acc= 0.57471 val_auc= 0.71330 time= 0.38290\n",
            "Epoch: 0027 train_loss= 0.67636 train_acc= 0.65561 train_auc= 0.78719 val_loss= 0.69446 val_acc= 0.54023 val_auc= 0.67338 time= 0.43647\n",
            "Epoch: 0027 train_loss= 0.67803 train_acc= 0.64031 train_auc= 0.76581 val_loss= 0.68793 val_acc= 0.64368 val_auc= 0.77181 time= 0.38860\n",
            "Epoch: 0027 train_loss= 0.67799 train_acc= 0.63903 train_auc= 0.78413 val_loss= 0.68433 val_acc= 0.59770 val_auc= 0.77128 time= 0.40237\n",
            "Epoch: 0026 train_loss= 0.68325 train_acc= 0.63776 train_auc= 0.72952 val_loss= 0.68949 val_acc= 0.55172 val_auc= 0.75798 time= 0.41695\n",
            "Epoch: 0026 train_loss= 0.68166 train_acc= 0.63520 train_auc= 0.75341 val_loss= 0.69101 val_acc= 0.56322 val_auc= 0.72021 time= 0.42735\n",
            "Epoch: 0027 train_loss= 0.67638 train_acc= 0.65134 train_auc= 0.78980 val_loss= 0.69184 val_acc= 0.56818 val_auc= 0.70628 time= 0.39329\n",
            "Epoch: 0027 train_loss= 0.67524 train_acc= 0.66837 train_auc= 0.79752 val_loss= 0.69470 val_acc= 0.54023 val_auc= 0.72481 time= 0.39369\n",
            "Epoch: 0028 train_loss= 0.67833 train_acc= 0.64031 train_auc= 0.77390 val_loss= 0.69631 val_acc= 0.55172 val_auc= 0.66915 time= 0.40841\n",
            "Epoch: 0027 train_loss= 0.67720 train_acc= 0.65179 train_auc= 0.77997 val_loss= 0.68990 val_acc= 0.60920 val_auc= 0.70904 time= 0.43467\n",
            "Epoch: 0028 train_loss= 0.67563 train_acc= 0.64923 train_auc= 0.80204 val_loss= 0.69445 val_acc= 0.57471 val_auc= 0.67444 time= 0.39405\n",
            "Epoch: 0027 train_loss= 0.67543 train_acc= 0.67602 train_auc= 0.78416 val_loss= 0.69366 val_acc= 0.56322 val_auc= 0.71809 time= 0.43850\n",
            "Epoch: 0028 train_loss= 0.67911 train_acc= 0.63393 train_auc= 0.76715 val_loss= 0.68788 val_acc= 0.64368 val_auc= 0.77128 time= 0.47354\n",
            "Epoch: 0028 train_loss= 0.67785 train_acc= 0.63776 train_auc= 0.79013 val_loss= 0.68391 val_acc= 0.63218 val_auc= 0.77606 time= 0.41010\n",
            "Epoch: 0028 train_loss= 0.67799 train_acc= 0.63729 train_auc= 0.77356 val_loss= 0.69162 val_acc= 0.55682 val_auc= 0.70472 time= 0.36633\n",
            "Epoch: 0027 train_loss= 0.67807 train_acc= 0.63776 train_auc= 0.79014 val_loss= 0.68912 val_acc= 0.55172 val_auc= 0.76223 time= 0.44759\n",
            "Epoch: 0027 train_loss= 0.67759 train_acc= 0.64796 train_auc= 0.79406 val_loss= 0.69104 val_acc= 0.56322 val_auc= 0.71543 time= 0.43215\n",
            "Epoch: 0028 train_loss= 0.67666 train_acc= 0.64923 train_auc= 0.79478 val_loss= 0.69471 val_acc= 0.55172 val_auc= 0.72428 time= 0.38993\n",
            "Epoch: 0029 train_loss= 0.67658 train_acc= 0.63903 train_auc= 0.80087 val_loss= 0.69636 val_acc= 0.56322 val_auc= 0.67074 time= 0.35379\n",
            "Epoch: 0028 train_loss= 0.67903 train_acc= 0.63393 train_auc= 0.76148 val_loss= 0.68957 val_acc= 0.60920 val_auc= 0.71011 time= 0.44872\n",
            "Epoch: 0028 train_loss= 0.67652 train_acc= 0.65434 train_auc= 0.77354 val_loss= 0.69360 val_acc= 0.56322 val_auc= 0.72181 time= 0.44022\n",
            "Epoch: 0029 train_loss= 0.67581 train_acc= 0.67092 train_auc= 0.80432 val_loss= 0.69455 val_acc= 0.58621 val_auc= 0.67869 time= 0.46191\n",
            "Epoch: 0029 train_loss= 0.67655 train_acc= 0.67219 train_auc= 0.79835 val_loss= 0.68335 val_acc= 0.62069 val_auc= 0.77553 time= 0.43025\n",
            "Epoch: 0028 train_loss= 0.67910 train_acc= 0.63520 train_auc= 0.77306 val_loss= 0.69126 val_acc= 0.56322 val_auc= 0.70904 time= 0.36768\n",
            "Epoch: 0029 train_loss= 0.67691 train_acc= 0.65645 train_auc= 0.78855 val_loss= 0.69140 val_acc= 0.60227 val_auc= 0.70680 time= 0.42550\n",
            "Epoch: 0029 train_loss= 0.67755 train_acc= 0.65434 train_auc= 0.78296 val_loss= 0.68698 val_acc= 0.64368 val_auc= 0.77234 time= 0.45252\n",
            "Epoch: 0028 train_loss= 0.67874 train_acc= 0.63393 train_auc= 0.77829 val_loss= 0.68872 val_acc= 0.55172 val_auc= 0.76117 time= 0.41324\n",
            "Epoch: 0029 train_loss= 0.67619 train_acc= 0.66327 train_auc= 0.80225 val_loss= 0.69473 val_acc= 0.55172 val_auc= 0.72641 time= 0.45203\n",
            "Epoch: 0030 train_loss= 0.67598 train_acc= 0.64796 train_auc= 0.79978 val_loss= 0.69660 val_acc= 0.57471 val_auc= 0.66862 time= 0.44414\n",
            "Epoch: 0029 train_loss= 0.67482 train_acc= 0.66709 train_auc= 0.79906 val_loss= 0.69368 val_acc= 0.58621 val_auc= 0.72500 time= 0.37388\n",
            "Epoch: 0030 train_loss= 0.67515 train_acc= 0.67092 train_auc= 0.80903 val_loss= 0.69446 val_acc= 0.59770 val_auc= 0.68187 time= 0.39451\n",
            "Epoch: 0029 train_loss= 0.67743 train_acc= 0.64796 train_auc= 0.78699 val_loss= 0.68950 val_acc= 0.60920 val_auc= 0.71330 time= 0.47915\n",
            "Epoch: 0030 train_loss= 0.67718 train_acc= 0.64923 train_auc= 0.79165 val_loss= 0.68284 val_acc= 0.63218 val_auc= 0.77660 time= 0.40128\n",
            "Epoch: 0030 train_loss= 0.67691 train_acc= 0.64668 train_auc= 0.78860 val_loss= 0.68661 val_acc= 0.65517 val_auc= 0.77234 time= 0.38920\n",
            "Epoch: 0029 train_loss= 0.67764 train_acc= 0.62500 train_auc= 0.79737 val_loss= 0.69125 val_acc= 0.56322 val_auc= 0.70957 time= 0.43887\n",
            "Epoch: 0029 train_loss= 0.67738 train_acc= 0.65434 train_auc= 0.80945 val_loss= 0.68840 val_acc= 0.55172 val_auc= 0.76170 time= 0.38362\n",
            "Epoch: 0030 train_loss= 0.67695 train_acc= 0.64240 train_auc= 0.77378 val_loss= 0.69120 val_acc= 0.60227 val_auc= 0.70732 time= 0.40609\n",
            "Epoch: 0030 train_loss= 0.67509 train_acc= 0.65944 train_auc= 0.80158 val_loss= 0.69476 val_acc= 0.55172 val_auc= 0.72534 time= 0.40183\n",
            "Epoch: 0031 train_loss= 0.67218 train_acc= 0.69770 train_auc= 0.82162 val_loss= 0.69697 val_acc= 0.58621 val_auc= 0.66809 time= 0.43259\n",
            "Epoch: 0030 train_loss= 0.67473 train_acc= 0.64286 train_auc= 0.79620 val_loss= 0.69379 val_acc= 0.60920 val_auc= 0.72660 time= 0.44703\n",
            "Epoch: 0030 train_loss= 0.67711 train_acc= 0.64413 train_auc= 0.77983 val_loss= 0.68927 val_acc= 0.63218 val_auc= 0.71383 time= 0.42289\n",
            "Epoch: 0031 train_loss= 0.67356 train_acc= 0.68112 train_auc= 0.81056 val_loss= 0.69422 val_acc= 0.59770 val_auc= 0.68240 time= 0.44324\n",
            "Epoch: 0031 train_loss= 0.67453 train_acc= 0.69388 train_auc= 0.79793 val_loss= 0.68244 val_acc= 0.65517 val_auc= 0.77819 time= 0.39846\n",
            "Epoch: 0031 train_loss= 0.67135 train_acc= 0.69005 train_auc= 0.81373 val_loss= 0.68644 val_acc= 0.65517 val_auc= 0.77287 time= 0.41584\n",
            "Epoch: 0031 train_loss= 0.67181 train_acc= 0.66794 train_auc= 0.79698 val_loss= 0.69113 val_acc= 0.63636 val_auc= 0.70576 time= 0.41070\n",
            "Epoch: 0030 train_loss= 0.67798 train_acc= 0.60077 train_auc= 0.78137 val_loss= 0.69110 val_acc= 0.56322 val_auc= 0.71223 time= 0.45884\n",
            "Epoch: 0030 train_loss= 0.67936 train_acc= 0.62628 train_auc= 0.78548 val_loss= 0.68789 val_acc= 0.56322 val_auc= 0.76489 time= 0.43721\n",
            "Epoch: 0031 train_loss= 0.66998 train_acc= 0.70153 train_auc= 0.83005 val_loss= 0.69488 val_acc= 0.57471 val_auc= 0.72428 time= 0.61150\n",
            "Epoch: 0031 train_loss= 0.67514 train_acc= 0.67474 train_auc= 0.79286 val_loss= 0.68911 val_acc= 0.60920 val_auc= 0.71436 time= 0.42208\n",
            "Epoch: 0031 train_loss= 0.67154 train_acc= 0.70026 train_auc= 0.80903 val_loss= 0.69410 val_acc= 0.63218 val_auc= 0.72766 time= 0.44745\n",
            "Epoch: 0032 train_loss= 0.67235 train_acc= 0.69770 train_auc= 0.80547 val_loss= 0.69708 val_acc= 0.58621 val_auc= 0.67181 time= 0.57433\n",
            "Epoch: 0032 train_loss= 0.66968 train_acc= 0.69005 train_auc= 0.82608 val_loss= 0.69413 val_acc= 0.59770 val_auc= 0.68293 time= 0.42294\n",
            "Epoch: 0032 train_loss= 0.67216 train_acc= 0.70153 train_auc= 0.80880 val_loss= 0.68198 val_acc= 0.63218 val_auc= 0.77713 time= 0.41842\n",
            "Epoch: 0032 train_loss= 0.67161 train_acc= 0.70918 train_auc= 0.80002 val_loss= 0.68622 val_acc= 0.65517 val_auc= 0.77181 time= 0.45550\n",
            "Epoch: 0032 train_loss= 0.67028 train_acc= 0.71264 train_auc= 0.80621 val_loss= 0.69084 val_acc= 0.61364 val_auc= 0.70887 time= 0.41333\n",
            "Epoch: 0031 train_loss= 0.67429 train_acc= 0.67092 train_auc= 0.80631 val_loss= 0.68754 val_acc= 0.58621 val_auc= 0.76755 time= 0.40636\n",
            "Epoch: 0031 train_loss= 0.67575 train_acc= 0.64158 train_auc= 0.78906 val_loss= 0.69103 val_acc= 0.56322 val_auc= 0.71489 time= 0.45279\n",
            "Epoch: 0032 train_loss= 0.66834 train_acc= 0.71301 train_auc= 0.82229 val_loss= 0.69533 val_acc= 0.57471 val_auc= 0.70891 time= 0.36983\n",
            "Epoch: 0032 train_loss= 0.67014 train_acc= 0.71046 train_auc= 0.82212 val_loss= 0.69410 val_acc= 0.63218 val_auc= 0.72766 time= 0.47760\n",
            "Epoch: 0033 train_loss= 0.67461 train_acc= 0.66454 train_auc= 0.77232 val_loss= 0.69701 val_acc= 0.58621 val_auc= 0.67234 time= 0.45226\n",
            "Epoch: 0032 train_loss= 0.67339 train_acc= 0.69515 train_auc= 0.80265 val_loss= 0.68887 val_acc= 0.60920 val_auc= 0.71383 time= 0.50640\n",
            "Epoch: 0033 train_loss= 0.67463 train_acc= 0.67602 train_auc= 0.77439 val_loss= 0.69422 val_acc= 0.57471 val_auc= 0.68134 time= 0.53990\n",
            "Epoch: 0033 train_loss= 0.67687 train_acc= 0.67219 train_auc= 0.75845 val_loss= 0.68151 val_acc= 0.62069 val_auc= 0.77819 time= 0.48906\n",
            "Epoch: 0033 train_loss= 0.67405 train_acc= 0.67857 train_auc= 0.77566 val_loss= 0.68580 val_acc= 0.65517 val_auc= 0.77287 time= 0.45672\n",
            "Epoch: 0032 train_loss= 0.67406 train_acc= 0.67602 train_auc= 0.80576 val_loss= 0.68748 val_acc= 0.60920 val_auc= 0.76862 time= 0.43934\n",
            "Epoch: 0033 train_loss= 0.67439 train_acc= 0.65773 train_auc= 0.76759 val_loss= 0.69050 val_acc= 0.61364 val_auc= 0.71147 time= 0.47807\n",
            "Epoch: 0032 train_loss= 0.67252 train_acc= 0.67602 train_auc= 0.80572 val_loss= 0.69114 val_acc= 0.57471 val_auc= 0.71489 time= 0.47584\n",
            "Epoch: 0033 train_loss= 0.67381 train_acc= 0.67347 train_auc= 0.78323 val_loss= 0.69389 val_acc= 0.63218 val_auc= 0.72660 time= 0.39462\n",
            "Epoch: 0033 train_loss= 0.67316 train_acc= 0.67092 train_auc= 0.77282 val_loss= 0.69579 val_acc= 0.56322 val_auc= 0.70573 time= 0.50569\n",
            "Epoch: 0034 train_loss= 0.66818 train_acc= 0.69643 train_auc= 0.80789 val_loss= 0.69702 val_acc= 0.57471 val_auc= 0.67181 time= 0.42636\n",
            "Epoch: 0033 train_loss= 0.67540 train_acc= 0.67219 train_auc= 0.77810 val_loss= 0.68846 val_acc= 0.60920 val_auc= 0.71915 time= 0.44013\n",
            "Epoch: 0034 train_loss= 0.67337 train_acc= 0.67985 train_auc= 0.78219 val_loss= 0.68110 val_acc= 0.63218 val_auc= 0.77766 time= 0.41350\n",
            "Epoch: 0034 train_loss= 0.66738 train_acc= 0.69133 train_auc= 0.81562 val_loss= 0.69438 val_acc= 0.57471 val_auc= 0.68134 time= 0.43610\n",
            "Epoch: 0034 train_loss= 0.66924 train_acc= 0.69221 train_auc= 0.79737 val_loss= 0.69019 val_acc= 0.61364 val_auc= 0.71718 time= 0.41651\n",
            "Epoch: 0034 train_loss= 0.66938 train_acc= 0.69005 train_auc= 0.79974 val_loss= 0.68550 val_acc= 0.64368 val_auc= 0.77181 time= 0.44233\n",
            "Epoch: 0033 train_loss= 0.67736 train_acc= 0.63776 train_auc= 0.74883 val_loss= 0.69129 val_acc= 0.57471 val_auc= 0.71223 time= 0.39063\n",
            "Epoch: 0033 train_loss= 0.67520 train_acc= 0.66454 train_auc= 0.78092 val_loss= 0.68713 val_acc= 0.62069 val_auc= 0.77074 time= 0.43948\n",
            "Epoch: 0034 train_loss= 0.66680 train_acc= 0.69133 train_auc= 0.80146 val_loss= 0.69627 val_acc= 0.56322 val_auc= 0.70148 time= 0.37072\n",
            "Epoch: 0034 train_loss= 0.66800 train_acc= 0.68112 train_auc= 0.81490 val_loss= 0.69363 val_acc= 0.60920 val_auc= 0.72766 time= 0.39865\n",
            "Epoch: 0034 train_loss= 0.67012 train_acc= 0.71429 train_auc= 0.81726 val_loss= 0.68814 val_acc= 0.62069 val_auc= 0.72021 time= 0.44345\n",
            "Epoch: 0035 train_loss= 0.66960 train_acc= 0.68112 train_auc= 0.78718 val_loss= 0.69715 val_acc= 0.58621 val_auc= 0.66809 time= 0.50300\n",
            "Epoch: 0035 train_loss= 0.66998 train_acc= 0.68878 train_auc= 0.79934 val_loss= 0.68073 val_acc= 0.63218 val_auc= 0.77766 time= 0.43414\n",
            "Epoch: 0035 train_loss= 0.66869 train_acc= 0.66837 train_auc= 0.80482 val_loss= 0.69439 val_acc= 0.59770 val_auc= 0.68293 time= 0.44709\n",
            "Epoch: 0034 train_loss= 0.67106 train_acc= 0.68367 train_auc= 0.79605 val_loss= 0.68694 val_acc= 0.63218 val_auc= 0.77340 time= 0.35115\n",
            "Epoch: 0034 train_loss= 0.67107 train_acc= 0.67730 train_auc= 0.79132 val_loss= 0.69137 val_acc= 0.57471 val_auc= 0.71117 time= 0.40372\n",
            "Epoch: 0035 train_loss= 0.67093 train_acc= 0.67050 train_auc= 0.77763 val_loss= 0.69003 val_acc= 0.61364 val_auc= 0.72289 time= 0.43124\n",
            "Epoch: 0035 train_loss= 0.67062 train_acc= 0.67730 train_auc= 0.77374 val_loss= 0.68524 val_acc= 0.65517 val_auc= 0.77074 time= 0.46277\n",
            "Epoch: 0035 train_loss= 0.66969 train_acc= 0.68240 train_auc= 0.77329 val_loss= 0.69665 val_acc= 0.56322 val_auc= 0.69353 time= 0.40113\n",
            "Epoch: 0035 train_loss= 0.66853 train_acc= 0.68367 train_auc= 0.79994 val_loss= 0.69337 val_acc= 0.58621 val_auc= 0.72819 time= 0.40588\n",
            "Epoch: 0035 train_loss= 0.67099 train_acc= 0.69260 train_auc= 0.79575 val_loss= 0.68787 val_acc= 0.63218 val_auc= 0.72021 time= 0.43710\n",
            "Epoch: 0036 train_loss= 0.67072 train_acc= 0.67474 train_auc= 0.78513 val_loss= 0.69724 val_acc= 0.57471 val_auc= 0.67128 time= 0.44207\n",
            "Epoch: 0036 train_loss= 0.67116 train_acc= 0.67730 train_auc= 0.79171 val_loss= 0.68038 val_acc= 0.64368 val_auc= 0.77447 time= 0.41771\n",
            "Epoch: 0036 train_loss= 0.66986 train_acc= 0.67602 train_auc= 0.79097 val_loss= 0.69461 val_acc= 0.59770 val_auc= 0.68240 time= 0.39926\n",
            "Epoch: 0036 train_loss= 0.67099 train_acc= 0.66582 train_auc= 0.76988 val_loss= 0.68496 val_acc= 0.65517 val_auc= 0.77181 time= 0.33483\n",
            "Epoch: 0035 train_loss= 0.67210 train_acc= 0.67347 train_auc= 0.77939 val_loss= 0.68692 val_acc= 0.62069 val_auc= 0.77394 time= 0.46505\n",
            "Epoch: 0035 train_loss= 0.67114 train_acc= 0.68878 train_auc= 0.77840 val_loss= 0.69140 val_acc= 0.57471 val_auc= 0.71117 time= 0.42130\n",
            "Epoch: 0036 train_loss= 0.67193 train_acc= 0.65773 train_auc= 0.77340 val_loss= 0.68992 val_acc= 0.60227 val_auc= 0.72496 time= 0.49920\n",
            "Epoch: 0036 train_loss= 0.67007 train_acc= 0.67985 train_auc= 0.78643 val_loss= 0.69314 val_acc= 0.58621 val_auc= 0.73032 time= 0.42302\n",
            "Epoch: 0036 train_loss= 0.67148 train_acc= 0.66199 train_auc= 0.75774 val_loss= 0.69699 val_acc= 0.56322 val_auc= 0.68982 time= 0.45685\n",
            "Epoch: 0037 train_loss= 0.66988 train_acc= 0.64541 train_auc= 0.79535 val_loss= 0.68004 val_acc= 0.62069 val_auc= 0.77447 time= 0.40916\n",
            "Epoch: 0037 train_loss= 0.66969 train_acc= 0.65816 train_auc= 0.77624 val_loss= 0.69747 val_acc= 0.57471 val_auc= 0.67128 time= 0.40548\n",
            "Epoch: 0036 train_loss= 0.67084 train_acc= 0.67730 train_auc= 0.80798 val_loss= 0.68755 val_acc= 0.62069 val_auc= 0.72128 time= 0.39579\n",
            "Epoch: 0037 train_loss= 0.66752 train_acc= 0.68367 train_auc= 0.79258 val_loss= 0.69467 val_acc= 0.59770 val_auc= 0.68505 time= 0.41699\n",
            "Epoch: 0037 train_loss= 0.67037 train_acc= 0.65944 train_auc= 0.76864 val_loss= 0.68452 val_acc= 0.65517 val_auc= 0.77128 time= 0.38740\n",
            "Epoch: 0036 train_loss= 0.67294 train_acc= 0.68367 train_auc= 0.77689 val_loss= 0.69159 val_acc= 0.57471 val_auc= 0.70745 time= 0.39557\n",
            "Epoch: 0036 train_loss= 0.67402 train_acc= 0.65689 train_auc= 0.76858 val_loss= 0.68694 val_acc= 0.62069 val_auc= 0.77340 time= 0.41984\n",
            "Epoch: 0037 train_loss= 0.66860 train_acc= 0.67561 train_auc= 0.78111 val_loss= 0.68983 val_acc= 0.61364 val_auc= 0.72289 time= 0.42673\n",
            "Epoch: 0037 train_loss= 0.66696 train_acc= 0.68112 train_auc= 0.80278 val_loss= 0.69307 val_acc= 0.58621 val_auc= 0.73245 time= 0.40482\n",
            "Epoch: 0037 train_loss= 0.66741 train_acc= 0.67602 train_auc= 0.78112 val_loss= 0.69702 val_acc= 0.56322 val_auc= 0.69247 time= 0.45551\n",
            "Epoch: 0038 train_loss= 0.67170 train_acc= 0.65306 train_auc= 0.76022 val_loss= 0.69782 val_acc= 0.59770 val_auc= 0.67021 time= 0.35273\n",
            "Epoch: 0038 train_loss= 0.67014 train_acc= 0.67602 train_auc= 0.78632 val_loss= 0.67961 val_acc= 0.66667 val_auc= 0.77234 time= 0.41100\n",
            "Epoch: 0037 train_loss= 0.67050 train_acc= 0.66582 train_auc= 0.79599 val_loss= 0.68729 val_acc= 0.63218 val_auc= 0.72287 time= 0.41020\n",
            "Epoch: 0038 train_loss= 0.66729 train_acc= 0.68750 train_auc= 0.79840 val_loss= 0.69474 val_acc= 0.63218 val_auc= 0.68505 time= 0.44841\n",
            "Epoch: 0037 train_loss= 0.66990 train_acc= 0.66837 train_auc= 0.78121 val_loss= 0.69175 val_acc= 0.57471 val_auc= 0.70426 time= 0.42583\n",
            "Epoch: 0038 train_loss= 0.67189 train_acc= 0.64796 train_auc= 0.75304 val_loss= 0.68428 val_acc= 0.65517 val_auc= 0.77234 time= 0.46655\n",
            "Epoch: 0037 train_loss= 0.67072 train_acc= 0.65816 train_auc= 0.77910 val_loss= 0.68685 val_acc= 0.63218 val_auc= 0.77340 time= 0.39785\n",
            "Epoch: 0038 train_loss= 0.67361 train_acc= 0.64368 train_auc= 0.75854 val_loss= 0.68972 val_acc= 0.60227 val_auc= 0.71873 time= 0.39433\n",
            "Epoch: 0038 train_loss= 0.66955 train_acc= 0.66327 train_auc= 0.77864 val_loss= 0.69319 val_acc= 0.60920 val_auc= 0.73404 time= 0.38098\n",
            "Epoch: 0038 train_loss= 0.67099 train_acc= 0.65816 train_auc= 0.75181 val_loss= 0.69679 val_acc= 0.59770 val_auc= 0.70414 time= 0.34593\n",
            "Epoch: 0039 train_loss= 0.67004 train_acc= 0.66582 train_auc= 0.76365 val_loss= 0.69820 val_acc= 0.64368 val_auc= 0.67394 time= 0.48896\n",
            "Epoch: 0039 train_loss= 0.67121 train_acc= 0.67602 train_auc= 0.77079 val_loss= 0.67921 val_acc= 0.66667 val_auc= 0.77074 time= 0.52059\n",
            "Epoch: 0039 train_loss= 0.66646 train_acc= 0.69770 train_auc= 0.78943 val_loss= 0.69479 val_acc= 0.63218 val_auc= 0.68452 time= 0.40656\n",
            "Epoch: 0038 train_loss= 0.67152 train_acc= 0.66964 train_auc= 0.77734 val_loss= 0.68705 val_acc= 0.59770 val_auc= 0.72021 time= 0.49318\n",
            "Epoch: 0039 train_loss= 0.67175 train_acc= 0.67857 train_auc= 0.76119 val_loss= 0.68422 val_acc= 0.63218 val_auc= 0.77128 time= 0.39377\n",
            "Epoch: 0038 train_loss= 0.67570 train_acc= 0.63393 train_auc= 0.74237 val_loss= 0.69175 val_acc= 0.57471 val_auc= 0.70319 time= 0.50644\n",
            "Epoch: 0038 train_loss= 0.67223 train_acc= 0.65816 train_auc= 0.76210 val_loss= 0.68656 val_acc= 0.64368 val_auc= 0.77660 time= 0.52587\n",
            "Epoch: 0039 train_loss= 0.66932 train_acc= 0.69860 train_auc= 0.77990 val_loss= 0.68957 val_acc= 0.65909 val_auc= 0.71770 time= 0.47036\n",
            "Epoch: 0039 train_loss= 0.66760 train_acc= 0.69260 train_auc= 0.78881 val_loss= 0.69326 val_acc= 0.63218 val_auc= 0.73298 time= 0.44014\n",
            "Epoch: 0039 train_loss= 0.66745 train_acc= 0.68112 train_auc= 0.77816 val_loss= 0.69642 val_acc= 0.60920 val_auc= 0.70891 time= 0.49629\n",
            "Epoch: 0039 train_loss= 0.67064 train_acc= 0.69260 train_auc= 0.77609 val_loss= 0.68688 val_acc= 0.63218 val_auc= 0.72021 time= 0.40377\n",
            "Epoch: 0040 train_loss= 0.66565 train_acc= 0.70536 train_auc= 0.79805 val_loss= 0.69847 val_acc= 0.65517 val_auc= 0.67394 time= 0.47376\n",
            "Epoch: 0040 train_loss= 0.66577 train_acc= 0.70663 train_auc= 0.80323 val_loss= 0.67870 val_acc= 0.66667 val_auc= 0.77074 time= 0.46637\n",
            "Epoch: 0040 train_loss= 0.66705 train_acc= 0.69643 train_auc= 0.79124 val_loss= 0.68393 val_acc= 0.64368 val_auc= 0.76968 time= 0.42895\n",
            "Epoch: 0040 train_loss= 0.66540 train_acc= 0.70281 train_auc= 0.80438 val_loss= 0.69495 val_acc= 0.63218 val_auc= 0.68664 time= 0.49119\n",
            "Epoch: 0039 train_loss= 0.67153 train_acc= 0.66327 train_auc= 0.76702 val_loss= 0.69154 val_acc= 0.58621 val_auc= 0.70638 time= 0.45254\n",
            "Epoch: 0039 train_loss= 0.67217 train_acc= 0.68112 train_auc= 0.76395 val_loss= 0.68604 val_acc= 0.64368 val_auc= 0.77819 time= 0.39335\n",
            "Epoch: 0040 train_loss= 0.66232 train_acc= 0.70918 train_auc= 0.81976 val_loss= 0.69316 val_acc= 0.64368 val_auc= 0.73457 time= 0.44698\n",
            "Epoch: 0040 train_loss= 0.66528 train_acc= 0.71648 train_auc= 0.81519 val_loss= 0.68928 val_acc= 0.64773 val_auc= 0.71770 time= 0.46946\n",
            "Epoch: 0040 train_loss= 0.66415 train_acc= 0.71811 train_auc= 0.79869 val_loss= 0.69607 val_acc= 0.59770 val_auc= 0.71103 time= 0.44981\n",
            "Epoch: 0040 train_loss= 0.66525 train_acc= 0.71173 train_auc= 0.81392 val_loss= 0.68654 val_acc= 0.65517 val_auc= 0.72181 time= 0.44646\n",
            "Epoch: 0041 train_loss= 0.66574 train_acc= 0.70918 train_auc= 0.80282 val_loss= 0.69852 val_acc= 0.65517 val_auc= 0.67074 time= 0.46060\n",
            "Epoch: 0041 train_loss= 0.66543 train_acc= 0.69388 train_auc= 0.80604 val_loss= 0.67814 val_acc= 0.66667 val_auc= 0.77181 time= 0.44333\n",
            "Epoch: 0041 train_loss= 0.66276 train_acc= 0.72704 train_auc= 0.81908 val_loss= 0.69510 val_acc= 0.63218 val_auc= 0.68717 time= 0.41938\n",
            "Epoch: 0041 train_loss= 0.66610 train_acc= 0.70408 train_auc= 0.79825 val_loss= 0.68387 val_acc= 0.63218 val_auc= 0.77021 time= 0.49661\n",
            "Epoch: 0040 train_loss= 0.66642 train_acc= 0.69005 train_auc= 0.80573 val_loss= 0.69131 val_acc= 0.59770 val_auc= 0.70585 time= 0.40122\n",
            "Epoch: 0040 train_loss= 0.66728 train_acc= 0.70791 train_auc= 0.79487 val_loss= 0.68530 val_acc= 0.64368 val_auc= 0.77819 time= 0.41695\n",
            "Epoch: 0041 train_loss= 0.66708 train_acc= 0.71137 train_auc= 0.81253 val_loss= 0.68899 val_acc= 0.65909 val_auc= 0.71873 time= 0.39711\n",
            "Epoch: 0041 train_loss= 0.66486 train_acc= 0.70408 train_auc= 0.80705 val_loss= 0.69303 val_acc= 0.64368 val_auc= 0.73617 time= 0.42011\n",
            "Epoch: 0041 train_loss= 0.66577 train_acc= 0.70281 train_auc= 0.79349 val_loss= 0.69592 val_acc= 0.59770 val_auc= 0.71156 time= 0.39263\n",
            "Epoch: 0042 train_loss= 0.66497 train_acc= 0.71684 train_auc= 0.79868 val_loss= 0.69858 val_acc= 0.64368 val_auc= 0.67181 time= 0.38940\n",
            "Epoch: 0042 train_loss= 0.66587 train_acc= 0.69770 train_auc= 0.78865 val_loss= 0.67769 val_acc= 0.67816 val_auc= 0.77181 time= 0.41368\n",
            "Epoch: 0042 train_loss= 0.66632 train_acc= 0.69133 train_auc= 0.78556 val_loss= 0.68342 val_acc= 0.65517 val_auc= 0.76968 time= 0.35503\n",
            "Epoch: 0041 train_loss= 0.66604 train_acc= 0.71811 train_auc= 0.80475 val_loss= 0.68604 val_acc= 0.65517 val_auc= 0.72287 time= 0.43409\n",
            "Epoch: 0042 train_loss= 0.66512 train_acc= 0.69133 train_auc= 0.80289 val_loss= 0.69553 val_acc= 0.63218 val_auc= 0.68770 time= 0.41458\n",
            "Epoch: 0041 train_loss= 0.67127 train_acc= 0.67857 train_auc= 0.77333 val_loss= 0.69127 val_acc= 0.60920 val_auc= 0.70691 time= 0.40755\n",
            "Epoch: 0041 train_loss= 0.66757 train_acc= 0.70026 train_auc= 0.79281 val_loss= 0.68460 val_acc= 0.64368 val_auc= 0.77979 time= 0.41980\n",
            "Epoch: 0042 train_loss= 0.66465 train_acc= 0.72286 train_auc= 0.81740 val_loss= 0.68911 val_acc= 0.61364 val_auc= 0.72185 time= 0.36923\n",
            "Epoch: 0042 train_loss= 0.66253 train_acc= 0.71939 train_auc= 0.79953 val_loss= 0.69587 val_acc= 0.59770 val_auc= 0.71474 time= 0.40244\n",
            "Epoch: 0042 train_loss= 0.66414 train_acc= 0.72194 train_auc= 0.81428 val_loss= 0.69305 val_acc= 0.65517 val_auc= 0.73617 time= 0.42294\n",
            "Epoch: 0042 train_loss= 0.66512 train_acc= 0.70791 train_auc= 0.80397 val_loss= 0.68554 val_acc= 0.63218 val_auc= 0.72447 time= 0.39030\n",
            "Epoch: 0043 train_loss= 0.66096 train_acc= 0.70026 train_auc= 0.82067 val_loss= 0.68313 val_acc= 0.65517 val_auc= 0.76755 time= 0.46005\n",
            "Epoch: 0043 train_loss= 0.66089 train_acc= 0.70791 train_auc= 0.82291 val_loss= 0.67781 val_acc= 0.66667 val_auc= 0.77181 time= 0.46243\n",
            "Epoch: 0043 train_loss= 0.66143 train_acc= 0.71429 train_auc= 0.82172 val_loss= 0.69862 val_acc= 0.59770 val_auc= 0.67181 time= 0.48890\n",
            "Epoch: 0043 train_loss= 0.65950 train_acc= 0.73724 train_auc= 0.84295 val_loss= 0.69579 val_acc= 0.62069 val_auc= 0.68558 time= 0.45046\n",
            "Epoch: 0042 train_loss= 0.66933 train_acc= 0.69260 train_auc= 0.78420 val_loss= 0.69130 val_acc= 0.62069 val_auc= 0.71011 time= 0.42008\n",
            "Epoch: 0042 train_loss= 0.66844 train_acc= 0.70026 train_auc= 0.78919 val_loss= 0.68394 val_acc= 0.64368 val_auc= 0.77979 time= 0.42683\n",
            "Epoch: 0043 train_loss= 0.66226 train_acc= 0.72414 train_auc= 0.82921 val_loss= 0.68898 val_acc= 0.60227 val_auc= 0.72340 time= 0.37963\n",
            "Epoch: 0043 train_loss= 0.65786 train_acc= 0.73469 train_auc= 0.83196 val_loss= 0.69594 val_acc= 0.59770 val_auc= 0.71474 time= 0.40064\n",
            "Epoch: 0043 train_loss= 0.66156 train_acc= 0.71811 train_auc= 0.82321 val_loss= 0.69299 val_acc= 0.66667 val_auc= 0.73989 time= 0.40226\n",
            "Epoch: 0043 train_loss= 0.66055 train_acc= 0.72704 train_auc= 0.82385 val_loss= 0.68552 val_acc= 0.62069 val_auc= 0.73032 time= 0.42234\n",
            "Epoch: 0044 train_loss= 0.66608 train_acc= 0.68240 train_auc= 0.79389 val_loss= 0.67680 val_acc= 0.66667 val_auc= 0.77181 time= 0.39847\n",
            "Epoch: 0044 train_loss= 0.66513 train_acc= 0.68878 train_auc= 0.79835 val_loss= 0.69899 val_acc= 0.59770 val_auc= 0.67021 time= 0.39105\n",
            "Epoch: 0044 train_loss= 0.66591 train_acc= 0.70281 train_auc= 0.79357 val_loss= 0.68343 val_acc= 0.63218 val_auc= 0.76596 time= 0.42898\n",
            "Epoch: 0044 train_loss= 0.66146 train_acc= 0.71939 train_auc= 0.82580 val_loss= 0.69628 val_acc= 0.63218 val_auc= 0.68399 time= 0.41003\n",
            "Epoch: 0043 train_loss= 0.66570 train_acc= 0.69770 train_auc= 0.81376 val_loss= 0.69112 val_acc= 0.60920 val_auc= 0.71383 time= 0.45027\n",
            "Epoch: 0043 train_loss= 0.66199 train_acc= 0.71939 train_auc= 0.81868 val_loss= 0.68350 val_acc= 0.66667 val_auc= 0.78245 time= 0.41804\n",
            "Epoch: 0044 train_loss= 0.66672 train_acc= 0.69093 train_auc= 0.80225 val_loss= 0.68835 val_acc= 0.62500 val_auc= 0.72133 time= 0.38999\n",
            "Epoch: 0044 train_loss= 0.66615 train_acc= 0.68495 train_auc= 0.79853 val_loss= 0.69261 val_acc= 0.66667 val_auc= 0.74043 time= 0.36994\n",
            "Epoch: 0044 train_loss= 0.66498 train_acc= 0.71046 train_auc= 0.79510 val_loss= 0.69597 val_acc= 0.59770 val_auc= 0.71527 time= 0.44993\n",
            "Epoch: 0044 train_loss= 0.66557 train_acc= 0.69643 train_auc= 0.78977 val_loss= 0.68524 val_acc= 0.62069 val_auc= 0.73085 time= 0.42822\n",
            "Epoch: 0045 train_loss= 0.66192 train_acc= 0.72704 train_auc= 0.82660 val_loss= 0.69605 val_acc= 0.64368 val_auc= 0.68611 time= 0.39537\n",
            "Epoch: 0045 train_loss= 0.66376 train_acc= 0.69643 train_auc= 0.81331 val_loss= 0.69922 val_acc= 0.58621 val_auc= 0.66915 time= 0.40536\n",
            "Epoch: 0045 train_loss= 0.66484 train_acc= 0.70791 train_auc= 0.80097 val_loss= 0.68263 val_acc= 0.64368 val_auc= 0.76596 time= 0.44329\n",
            "Epoch: 0045 train_loss= 0.66307 train_acc= 0.69898 train_auc= 0.81418 val_loss= 0.67618 val_acc= 0.66667 val_auc= 0.77234 time= 0.48027\n",
            "Epoch: 0044 train_loss= 0.67076 train_acc= 0.66964 train_auc= 0.77523 val_loss= 0.69086 val_acc= 0.60920 val_auc= 0.71755 time= 0.42604\n",
            "Epoch: 0044 train_loss= 0.66602 train_acc= 0.69770 train_auc= 0.79710 val_loss= 0.68298 val_acc= 0.68966 val_auc= 0.78511 time= 0.42648\n",
            "Epoch: 0045 train_loss= 0.66134 train_acc= 0.71775 train_auc= 0.82134 val_loss= 0.68834 val_acc= 0.64773 val_auc= 0.71925 time= 0.41738\n",
            "Epoch: 0045 train_loss= 0.66258 train_acc= 0.71173 train_auc= 0.81482 val_loss= 0.69258 val_acc= 0.66667 val_auc= 0.74255 time= 0.38511\n",
            "Epoch: 0045 train_loss= 0.66333 train_acc= 0.70153 train_auc= 0.80787 val_loss= 0.69597 val_acc= 0.59770 val_auc= 0.71633 time= 0.40117\n",
            "Epoch: 0045 train_loss= 0.66539 train_acc= 0.70536 train_auc= 0.80382 val_loss= 0.68475 val_acc= 0.62069 val_auc= 0.73138 time= 0.39608\n",
            "Epoch: 0046 train_loss= 0.66496 train_acc= 0.70408 train_auc= 0.80535 val_loss= 0.69935 val_acc= 0.58621 val_auc= 0.66968 time= 0.45656\n",
            "Epoch: 0046 train_loss= 0.66170 train_acc= 0.72066 train_auc= 0.82308 val_loss= 0.67588 val_acc= 0.66667 val_auc= 0.77340 time= 0.40801\n",
            "Epoch: 0046 train_loss= 0.66334 train_acc= 0.72449 train_auc= 0.81436 val_loss= 0.68223 val_acc= 0.64368 val_auc= 0.76489 time= 0.43567\n",
            "Epoch: 0046 train_loss= 0.66255 train_acc= 0.72704 train_auc= 0.82711 val_loss= 0.69587 val_acc= 0.63218 val_auc= 0.68611 time= 0.45902\n",
            "Epoch: 0045 train_loss= 0.66566 train_acc= 0.67985 train_auc= 0.80716 val_loss= 0.69049 val_acc= 0.60920 val_auc= 0.71862 time= 0.38401\n",
            "Epoch: 0045 train_loss= 0.66242 train_acc= 0.70663 train_auc= 0.82156 val_loss= 0.68284 val_acc= 0.67816 val_auc= 0.78457 time= 0.43166\n",
            "Epoch: 0046 train_loss= 0.66274 train_acc= 0.72414 train_auc= 0.82051 val_loss= 0.68834 val_acc= 0.63636 val_auc= 0.72029 time= 0.41045\n",
            "Epoch: 0046 train_loss= 0.66113 train_acc= 0.73214 train_auc= 0.83710 val_loss= 0.69244 val_acc= 0.66667 val_auc= 0.74415 time= 0.40815\n",
            "Epoch: 0046 train_loss= 0.66190 train_acc= 0.71811 train_auc= 0.81678 val_loss= 0.69595 val_acc= 0.60920 val_auc= 0.71739 time= 0.39344\n",
            "Epoch: 0046 train_loss= 0.66304 train_acc= 0.71556 train_auc= 0.81863 val_loss= 0.68445 val_acc= 0.63218 val_auc= 0.73191 time= 0.40556\n",
            "Epoch: 0047 train_loss= 0.65869 train_acc= 0.72066 train_auc= 0.83888 val_loss= 0.69590 val_acc= 0.63218 val_auc= 0.68664 time= 0.35770\n",
            "Epoch: 0047 train_loss= 0.66154 train_acc= 0.72959 train_auc= 0.82113 val_loss= 0.67553 val_acc= 0.66667 val_auc= 0.77394 time= 0.38201\n",
            "Epoch: 0047 train_loss= 0.66316 train_acc= 0.70791 train_auc= 0.80701 val_loss= 0.68213 val_acc= 0.63218 val_auc= 0.76277 time= 0.41912\n",
            "Epoch: 0047 train_loss= 0.66205 train_acc= 0.70408 train_auc= 0.83241 val_loss= 0.69949 val_acc= 0.59770 val_auc= 0.67128 time= 0.41491\n",
            "Epoch: 0046 train_loss= 0.66668 train_acc= 0.70663 train_auc= 0.81017 val_loss= 0.69044 val_acc= 0.60920 val_auc= 0.71649 time= 0.40306\n",
            "Epoch: 0046 train_loss= 0.66452 train_acc= 0.71556 train_auc= 0.80723 val_loss= 0.68210 val_acc= 0.66667 val_auc= 0.78564 time= 0.42742\n",
            "Epoch: 0047 train_loss= 0.65847 train_acc= 0.74107 train_auc= 0.83484 val_loss= 0.69205 val_acc= 0.66667 val_auc= 0.74521 time= 0.35608\n",
            "Epoch: 0047 train_loss= 0.65997 train_acc= 0.73819 train_auc= 0.82822 val_loss= 0.68834 val_acc= 0.63636 val_auc= 0.71873 time= 0.41991\n",
            "Epoch: 0047 train_loss= 0.66012 train_acc= 0.71556 train_auc= 0.81613 val_loss= 0.69599 val_acc= 0.59770 val_auc= 0.71633 time= 0.43082\n",
            "Epoch: 0047 train_loss= 0.66216 train_acc= 0.71811 train_auc= 0.82557 val_loss= 0.68436 val_acc= 0.64368 val_auc= 0.73351 time= 0.39997\n",
            "Epoch: 0048 train_loss= 0.66023 train_acc= 0.72577 train_auc= 0.81961 val_loss= 0.67491 val_acc= 0.67816 val_auc= 0.77553 time= 0.37245\n",
            "Epoch: 0048 train_loss= 0.65482 train_acc= 0.72066 train_auc= 0.84560 val_loss= 0.69642 val_acc= 0.63218 val_auc= 0.68505 time= 0.38571\n",
            "Epoch: 0048 train_loss= 0.65679 train_acc= 0.71046 train_auc= 0.84909 val_loss= 0.69976 val_acc= 0.64368 val_auc= 0.67287 time= 0.37519\n",
            "Epoch: 0047 train_loss= 0.66431 train_acc= 0.71556 train_auc= 0.81104 val_loss= 0.69042 val_acc= 0.60920 val_auc= 0.71596 time= 0.45956\n",
            "Epoch: 0048 train_loss= 0.65865 train_acc= 0.71556 train_auc= 0.82826 val_loss= 0.68240 val_acc= 0.64368 val_auc= 0.76223 time= 0.45706\n",
            "Epoch: 0048 train_loss= 0.65605 train_acc= 0.72704 train_auc= 0.85375 val_loss= 0.69220 val_acc= 0.66667 val_auc= 0.74681 time= 0.37495\n",
            "Epoch: 0048 train_loss= 0.65822 train_acc= 0.71903 train_auc= 0.83676 val_loss= 0.68869 val_acc= 0.63636 val_auc= 0.72081 time= 0.40599\n",
            "Epoch: 0047 train_loss= 0.66235 train_acc= 0.71556 train_auc= 0.81465 val_loss= 0.68170 val_acc= 0.65517 val_auc= 0.78457 time= 0.46468\n",
            "Epoch: 0048 train_loss= 0.65678 train_acc= 0.71429 train_auc= 0.83678 val_loss= 0.69595 val_acc= 0.60920 val_auc= 0.71845 time= 0.38725\n",
            "Epoch: 0048 train_loss= 0.66050 train_acc= 0.71811 train_auc= 0.83232 val_loss= 0.68434 val_acc= 0.65517 val_auc= 0.73138 time= 0.38086\n",
            "Epoch: 0049 train_loss= 0.65758 train_acc= 0.73852 train_auc= 0.84368 val_loss= 0.67432 val_acc= 0.67816 val_auc= 0.77660 time= 0.39321\n",
            "Epoch: 0049 train_loss= 0.65597 train_acc= 0.74107 train_auc= 0.84092 val_loss= 0.69654 val_acc= 0.63218 val_auc= 0.68717 time= 0.44450\n",
            "Epoch: 0049 train_loss= 0.65840 train_acc= 0.74362 train_auc= 0.84777 val_loss= 0.70022 val_acc= 0.65517 val_auc= 0.67660 time= 0.41594\n",
            "Epoch: 0049 train_loss= 0.66058 train_acc= 0.71684 train_auc= 0.81920 val_loss= 0.68102 val_acc= 0.64368 val_auc= 0.76489 time= 0.39259\n",
            "Epoch: 0048 train_loss= 0.66139 train_acc= 0.71556 train_auc= 0.82982 val_loss= 0.69052 val_acc= 0.60920 val_auc= 0.71117 time= 0.39681\n",
            "Epoch: 0049 train_loss= 0.66007 train_acc= 0.73980 train_auc= 0.83779 val_loss= 0.69179 val_acc= 0.66667 val_auc= 0.74628 time= 0.41445\n",
            "Epoch: 0049 train_loss= 0.65823 train_acc= 0.74330 train_auc= 0.83446 val_loss= 0.68845 val_acc= 0.63636 val_auc= 0.71614 time= 0.40398\n",
            "Epoch: 0048 train_loss= 0.66144 train_acc= 0.69643 train_auc= 0.81471 val_loss= 0.68157 val_acc= 0.67816 val_auc= 0.78351 time= 0.43552\n",
            "Epoch: 0049 train_loss= 0.65512 train_acc= 0.74617 train_auc= 0.83842 val_loss= 0.69608 val_acc= 0.62069 val_auc= 0.71792 time= 0.44764\n",
            "Epoch: 0050 train_loss= 0.65909 train_acc= 0.73469 train_auc= 0.82442 val_loss= 0.67349 val_acc= 0.68966 val_auc= 0.77979 time= 0.46220\n",
            "Epoch: 0049 train_loss= 0.65803 train_acc= 0.73342 train_auc= 0.84611 val_loss= 0.68374 val_acc= 0.67816 val_auc= 0.73085 time= 0.45893\n",
            "Epoch: 0050 train_loss= 0.65747 train_acc= 0.72577 train_auc= 0.83226 val_loss= 0.70049 val_acc= 0.66667 val_auc= 0.67394 time= 0.44388\n",
            "Epoch: 0050 train_loss= 0.65588 train_acc= 0.73852 train_auc= 0.83727 val_loss= 0.69626 val_acc= 0.63218 val_auc= 0.68664 time= 0.45786\n",
            "Epoch: 0050 train_loss= 0.65763 train_acc= 0.74362 train_auc= 0.82509 val_loss= 0.68045 val_acc= 0.64368 val_auc= 0.76543 time= 0.42356\n",
            "Epoch: 0049 train_loss= 0.66086 train_acc= 0.73342 train_auc= 0.82980 val_loss= 0.69025 val_acc= 0.62069 val_auc= 0.71011 time= 0.43688\n",
            "Epoch: 0050 train_loss= 0.65502 train_acc= 0.73214 train_auc= 0.83446 val_loss= 0.69175 val_acc= 0.68966 val_auc= 0.75266 time= 0.44877\n",
            "Epoch: 0050 train_loss= 0.65807 train_acc= 0.71903 train_auc= 0.82366 val_loss= 0.68815 val_acc= 0.65909 val_auc= 0.71354 time= 0.43402\n",
            "Epoch: 0049 train_loss= 0.65579 train_acc= 0.75383 train_auc= 0.85680 val_loss= 0.68133 val_acc= 0.67816 val_auc= 0.78777 time= 0.46192\n",
            "Epoch: 0050 train_loss= 0.65498 train_acc= 0.73469 train_auc= 0.82951 val_loss= 0.69537 val_acc= 0.63218 val_auc= 0.72110 time= 0.40867\n",
            "Epoch: 0051 train_loss= 0.65809 train_acc= 0.72704 train_auc= 0.81675 val_loss= 0.67263 val_acc= 0.68966 val_auc= 0.78032 time= 0.36513\n",
            "Epoch: 0050 train_loss= 0.65716 train_acc= 0.73852 train_auc= 0.83719 val_loss= 0.68304 val_acc= 0.68966 val_auc= 0.73032 time= 0.37279\n",
            "Epoch: 0051 train_loss= 0.65430 train_acc= 0.75000 train_auc= 0.83285 val_loss= 0.69619 val_acc= 0.64368 val_auc= 0.68505 time= 0.36139\n",
            "Epoch: 0050 train_loss= 0.65964 train_acc= 0.71556 train_auc= 0.82465 val_loss= 0.68982 val_acc= 0.62069 val_auc= 0.71064 time= 0.37979\n",
            "Epoch: 0051 train_loss= 0.65773 train_acc= 0.71301 train_auc= 0.82020 val_loss= 0.68017 val_acc= 0.63218 val_auc= 0.76596 time= 0.40909\n",
            "Epoch: 0051 train_loss= 0.65665 train_acc= 0.74235 train_auc= 0.82692 val_loss= 0.70079 val_acc= 0.65517 val_auc= 0.67500 time= 0.47157\n",
            "Epoch: 0051 train_loss= 0.65458 train_acc= 0.73852 train_auc= 0.82331 val_loss= 0.69187 val_acc= 0.64368 val_auc= 0.75426 time= 0.39511\n",
            "Epoch: 0051 train_loss= 0.65517 train_acc= 0.72925 train_auc= 0.81538 val_loss= 0.68807 val_acc= 0.65909 val_auc= 0.71303 time= 0.37934\n",
            "Epoch: 0051 train_loss= 0.65307 train_acc= 0.73087 train_auc= 0.82540 val_loss= 0.69493 val_acc= 0.63218 val_auc= 0.72587 time= 0.34922\n",
            "Epoch: 0050 train_loss= 0.65805 train_acc= 0.74107 train_auc= 0.83461 val_loss= 0.68079 val_acc= 0.70115 val_auc= 0.78777 time= 0.38461\n",
            "Epoch: 0052 train_loss= 0.65209 train_acc= 0.74745 train_auc= 0.83171 val_loss= 0.67211 val_acc= 0.70115 val_auc= 0.78085 time= 0.39543\n",
            "Epoch: 0052 train_loss= 0.64981 train_acc= 0.76148 train_auc= 0.84130 val_loss= 0.69630 val_acc= 0.64368 val_auc= 0.68823 time= 0.38153\n",
            "Epoch: 0051 train_loss= 0.65686 train_acc= 0.73214 train_auc= 0.81784 val_loss= 0.68287 val_acc= 0.68966 val_auc= 0.73032 time= 0.40237\n",
            "Epoch: 0052 train_loss= 0.65333 train_acc= 0.72449 train_auc= 0.82166 val_loss= 0.68042 val_acc= 0.64368 val_auc= 0.76489 time= 0.40430\n",
            "Epoch: 0051 train_loss= 0.66197 train_acc= 0.71429 train_auc= 0.80197 val_loss= 0.68943 val_acc= 0.62069 val_auc= 0.71223 time= 0.40127\n",
            "Epoch: 0052 train_loss= 0.65063 train_acc= 0.73724 train_auc= 0.83035 val_loss= 0.69188 val_acc= 0.65517 val_auc= 0.75479 time= 0.41218\n",
            "Epoch: 0052 train_loss= 0.65232 train_acc= 0.73980 train_auc= 0.82883 val_loss= 0.70093 val_acc= 0.65517 val_auc= 0.67500 time= 0.49091\n",
            "Epoch: 0052 train_loss= 0.65464 train_acc= 0.72542 train_auc= 0.82043 val_loss= 0.68793 val_acc= 0.65909 val_auc= 0.71458 time= 0.41084\n",
            "Epoch: 0052 train_loss= 0.65086 train_acc= 0.73980 train_auc= 0.83260 val_loss= 0.69484 val_acc= 0.63218 val_auc= 0.72959 time= 0.39160\n",
            "Epoch: 0051 train_loss= 0.65455 train_acc= 0.72832 train_auc= 0.83285 val_loss= 0.68018 val_acc= 0.72414 val_auc= 0.79043 time= 0.40693\n",
            "Epoch: 0053 train_loss= 0.65994 train_acc= 0.71556 train_auc= 0.79750 val_loss= 0.67153 val_acc= 0.67816 val_auc= 0.78191 time= 0.39312\n",
            "Epoch: 0052 train_loss= 0.65285 train_acc= 0.73469 train_auc= 0.83703 val_loss= 0.68269 val_acc= 0.68966 val_auc= 0.73085 time= 0.40550\n",
            "Epoch: 0053 train_loss= 0.65372 train_acc= 0.73724 train_auc= 0.82086 val_loss= 0.69653 val_acc= 0.63218 val_auc= 0.68982 time= 0.41804\n",
            "Epoch: 0052 train_loss= 0.65594 train_acc= 0.71429 train_auc= 0.82268 val_loss= 0.68942 val_acc= 0.62069 val_auc= 0.71489 time= 0.37059\n",
            "Epoch: 0053 train_loss= 0.65647 train_acc= 0.72704 train_auc= 0.81054 val_loss= 0.68044 val_acc= 0.64368 val_auc= 0.76649 time= 0.39534\n",
            "Epoch: 0053 train_loss= 0.65536 train_acc= 0.73597 train_auc= 0.81597 val_loss= 0.69156 val_acc= 0.66667 val_auc= 0.75532 time= 0.41673\n",
            "Epoch: 0053 train_loss= 0.65339 train_acc= 0.75765 train_auc= 0.82661 val_loss= 0.70113 val_acc= 0.64368 val_auc= 0.67606 time= 0.37504\n",
            "Epoch: 0053 train_loss= 0.65795 train_acc= 0.73052 train_auc= 0.79666 val_loss= 0.68768 val_acc= 0.65909 val_auc= 0.71666 time= 0.40614\n",
            "Epoch: 0053 train_loss= 0.65659 train_acc= 0.73087 train_auc= 0.80006 val_loss= 0.69503 val_acc= 0.62069 val_auc= 0.72481 time= 0.37589\n",
            "Epoch: 0052 train_loss= 0.65124 train_acc= 0.74235 train_auc= 0.83813 val_loss= 0.67979 val_acc= 0.73563 val_auc= 0.79202 time= 0.37652\n",
            "Epoch: 0054 train_loss= 0.65461 train_acc= 0.73469 train_auc= 0.81941 val_loss= 0.67094 val_acc= 0.67816 val_auc= 0.78351 time= 0.40028\n",
            "Epoch: 0054 train_loss= 0.64892 train_acc= 0.73469 train_auc= 0.83375 val_loss= 0.69708 val_acc= 0.63218 val_auc= 0.68823 time= 0.38182\n",
            "Epoch: 0053 train_loss= 0.66028 train_acc= 0.72066 train_auc= 0.79648 val_loss= 0.68233 val_acc= 0.70115 val_auc= 0.73138 time= 0.40779\n",
            "Epoch: 0053 train_loss= 0.65734 train_acc= 0.72959 train_auc= 0.81444 val_loss= 0.68961 val_acc= 0.62069 val_auc= 0.71383 time= 0.38855\n",
            "Epoch: 0054 train_loss= 0.65683 train_acc= 0.71556 train_auc= 0.80925 val_loss= 0.67953 val_acc= 0.64368 val_auc= 0.76543 time= 0.43698\n",
            "Epoch: 0054 train_loss= 0.65035 train_acc= 0.74107 train_auc= 0.82789 val_loss= 0.69139 val_acc= 0.66667 val_auc= 0.75479 time= 0.35107\n",
            "Epoch: 0054 train_loss= 0.65308 train_acc= 0.72577 train_auc= 0.82530 val_loss= 0.70110 val_acc= 0.65517 val_auc= 0.67447 time= 0.38709\n",
            "Epoch: 0053 train_loss= 0.65912 train_acc= 0.72449 train_auc= 0.79735 val_loss= 0.67946 val_acc= 0.73563 val_auc= 0.79309 time= 0.36042\n",
            "Epoch: 0054 train_loss= 0.65168 train_acc= 0.74235 train_auc= 0.81971 val_loss= 0.69542 val_acc= 0.62069 val_auc= 0.72163 time= 0.40687\n",
            "Epoch: 0054 train_loss= 0.65475 train_acc= 0.72542 train_auc= 0.81851 val_loss= 0.68757 val_acc= 0.64773 val_auc= 0.71718 time= 0.42395\n",
            "Epoch: 0055 train_loss= 0.64709 train_acc= 0.73724 train_auc= 0.83608 val_loss= 0.69768 val_acc= 0.60920 val_auc= 0.68558 time= 0.39263\n",
            "Epoch: 0055 train_loss= 0.65037 train_acc= 0.72832 train_auc= 0.81864 val_loss= 0.67056 val_acc= 0.66667 val_auc= 0.78670 time= 0.43180\n",
            "Epoch: 0054 train_loss= 0.65586 train_acc= 0.71684 train_auc= 0.81068 val_loss= 0.68970 val_acc= 0.65517 val_auc= 0.71543 time= 0.40867\n",
            "Epoch: 0054 train_loss= 0.65741 train_acc= 0.71939 train_auc= 0.80041 val_loss= 0.68205 val_acc= 0.70115 val_auc= 0.73298 time= 0.42575\n",
            "Epoch: 0055 train_loss= 0.64483 train_acc= 0.74362 train_auc= 0.83294 val_loss= 0.69109 val_acc= 0.67816 val_auc= 0.75426 time= 0.38644\n",
            "Epoch: 0055 train_loss= 0.65145 train_acc= 0.70408 train_auc= 0.81998 val_loss= 0.70131 val_acc= 0.65517 val_auc= 0.67447 time= 0.39052\n",
            "Epoch: 0055 train_loss= 0.64785 train_acc= 0.72704 train_auc= 0.82856 val_loss= 0.67900 val_acc= 0.64368 val_auc= 0.76809 time= 0.44489\n",
            "Epoch: 0054 train_loss= 0.65549 train_acc= 0.73852 train_auc= 0.80955 val_loss= 0.67921 val_acc= 0.72414 val_auc= 0.79521 time= 0.37911\n",
            "Epoch: 0055 train_loss= 0.65035 train_acc= 0.70754 train_auc= 0.81939 val_loss= 0.68764 val_acc= 0.65909 val_auc= 0.71821 time= 0.36567\n",
            "Epoch: 0055 train_loss= 0.64534 train_acc= 0.73724 train_auc= 0.82866 val_loss= 0.69581 val_acc= 0.63218 val_auc= 0.71792 time= 0.41463\n",
            "Epoch: 0056 train_loss= 0.64859 train_acc= 0.74107 train_auc= 0.82140 val_loss= 0.69798 val_acc= 0.63218 val_auc= 0.68293 time= 0.37861\n",
            "Epoch: 0056 train_loss= 0.65296 train_acc= 0.70153 train_auc= 0.80453 val_loss= 0.66978 val_acc= 0.66667 val_auc= 0.78989 time= 0.38798\n",
            "Epoch: 0055 train_loss= 0.65125 train_acc= 0.72449 train_auc= 0.81810 val_loss= 0.68960 val_acc= 0.66667 val_auc= 0.71862 time= 0.38513\n",
            "Epoch: 0055 train_loss= 0.64759 train_acc= 0.72577 train_auc= 0.82698 val_loss= 0.68203 val_acc= 0.70115 val_auc= 0.73511 time= 0.40552\n",
            "Epoch: 0056 train_loss= 0.64977 train_acc= 0.71939 train_auc= 0.81615 val_loss= 0.69113 val_acc= 0.67816 val_auc= 0.75691 time= 0.43580\n",
            "Epoch: 0056 train_loss= 0.64935 train_acc= 0.72066 train_auc= 0.82080 val_loss= 0.70147 val_acc= 0.65517 val_auc= 0.67500 time= 0.40392\n",
            "Epoch: 0056 train_loss= 0.65450 train_acc= 0.69005 train_auc= 0.79779 val_loss= 0.67865 val_acc= 0.63218 val_auc= 0.77021 time= 0.41376\n",
            "Epoch: 0056 train_loss= 0.64984 train_acc= 0.72797 train_auc= 0.82406 val_loss= 0.68729 val_acc= 0.65909 val_auc= 0.71873 time= 0.35483\n",
            "Epoch: 0055 train_loss= 0.64594 train_acc= 0.75383 train_auc= 0.83211 val_loss= 0.67892 val_acc= 0.72414 val_auc= 0.79787 time= 0.40002\n",
            "Epoch: 0056 train_loss= 0.64938 train_acc= 0.71684 train_auc= 0.81240 val_loss= 0.69595 val_acc= 0.63218 val_auc= 0.71739 time= 0.35868\n",
            "Epoch: 0057 train_loss= 0.64605 train_acc= 0.74872 train_auc= 0.83456 val_loss= 0.69823 val_acc= 0.63218 val_auc= 0.67869 time= 0.35945\n",
            "Epoch: 0056 train_loss= 0.65469 train_acc= 0.71429 train_auc= 0.80000 val_loss= 0.68176 val_acc= 0.68966 val_auc= 0.73617 time= 0.37318\n",
            "Epoch: 0056 train_loss= 0.65391 train_acc= 0.71173 train_auc= 0.80693 val_loss= 0.68992 val_acc= 0.67816 val_auc= 0.72021 time= 0.38766\n",
            "Epoch: 0057 train_loss= 0.64798 train_acc= 0.73342 train_auc= 0.81852 val_loss= 0.66901 val_acc= 0.67816 val_auc= 0.79255 time= 0.41613\n",
            "Epoch: 0057 train_loss= 0.64679 train_acc= 0.74872 train_auc= 0.83181 val_loss= 0.69125 val_acc= 0.66667 val_auc= 0.75691 time= 0.39641\n",
            "Epoch: 0057 train_loss= 0.65063 train_acc= 0.72321 train_auc= 0.82024 val_loss= 0.70171 val_acc= 0.65517 val_auc= 0.67713 time= 0.41922\n",
            "Epoch: 0057 train_loss= 0.64878 train_acc= 0.74872 train_auc= 0.83264 val_loss= 0.67876 val_acc= 0.62069 val_auc= 0.76968 time= 0.42226\n",
            "Epoch: 0056 train_loss= 0.64872 train_acc= 0.73469 train_auc= 0.81780 val_loss= 0.67877 val_acc= 0.72414 val_auc= 0.79840 time= 0.37238\n",
            "Epoch: 0057 train_loss= 0.64760 train_acc= 0.74713 train_auc= 0.83167 val_loss= 0.68719 val_acc= 0.65909 val_auc= 0.71873 time= 0.40022\n",
            "Epoch: 0057 train_loss= 0.64777 train_acc= 0.72832 train_auc= 0.82828 val_loss= 0.69600 val_acc= 0.62069 val_auc= 0.71951 time= 0.43705\n",
            "Epoch: 0058 train_loss= 0.64221 train_acc= 0.73980 train_auc= 0.83902 val_loss= 0.69847 val_acc= 0.63218 val_auc= 0.67603 time= 0.36798\n",
            "Epoch: 0057 train_loss= 0.64571 train_acc= 0.74235 train_auc= 0.83862 val_loss= 0.68145 val_acc= 0.68966 val_auc= 0.73404 time= 0.37900\n",
            "Epoch: 0058 train_loss= 0.65131 train_acc= 0.72832 train_auc= 0.81142 val_loss= 0.66786 val_acc= 0.67816 val_auc= 0.79415 time= 0.42507\n",
            "Epoch: 0057 train_loss= 0.65218 train_acc= 0.73980 train_auc= 0.83376 val_loss= 0.68972 val_acc= 0.66667 val_auc= 0.71862 time= 0.46650\n",
            "Epoch: 0058 train_loss= 0.64239 train_acc= 0.74617 train_auc= 0.84213 val_loss= 0.70211 val_acc= 0.66667 val_auc= 0.67819 time= 0.36514\n",
            "Epoch: 0058 train_loss= 0.64553 train_acc= 0.74872 train_auc= 0.83660 val_loss= 0.67800 val_acc= 0.63218 val_auc= 0.77021 time= 0.43448\n",
            "Epoch: 0058 train_loss= 0.64728 train_acc= 0.76628 train_auc= 0.84311 val_loss= 0.68694 val_acc= 0.65909 val_auc= 0.71977 time= 0.41223\n",
            "Epoch: 0058 train_loss= 0.64564 train_acc= 0.74107 train_auc= 0.82526 val_loss= 0.69134 val_acc= 0.66667 val_auc= 0.75745 time= 0.54457\n",
            "Epoch: 0057 train_loss= 0.64889 train_acc= 0.73342 train_auc= 0.82883 val_loss= 0.67853 val_acc= 0.70115 val_auc= 0.79894 time= 0.46686\n",
            "Epoch: 0058 train_loss= 0.64308 train_acc= 0.73724 train_auc= 0.84305 val_loss= 0.69583 val_acc= 0.62069 val_auc= 0.71739 time= 0.41538\n",
            "Epoch: 0059 train_loss= 0.64477 train_acc= 0.73597 train_auc= 0.82724 val_loss= 0.69900 val_acc= 0.64368 val_auc= 0.67550 time= 0.43201\n",
            "Epoch: 0058 train_loss= 0.64773 train_acc= 0.73852 train_auc= 0.81941 val_loss= 0.68122 val_acc= 0.68966 val_auc= 0.73617 time= 0.42423\n",
            "Epoch: 0059 train_loss= 0.64846 train_acc= 0.73852 train_auc= 0.81695 val_loss= 0.66721 val_acc= 0.68966 val_auc= 0.79468 time= 0.40313\n",
            "Epoch: 0059 train_loss= 0.64994 train_acc= 0.71939 train_auc= 0.81469 val_loss= 0.70251 val_acc= 0.65517 val_auc= 0.67979 time= 0.41208\n",
            "Epoch: 0058 train_loss= 0.64671 train_acc= 0.76020 train_auc= 0.83961 val_loss= 0.68906 val_acc= 0.64368 val_auc= 0.71862 time= 0.44234\n",
            "Epoch: 0059 train_loss= 0.64908 train_acc= 0.73308 train_auc= 0.82755 val_loss= 0.68702 val_acc= 0.65909 val_auc= 0.71718 time= 0.38031\n",
            "Epoch: 0059 train_loss= 0.65055 train_acc= 0.73342 train_auc= 0.81741 val_loss= 0.69173 val_acc= 0.66667 val_auc= 0.75532 time= 0.37437\n",
            "Epoch: 0058 train_loss= 0.64500 train_acc= 0.73852 train_auc= 0.83669 val_loss= 0.67828 val_acc= 0.70115 val_auc= 0.79840 time= 0.35534\n",
            "Epoch: 0059 train_loss= 0.64492 train_acc= 0.74235 train_auc= 0.83413 val_loss= 0.69550 val_acc= 0.62069 val_auc= 0.72322 time= 0.34017\n",
            "Epoch: 0059 train_loss= 0.65059 train_acc= 0.70791 train_auc= 0.81763 val_loss= 0.67744 val_acc= 0.67816 val_auc= 0.77340 time= 0.46724\n",
            "Epoch: 0060 train_loss= 0.64543 train_acc= 0.75255 train_auc= 0.83209 val_loss= 0.69968 val_acc= 0.64368 val_auc= 0.67497 time= 0.38228\n",
            "Epoch: 0059 train_loss= 0.65262 train_acc= 0.71939 train_auc= 0.81096 val_loss= 0.68104 val_acc= 0.68966 val_auc= 0.73617 time= 0.38016\n",
            "Epoch: 0060 train_loss= 0.64828 train_acc= 0.73342 train_auc= 0.81936 val_loss= 0.66724 val_acc= 0.72414 val_auc= 0.79255 time= 0.41534\n",
            "Epoch: 0060 train_loss= 0.64646 train_acc= 0.75510 train_auc= 0.83591 val_loss= 0.70281 val_acc= 0.66667 val_auc= 0.68351 time= 0.41144\n",
            "Epoch: 0059 train_loss= 0.65028 train_acc= 0.73342 train_auc= 0.82268 val_loss= 0.68893 val_acc= 0.64368 val_auc= 0.72021 time= 0.42750\n",
            "Epoch: 0060 train_loss= 0.64397 train_acc= 0.76118 train_auc= 0.84206 val_loss= 0.68736 val_acc= 0.64773 val_auc= 0.71666 time= 0.39868\n",
            "Epoch: 0059 train_loss= 0.65091 train_acc= 0.73469 train_auc= 0.81497 val_loss= 0.67778 val_acc= 0.71264 val_auc= 0.79840 time= 0.39112\n",
            "Epoch: 0060 train_loss= 0.64877 train_acc= 0.73724 train_auc= 0.81213 val_loss= 0.69184 val_acc= 0.66667 val_auc= 0.75745 time= 0.42256\n",
            "Epoch: 0060 train_loss= 0.64786 train_acc= 0.72194 train_auc= 0.82008 val_loss= 0.69519 val_acc= 0.62069 val_auc= 0.72641 time= 0.39598\n",
            "Epoch: 0060 train_loss= 0.64903 train_acc= 0.74362 train_auc= 0.82675 val_loss= 0.67767 val_acc= 0.67816 val_auc= 0.77340 time= 0.40472\n",
            "Epoch: 0061 train_loss= 0.64078 train_acc= 0.74617 train_auc= 0.84427 val_loss= 0.70010 val_acc= 0.64368 val_auc= 0.67285 time= 0.38424\n",
            "Epoch: 0060 train_loss= 0.64577 train_acc= 0.74235 train_auc= 0.84283 val_loss= 0.68113 val_acc= 0.67816 val_auc= 0.73617 time= 0.41912\n",
            "Epoch: 0061 train_loss= 0.64405 train_acc= 0.75638 train_auc= 0.84516 val_loss= 0.66743 val_acc= 0.72414 val_auc= 0.79309 time= 0.38965\n",
            "Epoch: 0060 train_loss= 0.65106 train_acc= 0.72832 train_auc= 0.81721 val_loss= 0.68946 val_acc= 0.64368 val_auc= 0.72074 time= 0.41114\n",
            "Epoch: 0061 train_loss= 0.64184 train_acc= 0.75638 train_auc= 0.84893 val_loss= 0.70291 val_acc= 0.67816 val_auc= 0.68511 time= 0.44627\n",
            "Epoch: 0060 train_loss= 0.64834 train_acc= 0.72321 train_auc= 0.82529 val_loss= 0.67757 val_acc= 0.72414 val_auc= 0.79734 time= 0.38285\n",
            "Epoch: 0061 train_loss= 0.64288 train_acc= 0.74235 train_auc= 0.84727 val_loss= 0.69193 val_acc= 0.67816 val_auc= 0.75691 time= 0.37834\n",
            "Epoch: 0061 train_loss= 0.64239 train_acc= 0.75096 train_auc= 0.84960 val_loss= 0.68696 val_acc= 0.64773 val_auc= 0.71770 time= 0.45396\n",
            "Epoch: 0061 train_loss= 0.63939 train_acc= 0.77168 train_auc= 0.85756 val_loss= 0.69498 val_acc= 0.62069 val_auc= 0.73012 time= 0.43997\n",
            "Epoch: 0061 train_loss= 0.64296 train_acc= 0.74617 train_auc= 0.85117 val_loss= 0.67745 val_acc= 0.67816 val_auc= 0.77553 time= 0.44497\n",
            "Epoch: 0062 train_loss= 0.64410 train_acc= 0.74107 train_auc= 0.83520 val_loss= 0.70020 val_acc= 0.64368 val_auc= 0.67550 time= 0.39832\n",
            "Epoch: 0062 train_loss= 0.64477 train_acc= 0.76148 train_auc= 0.84267 val_loss= 0.66663 val_acc= 0.72414 val_auc= 0.79362 time= 0.41883\n",
            "Epoch: 0061 train_loss= 0.64602 train_acc= 0.75893 train_auc= 0.84464 val_loss= 0.68090 val_acc= 0.68966 val_auc= 0.73617 time= 0.44598\n",
            "Epoch: 0062 train_loss= 0.64269 train_acc= 0.75510 train_auc= 0.84279 val_loss= 0.70324 val_acc= 0.66667 val_auc= 0.68723 time= 0.36632\n",
            "Epoch: 0062 train_loss= 0.64333 train_acc= 0.75351 train_auc= 0.83861 val_loss= 0.68646 val_acc= 0.65909 val_auc= 0.71821 time= 0.34128\n",
            "Epoch: 0061 train_loss= 0.64393 train_acc= 0.75255 train_auc= 0.84306 val_loss= 0.68941 val_acc= 0.65517 val_auc= 0.72021 time= 0.42225\n",
            "Epoch: 0061 train_loss= 0.64234 train_acc= 0.75383 train_auc= 0.85350 val_loss= 0.67755 val_acc= 0.72414 val_auc= 0.79894 time= 0.44043\n",
            "Epoch: 0062 train_loss= 0.64064 train_acc= 0.74362 train_auc= 0.84738 val_loss= 0.69168 val_acc= 0.65517 val_auc= 0.75691 time= 0.41660\n",
            "Epoch: 0062 train_loss= 0.64060 train_acc= 0.76531 train_auc= 0.85021 val_loss= 0.69521 val_acc= 0.63218 val_auc= 0.72853 time= 0.34687\n",
            "Epoch: 0062 train_loss= 0.64275 train_acc= 0.75000 train_auc= 0.84469 val_loss= 0.67627 val_acc= 0.67816 val_auc= 0.77713 time= 0.35531\n",
            "Epoch: 0063 train_loss= 0.64331 train_acc= 0.75128 train_auc= 0.84098 val_loss= 0.70011 val_acc= 0.64368 val_auc= 0.67709 time= 0.39748\n",
            "Epoch: 0063 train_loss= 0.64457 train_acc= 0.75510 train_auc= 0.84117 val_loss= 0.66522 val_acc= 0.72414 val_auc= 0.79521 time= 0.41622\n",
            "Epoch: 0062 train_loss= 0.64328 train_acc= 0.73087 train_auc= 0.83684 val_loss= 0.68046 val_acc= 0.68966 val_auc= 0.73617 time= 0.41940\n",
            "Epoch: 0062 train_loss= 0.64437 train_acc= 0.75510 train_auc= 0.84414 val_loss= 0.68904 val_acc= 0.66667 val_auc= 0.72819 time= 0.36950\n",
            "Epoch: 0063 train_loss= 0.64609 train_acc= 0.75383 train_auc= 0.83822 val_loss= 0.70353 val_acc= 0.66667 val_auc= 0.68457 time= 0.43397\n",
            "Epoch: 0063 train_loss= 0.64465 train_acc= 0.74074 train_auc= 0.83034 val_loss= 0.68639 val_acc= 0.65909 val_auc= 0.71977 time= 0.46640\n",
            "Epoch: 0062 train_loss= 0.64043 train_acc= 0.76020 train_auc= 0.85574 val_loss= 0.67730 val_acc= 0.73563 val_auc= 0.79947 time= 0.41541\n",
            "Epoch: 0063 train_loss= 0.64617 train_acc= 0.75255 train_auc= 0.83499 val_loss= 0.69514 val_acc= 0.63218 val_auc= 0.72853 time= 0.39993\n",
            "Epoch: 0063 train_loss= 0.64566 train_acc= 0.73852 train_auc= 0.83275 val_loss= 0.69167 val_acc= 0.65517 val_auc= 0.75851 time= 0.43240\n",
            "Epoch: 0063 train_loss= 0.64452 train_acc= 0.75128 train_auc= 0.84007 val_loss= 0.67562 val_acc= 0.65517 val_auc= 0.77713 time= 0.37709\n",
            "Epoch: 0064 train_loss= 0.64725 train_acc= 0.73087 train_auc= 0.81914 val_loss= 0.70024 val_acc= 0.64368 val_auc= 0.68028 time= 0.44014\n",
            "Epoch: 0064 train_loss= 0.64992 train_acc= 0.74362 train_auc= 0.82306 val_loss= 0.66459 val_acc= 0.72414 val_auc= 0.79681 time= 0.35726\n",
            "Epoch: 0064 train_loss= 0.64851 train_acc= 0.73980 train_auc= 0.82347 val_loss= 0.70385 val_acc= 0.66667 val_auc= 0.68404 time= 0.37810\n",
            "Epoch: 0063 train_loss= 0.64478 train_acc= 0.75383 train_auc= 0.84467 val_loss= 0.68016 val_acc= 0.67816 val_auc= 0.73777 time= 0.41681\n",
            "Epoch: 0063 train_loss= 0.64776 train_acc= 0.74362 train_auc= 0.83717 val_loss= 0.67714 val_acc= 0.73563 val_auc= 0.80000 time= 0.38520\n",
            "Epoch: 0063 train_loss= 0.64634 train_acc= 0.74362 train_auc= 0.83054 val_loss= 0.68873 val_acc= 0.65517 val_auc= 0.73404 time= 0.47335\n",
            "Epoch: 0064 train_loss= 0.64988 train_acc= 0.72031 train_auc= 0.80918 val_loss= 0.68623 val_acc= 0.65909 val_auc= 0.72081 time= 0.43981\n",
            "Epoch: 0064 train_loss= 0.65388 train_acc= 0.71811 train_auc= 0.80428 val_loss= 0.67521 val_acc= 0.65517 val_auc= 0.77660 time= 0.35094\n",
            "Epoch: 0064 train_loss= 0.64790 train_acc= 0.72577 train_auc= 0.81939 val_loss= 0.69505 val_acc= 0.63218 val_auc= 0.72959 time= 0.39239\n",
            "Epoch: 0064 train_loss= 0.65115 train_acc= 0.73597 train_auc= 0.80684 val_loss= 0.69164 val_acc= 0.65517 val_auc= 0.75745 time= 0.42203\n",
            "Epoch: 0065 train_loss= 0.63652 train_acc= 0.76020 train_auc= 0.84972 val_loss= 0.70047 val_acc= 0.66667 val_auc= 0.67922 time= 0.42380\n",
            "Epoch: 0065 train_loss= 0.63823 train_acc= 0.74745 train_auc= 0.84437 val_loss= 0.66451 val_acc= 0.72414 val_auc= 0.79574 time= 0.41824\n",
            "Epoch: 0064 train_loss= 0.64924 train_acc= 0.72832 train_auc= 0.82018 val_loss= 0.67989 val_acc= 0.67816 val_auc= 0.74202 time= 0.38166\n",
            "Epoch: 0065 train_loss= 0.63696 train_acc= 0.75638 train_auc= 0.85365 val_loss= 0.70410 val_acc= 0.65517 val_auc= 0.68511 time= 0.41613\n",
            "Epoch: 0064 train_loss= 0.64988 train_acc= 0.72577 train_auc= 0.82056 val_loss= 0.67704 val_acc= 0.73563 val_auc= 0.80213 time= 0.39851\n",
            "Epoch: 0065 train_loss= 0.63784 train_acc= 0.76118 train_auc= 0.84542 val_loss= 0.68597 val_acc= 0.65909 val_auc= 0.72392 time= 0.38742\n",
            "Epoch: 0065 train_loss= 0.63731 train_acc= 0.76658 train_auc= 0.85380 val_loss= 0.67531 val_acc= 0.64368 val_auc= 0.77500 time= 0.38261\n",
            "Epoch: 0065 train_loss= 0.63513 train_acc= 0.76531 train_auc= 0.84727 val_loss= 0.69160 val_acc= 0.65517 val_auc= 0.75745 time= 0.36506\n",
            "Epoch: 0064 train_loss= 0.65281 train_acc= 0.72194 train_auc= 0.80485 val_loss= 0.68881 val_acc= 0.65517 val_auc= 0.73245 time= 0.48193\n",
            "Epoch: 0065 train_loss= 0.63558 train_acc= 0.76020 train_auc= 0.84754 val_loss= 0.69533 val_acc= 0.63218 val_auc= 0.73012 time= 0.44632\n",
            "Epoch: 0066 train_loss= 0.63827 train_acc= 0.76913 train_auc= 0.85321 val_loss= 0.70098 val_acc= 0.64368 val_auc= 0.67815 time= 0.37216\n",
            "Epoch: 0065 train_loss= 0.63739 train_acc= 0.76658 train_auc= 0.85518 val_loss= 0.67983 val_acc= 0.68966 val_auc= 0.74255 time= 0.34725\n",
            "Epoch: 0066 train_loss= 0.64062 train_acc= 0.75638 train_auc= 0.84598 val_loss= 0.66413 val_acc= 0.70115 val_auc= 0.79574 time= 0.42575\n",
            "Epoch: 0066 train_loss= 0.63777 train_acc= 0.76658 train_auc= 0.85250 val_loss= 0.70423 val_acc= 0.65517 val_auc= 0.68617 time= 0.40935\n",
            "Epoch: 0065 train_loss= 0.63989 train_acc= 0.76148 train_auc= 0.84676 val_loss= 0.67692 val_acc= 0.73563 val_auc= 0.80160 time= 0.40186\n",
            "Epoch: 0066 train_loss= 0.64031 train_acc= 0.75990 train_auc= 0.84238 val_loss= 0.68547 val_acc= 0.65909 val_auc= 0.72392 time= 0.42049\n",
            "Epoch: 0066 train_loss= 0.63843 train_acc= 0.75765 train_auc= 0.85414 val_loss= 0.67600 val_acc= 0.65517 val_auc= 0.77872 time= 0.40408\n",
            "Epoch: 0066 train_loss= 0.63716 train_acc= 0.74745 train_auc= 0.84133 val_loss= 0.69194 val_acc= 0.67816 val_auc= 0.75638 time= 0.40670\n",
            "Epoch: 0066 train_loss= 0.63744 train_acc= 0.75638 train_auc= 0.84441 val_loss= 0.69494 val_acc= 0.63218 val_auc= 0.73224 time= 0.40952\n",
            "Epoch: 0065 train_loss= 0.63750 train_acc= 0.75638 train_auc= 0.84591 val_loss= 0.68923 val_acc= 0.65517 val_auc= 0.73245 time= 0.43516\n",
            "Epoch: 0067 train_loss= 0.63884 train_acc= 0.76531 train_auc= 0.85415 val_loss= 0.70082 val_acc= 0.63218 val_auc= 0.67603 time= 0.52404\n",
            "Epoch: 0067 train_loss= 0.63727 train_acc= 0.75128 train_auc= 0.84945 val_loss= 0.66330 val_acc= 0.68966 val_auc= 0.79628 time= 0.48795\n",
            "Epoch: 0066 train_loss= 0.64158 train_acc= 0.77806 train_auc= 0.85369 val_loss= 0.67973 val_acc= 0.67816 val_auc= 0.74468 time= 0.54286\n",
            "Epoch: 0066 train_loss= 0.64062 train_acc= 0.77168 train_auc= 0.85295 val_loss= 0.67635 val_acc= 0.72414 val_auc= 0.80266 time= 0.43084\n",
            "Epoch: 0067 train_loss= 0.63648 train_acc= 0.74617 train_auc= 0.84942 val_loss= 0.70462 val_acc= 0.65517 val_auc= 0.68670 time= 0.51204\n",
            "Epoch: 0067 train_loss= 0.63887 train_acc= 0.76531 train_auc= 0.85373 val_loss= 0.67559 val_acc= 0.66667 val_auc= 0.77872 time= 0.42987\n",
            "Epoch: 0067 train_loss= 0.63724 train_acc= 0.76531 train_auc= 0.84784 val_loss= 0.69206 val_acc= 0.67816 val_auc= 0.75426 time= 0.43506\n",
            "Epoch: 0067 train_loss= 0.63824 train_acc= 0.74202 train_auc= 0.84254 val_loss= 0.68502 val_acc= 0.67045 val_auc= 0.72496 time= 0.46133\n",
            "Epoch: 0067 train_loss= 0.63436 train_acc= 0.75510 train_auc= 0.85262 val_loss= 0.69450 val_acc= 0.64368 val_auc= 0.73383 time= 0.48081\n",
            "Epoch: 0066 train_loss= 0.64200 train_acc= 0.74872 train_auc= 0.84085 val_loss= 0.68968 val_acc= 0.67816 val_auc= 0.72926 time= 0.50292\n",
            "Epoch: 0068 train_loss= 0.63928 train_acc= 0.74745 train_auc= 0.83928 val_loss= 0.66278 val_acc= 0.70115 val_auc= 0.79628 time= 0.41229\n",
            "Epoch: 0068 train_loss= 0.63200 train_acc= 0.76913 train_auc= 0.86946 val_loss= 0.70102 val_acc= 0.66667 val_auc= 0.67391 time= 0.42262\n",
            "Epoch: 0067 train_loss= 0.63240 train_acc= 0.78061 train_auc= 0.87004 val_loss= 0.67570 val_acc= 0.72414 val_auc= 0.80213 time= 0.38669\n",
            "Epoch: 0067 train_loss= 0.63796 train_acc= 0.76913 train_auc= 0.85297 val_loss= 0.67932 val_acc= 0.67816 val_auc= 0.74362 time= 0.45420\n",
            "Epoch: 0068 train_loss= 0.64146 train_acc= 0.75128 train_auc= 0.84449 val_loss= 0.67453 val_acc= 0.66667 val_auc= 0.77926 time= 0.41513\n",
            "Epoch: 0068 train_loss= 0.63821 train_acc= 0.74362 train_auc= 0.84048 val_loss= 0.70493 val_acc= 0.65517 val_auc= 0.68989 time= 0.41901\n",
            "Epoch: 0068 train_loss= 0.64338 train_acc= 0.73691 train_auc= 0.81934 val_loss= 0.68463 val_acc= 0.68182 val_auc= 0.72911 time= 0.41064\n",
            "Epoch: 0068 train_loss= 0.64116 train_acc= 0.73597 train_auc= 0.82829 val_loss= 0.69211 val_acc= 0.67816 val_auc= 0.75585 time= 0.48627\n",
            "Epoch: 0068 train_loss= 0.63483 train_acc= 0.75128 train_auc= 0.84198 val_loss= 0.69433 val_acc= 0.66667 val_auc= 0.73542 time= 0.45249\n",
            "Epoch: 0067 train_loss= 0.63738 train_acc= 0.76913 train_auc= 0.85462 val_loss= 0.69025 val_acc= 0.66667 val_auc= 0.72766 time= 0.41059\n",
            "Epoch: 0069 train_loss= 0.63624 train_acc= 0.75255 train_auc= 0.84093 val_loss= 0.66278 val_acc= 0.70115 val_auc= 0.79681 time= 0.42508\n",
            "Epoch: 0069 train_loss= 0.63263 train_acc= 0.77934 train_auc= 0.86617 val_loss= 0.70112 val_acc= 0.66667 val_auc= 0.67497 time= 0.47688\n",
            "Epoch: 0068 train_loss= 0.64491 train_acc= 0.73342 train_auc= 0.82629 val_loss= 0.67894 val_acc= 0.67816 val_auc= 0.74362 time= 0.45124\n",
            "Epoch: 0069 train_loss= 0.63745 train_acc= 0.74745 train_auc= 0.84102 val_loss= 0.67424 val_acc= 0.67816 val_auc= 0.78085 time= 0.41197\n",
            "Epoch: 0069 train_loss= 0.63272 train_acc= 0.75765 train_auc= 0.85119 val_loss= 0.70525 val_acc= 0.65517 val_auc= 0.69362 time= 0.40594\n",
            "Epoch: 0069 train_loss= 0.64040 train_acc= 0.74202 train_auc= 0.82614 val_loss= 0.68405 val_acc= 0.67045 val_auc= 0.73015 time= 0.39776\n",
            "Epoch: 0068 train_loss= 0.64380 train_acc= 0.73597 train_auc= 0.82222 val_loss= 0.67535 val_acc= 0.72414 val_auc= 0.80213 time= 0.48465\n",
            "Epoch: 0069 train_loss= 0.63594 train_acc= 0.74490 train_auc= 0.83323 val_loss= 0.69253 val_acc= 0.67816 val_auc= 0.75372 time= 0.42626\n",
            "Epoch: 0069 train_loss= 0.63485 train_acc= 0.74107 train_auc= 0.84295 val_loss= 0.69424 val_acc= 0.66667 val_auc= 0.73648 time= 0.37135\n",
            "Epoch: 0068 train_loss= 0.64332 train_acc= 0.72321 train_auc= 0.82753 val_loss= 0.69022 val_acc= 0.66667 val_auc= 0.72660 time= 0.38166\n",
            "Epoch: 0069 train_loss= 0.64024 train_acc= 0.73724 train_auc= 0.83208 val_loss= 0.67876 val_acc= 0.67816 val_auc= 0.74468 time= 0.37296\n",
            "Epoch: 0070 train_loss= 0.63202 train_acc= 0.79592 train_auc= 0.87172 val_loss= 0.70120 val_acc= 0.66667 val_auc= 0.67444 time= 0.41745\n",
            "Epoch: 0070 train_loss= 0.63557 train_acc= 0.77679 train_auc= 0.86240 val_loss= 0.66307 val_acc= 0.71264 val_auc= 0.79628 time= 0.44286\n",
            "Epoch: 0070 train_loss= 0.63951 train_acc= 0.75224 train_auc= 0.84394 val_loss= 0.68356 val_acc= 0.68182 val_auc= 0.73171 time= 0.40905\n",
            "Epoch: 0070 train_loss= 0.63804 train_acc= 0.76148 train_auc= 0.85333 val_loss= 0.67340 val_acc= 0.68966 val_auc= 0.78511 time= 0.42469\n",
            "Epoch: 0069 train_loss= 0.63778 train_acc= 0.73852 train_auc= 0.84226 val_loss= 0.67496 val_acc= 0.72414 val_auc= 0.80000 time= 0.45128\n",
            "Epoch: 0070 train_loss= 0.64011 train_acc= 0.76786 train_auc= 0.84636 val_loss= 0.70544 val_acc= 0.63218 val_auc= 0.69415 time= 0.55795\n",
            "Epoch: 0070 train_loss= 0.63610 train_acc= 0.76786 train_auc= 0.85820 val_loss= 0.69253 val_acc= 0.70115 val_auc= 0.75160 time= 0.42973\n",
            "Epoch: 0069 train_loss= 0.63695 train_acc= 0.75000 train_auc= 0.83877 val_loss= 0.68997 val_acc= 0.66667 val_auc= 0.72660 time= 0.40715\n",
            "Epoch: 0070 train_loss= 0.64115 train_acc= 0.74872 train_auc= 0.84053 val_loss= 0.69419 val_acc= 0.66667 val_auc= 0.73648 time= 0.51766\n",
            "Epoch: 0071 train_loss= 0.63264 train_acc= 0.76531 train_auc= 0.85247 val_loss= 0.70112 val_acc= 0.65517 val_auc= 0.67656 time= 0.43048\n",
            "Epoch: 0070 train_loss= 0.63810 train_acc= 0.76658 train_auc= 0.85331 val_loss= 0.67802 val_acc= 0.68966 val_auc= 0.74468 time= 0.43447\n",
            "Epoch: 0071 train_loss= 0.63801 train_acc= 0.75765 train_auc= 0.84070 val_loss= 0.66207 val_acc= 0.70115 val_auc= 0.79521 time= 0.43240\n",
            "Epoch: 0071 train_loss= 0.63480 train_acc= 0.74107 train_auc= 0.83997 val_loss= 0.67296 val_acc= 0.68966 val_auc= 0.78457 time= 0.40992\n",
            "Epoch: 0071 train_loss= 0.63242 train_acc= 0.77012 train_auc= 0.84997 val_loss= 0.68300 val_acc= 0.67045 val_auc= 0.73430 time= 0.48233\n",
            "Epoch: 0070 train_loss= 0.63383 train_acc= 0.76786 train_auc= 0.86654 val_loss= 0.67440 val_acc= 0.73563 val_auc= 0.80213 time= 0.48337\n",
            "Epoch: 0071 train_loss= 0.63583 train_acc= 0.74872 train_auc= 0.84335 val_loss= 0.70565 val_acc= 0.63218 val_auc= 0.69521 time= 0.42662\n",
            "Epoch: 0071 train_loss= 0.63062 train_acc= 0.76148 train_auc= 0.85872 val_loss= 0.69281 val_acc= 0.70115 val_auc= 0.75053 time= 0.47215\n",
            "Epoch: 0071 train_loss= 0.63468 train_acc= 0.75128 train_auc= 0.84457 val_loss= 0.69418 val_acc= 0.66667 val_auc= 0.73648 time= 0.41175\n",
            "Epoch: 0070 train_loss= 0.63904 train_acc= 0.76148 train_auc= 0.85129 val_loss= 0.68992 val_acc= 0.65517 val_auc= 0.73032 time= 0.51119\n",
            "Epoch: 0072 train_loss= 0.63294 train_acc= 0.77168 train_auc= 0.85131 val_loss= 0.66154 val_acc= 0.70115 val_auc= 0.79574 time= 0.38786\n",
            "Epoch: 0072 train_loss= 0.63078 train_acc= 0.77296 train_auc= 0.86045 val_loss= 0.67236 val_acc= 0.68966 val_auc= 0.78617 time= 0.38928\n",
            "Epoch: 0072 train_loss= 0.63059 train_acc= 0.77679 train_auc= 0.85796 val_loss= 0.70112 val_acc= 0.64368 val_auc= 0.67656 time= 0.46425\n",
            "Epoch: 0071 train_loss= 0.63381 train_acc= 0.75510 train_auc= 0.85280 val_loss= 0.67754 val_acc= 0.68966 val_auc= 0.74574 time= 0.44729\n",
            "Epoch: 0071 train_loss= 0.63164 train_acc= 0.76786 train_auc= 0.84686 val_loss= 0.67402 val_acc= 0.73563 val_auc= 0.80266 time= 0.40311\n",
            "Epoch: 0072 train_loss= 0.63381 train_acc= 0.77934 train_auc= 0.85379 val_loss= 0.70568 val_acc= 0.62069 val_auc= 0.69574 time= 0.41679\n",
            "Epoch: 0072 train_loss= 0.63656 train_acc= 0.75479 train_auc= 0.84036 val_loss= 0.68253 val_acc= 0.65909 val_auc= 0.73586 time= 0.47522\n",
            "Epoch: 0072 train_loss= 0.62990 train_acc= 0.76913 train_auc= 0.86099 val_loss= 0.69305 val_acc= 0.70115 val_auc= 0.75000 time= 0.43230\n",
            "Epoch: 0072 train_loss= 0.62588 train_acc= 0.78699 train_auc= 0.87200 val_loss= 0.69443 val_acc= 0.66667 val_auc= 0.73648 time= 0.38220\n",
            "Epoch: 0071 train_loss= 0.63750 train_acc= 0.74362 train_auc= 0.84112 val_loss= 0.69015 val_acc= 0.67816 val_auc= 0.73511 time= 0.41423\n",
            "Epoch: 0073 train_loss= 0.62884 train_acc= 0.76786 train_auc= 0.85890 val_loss= 0.66163 val_acc= 0.71264 val_auc= 0.79521 time= 0.44819\n",
            "Epoch: 0073 train_loss= 0.62645 train_acc= 0.78316 train_auc= 0.86756 val_loss= 0.67200 val_acc= 0.68966 val_auc= 0.78617 time= 0.42450\n",
            "Epoch: 0073 train_loss= 0.62737 train_acc= 0.77934 train_auc= 0.86657 val_loss= 0.70133 val_acc= 0.64368 val_auc= 0.67550 time= 0.46101\n",
            "Epoch: 0072 train_loss= 0.63209 train_acc= 0.77041 train_auc= 0.85692 val_loss= 0.67741 val_acc= 0.67816 val_auc= 0.74574 time= 0.46425\n",
            "Epoch: 0073 train_loss= 0.63018 train_acc= 0.78189 train_auc= 0.86552 val_loss= 0.70569 val_acc= 0.63218 val_auc= 0.69628 time= 0.44081\n",
            "Epoch: 0072 train_loss= 0.63482 train_acc= 0.75893 train_auc= 0.84942 val_loss= 0.67409 val_acc= 0.75862 val_auc= 0.80319 time= 0.47526\n",
            "Epoch: 0073 train_loss= 0.62684 train_acc= 0.76884 train_auc= 0.86305 val_loss= 0.68266 val_acc= 0.65909 val_auc= 0.73534 time= 0.44754\n",
            "Epoch: 0073 train_loss= 0.62395 train_acc= 0.78827 train_auc= 0.88226 val_loss= 0.69331 val_acc= 0.70115 val_auc= 0.74734 time= 0.44210\n",
            "Epoch: 0072 train_loss= 0.63232 train_acc= 0.78699 train_auc= 0.86418 val_loss= 0.68987 val_acc= 0.67816 val_auc= 0.73457 time= 0.46426\n",
            "Epoch: 0073 train_loss= 0.62700 train_acc= 0.77296 train_auc= 0.86821 val_loss= 0.69488 val_acc= 0.66667 val_auc= 0.73701 time= 0.49933\n",
            "Epoch: 0074 train_loss= 0.62919 train_acc= 0.76276 train_auc= 0.85965 val_loss= 0.66185 val_acc= 0.71264 val_auc= 0.79628 time= 0.44810\n",
            "Epoch: 0074 train_loss= 0.62758 train_acc= 0.78571 train_auc= 0.86172 val_loss= 0.67208 val_acc= 0.67816 val_auc= 0.78777 time= 0.45060\n",
            "Epoch: 0074 train_loss= 0.62469 train_acc= 0.76658 train_auc= 0.86537 val_loss= 0.70180 val_acc= 0.64368 val_auc= 0.67497 time= 0.49353\n",
            "Epoch: 0074 train_loss= 0.62903 train_acc= 0.77423 train_auc= 0.86634 val_loss= 0.70576 val_acc= 0.64368 val_auc= 0.69840 time= 0.45805\n",
            "Epoch: 0073 train_loss= 0.63456 train_acc= 0.77679 train_auc= 0.85402 val_loss= 0.67718 val_acc= 0.67816 val_auc= 0.74521 time= 0.51969\n",
            "Epoch: 0073 train_loss= 0.62633 train_acc= 0.78571 train_auc= 0.87198 val_loss= 0.67423 val_acc= 0.73563 val_auc= 0.80160 time= 0.47832\n",
            "Epoch: 0074 train_loss= 0.62341 train_acc= 0.78927 train_auc= 0.87662 val_loss= 0.68297 val_acc= 0.65909 val_auc= 0.73534 time= 0.50175\n",
            "Epoch: 0074 train_loss= 0.62715 train_acc= 0.77168 train_auc= 0.85731 val_loss= 0.69339 val_acc= 0.68966 val_auc= 0.74521 time= 0.47245\n",
            "Epoch: 0073 train_loss= 0.63180 train_acc= 0.77296 train_auc= 0.86205 val_loss= 0.68885 val_acc= 0.66667 val_auc= 0.73457 time= 0.43938\n",
            "Epoch: 0074 train_loss= 0.62431 train_acc= 0.77423 train_auc= 0.87091 val_loss= 0.69552 val_acc= 0.64368 val_auc= 0.73436 time= 0.52572\n",
            "Epoch: 0075 train_loss= 0.63141 train_acc= 0.75638 train_auc= 0.84848 val_loss= 0.67168 val_acc= 0.67816 val_auc= 0.78777 time= 0.43180\n",
            "Epoch: 0075 train_loss= 0.63025 train_acc= 0.76913 train_auc= 0.85394 val_loss= 0.66130 val_acc= 0.72414 val_auc= 0.79415 time= 0.48807\n",
            "Epoch: 0075 train_loss= 0.62891 train_acc= 0.77296 train_auc= 0.86094 val_loss= 0.70179 val_acc= 0.64368 val_auc= 0.67709 time= 0.42477\n",
            "Epoch: 0075 train_loss= 0.63505 train_acc= 0.73980 train_auc= 0.83884 val_loss= 0.70620 val_acc= 0.65517 val_auc= 0.69734 time= 0.44778\n",
            "Epoch: 0074 train_loss= 0.63322 train_acc= 0.75000 train_auc= 0.84624 val_loss= 0.67713 val_acc= 0.67816 val_auc= 0.74947 time= 0.45242\n",
            "Epoch: 0074 train_loss= 0.63153 train_acc= 0.76531 train_auc= 0.85916 val_loss= 0.67407 val_acc= 0.73563 val_auc= 0.79947 time= 0.45987\n",
            "Epoch: 0075 train_loss= 0.63321 train_acc= 0.77267 train_auc= 0.85047 val_loss= 0.68261 val_acc= 0.67045 val_auc= 0.73638 time= 0.46003\n",
            "Epoch: 0075 train_loss= 0.62732 train_acc= 0.77296 train_auc= 0.85972 val_loss= 0.69296 val_acc= 0.68966 val_auc= 0.74681 time= 0.39975\n",
            "Epoch: 0074 train_loss= 0.63147 train_acc= 0.78061 train_auc= 0.85583 val_loss= 0.68816 val_acc= 0.65517 val_auc= 0.73404 time= 0.41827\n",
            "Epoch: 0075 train_loss= 0.62807 train_acc= 0.78444 train_auc= 0.86418 val_loss= 0.69605 val_acc= 0.64368 val_auc= 0.73436 time= 0.38165\n",
            "Epoch: 0076 train_loss= 0.62815 train_acc= 0.76148 train_auc= 0.85123 val_loss= 0.67090 val_acc= 0.68966 val_auc= 0.78723 time= 0.47261\n",
            "Epoch: 0076 train_loss= 0.62988 train_acc= 0.75383 train_auc= 0.84239 val_loss= 0.66067 val_acc= 0.72414 val_auc= 0.79255 time= 0.45648\n",
            "Epoch: 0076 train_loss= 0.62653 train_acc= 0.75128 train_auc= 0.84695 val_loss= 0.70182 val_acc= 0.65517 val_auc= 0.67656 time= 0.45093\n",
            "Epoch: 0076 train_loss= 0.63089 train_acc= 0.75510 train_auc= 0.84845 val_loss= 0.70643 val_acc= 0.65517 val_auc= 0.69787 time= 0.40971\n",
            "Epoch: 0075 train_loss= 0.62922 train_acc= 0.76403 train_auc= 0.85386 val_loss= 0.67689 val_acc= 0.67816 val_auc= 0.74947 time= 0.41053\n",
            "Epoch: 0075 train_loss= 0.62906 train_acc= 0.76276 train_auc= 0.85933 val_loss= 0.67422 val_acc= 0.74713 val_auc= 0.79734 time= 0.42354\n",
            "Epoch: 0076 train_loss= 0.63095 train_acc= 0.73469 train_auc= 0.83317 val_loss= 0.69287 val_acc= 0.67816 val_auc= 0.74787 time= 0.40169\n",
            "Epoch: 0076 train_loss= 0.63093 train_acc= 0.76373 train_auc= 0.83944 val_loss= 0.68246 val_acc= 0.67045 val_auc= 0.73742 time= 0.44418\n",
            "Epoch: 0075 train_loss= 0.63374 train_acc= 0.75128 train_auc= 0.83885 val_loss= 0.68717 val_acc= 0.68966 val_auc= 0.73351 time= 0.41188\n",
            "Epoch: 0076 train_loss= 0.62988 train_acc= 0.74490 train_auc= 0.84680 val_loss= 0.69619 val_acc= 0.62069 val_auc= 0.73489 time= 0.46493\n",
            "Epoch: 0077 train_loss= 0.63360 train_acc= 0.75383 train_auc= 0.83343 val_loss= 0.66029 val_acc= 0.72414 val_auc= 0.79202 time= 0.41932\n",
            "Epoch: 0077 train_loss= 0.62700 train_acc= 0.76531 train_auc= 0.86012 val_loss= 0.67030 val_acc= 0.67816 val_auc= 0.78777 time= 0.43062\n",
            "Epoch: 0077 train_loss= 0.62230 train_acc= 0.78571 train_auc= 0.87245 val_loss= 0.70688 val_acc= 0.65517 val_auc= 0.69574 time= 0.39838\n",
            "Epoch: 0077 train_loss= 0.61944 train_acc= 0.78061 train_auc= 0.86748 val_loss= 0.70188 val_acc= 0.64368 val_auc= 0.67656 time= 0.43297\n",
            "Epoch: 0076 train_loss= 0.63494 train_acc= 0.72832 train_auc= 0.83262 val_loss= 0.67635 val_acc= 0.68966 val_auc= 0.74681 time= 0.40278\n",
            "Epoch: 0076 train_loss= 0.62833 train_acc= 0.76786 train_auc= 0.84845 val_loss= 0.67429 val_acc= 0.73563 val_auc= 0.79787 time= 0.41533\n",
            "Epoch: 0077 train_loss= 0.62565 train_acc= 0.76373 train_auc= 0.85099 val_loss= 0.68258 val_acc= 0.65909 val_auc= 0.73638 time= 0.40357\n",
            "Epoch: 0077 train_loss= 0.62732 train_acc= 0.75383 train_auc= 0.84680 val_loss= 0.69305 val_acc= 0.67816 val_auc= 0.74734 time= 0.46808\n",
            "Epoch: 0076 train_loss= 0.63343 train_acc= 0.74617 train_auc= 0.83700 val_loss= 0.68629 val_acc= 0.66667 val_auc= 0.73723 time= 0.39731\n",
            "Epoch: 0077 train_loss= 0.62492 train_acc= 0.76658 train_auc= 0.85876 val_loss= 0.69623 val_acc= 0.62069 val_auc= 0.73648 time= 0.43457\n",
            "Epoch: 0078 train_loss= 0.63003 train_acc= 0.74107 train_auc= 0.83935 val_loss= 0.66029 val_acc= 0.72414 val_auc= 0.79149 time= 0.39083\n",
            "Epoch: 0078 train_loss= 0.62202 train_acc= 0.77551 train_auc= 0.86594 val_loss= 0.67012 val_acc= 0.70115 val_auc= 0.78670 time= 0.38180\n",
            "Epoch: 0078 train_loss= 0.61485 train_acc= 0.78571 train_auc= 0.88569 val_loss= 0.70206 val_acc= 0.63218 val_auc= 0.67815 time= 0.40788\n",
            "Epoch: 0077 train_loss= 0.62778 train_acc= 0.74872 train_auc= 0.85453 val_loss= 0.67617 val_acc= 0.68966 val_auc= 0.74521 time= 0.43845\n",
            "Epoch: 0078 train_loss= 0.62158 train_acc= 0.77168 train_auc= 0.87225 val_loss= 0.70736 val_acc= 0.64368 val_auc= 0.69362 time= 0.46128\n",
            "Epoch: 0077 train_loss= 0.62731 train_acc= 0.76148 train_auc= 0.84914 val_loss= 0.67456 val_acc= 0.72414 val_auc= 0.79574 time= 0.45082\n",
            "Epoch: 0078 train_loss= 0.62321 train_acc= 0.76756 train_auc= 0.86057 val_loss= 0.68268 val_acc= 0.67045 val_auc= 0.73482 time= 0.41729\n",
            "Epoch: 0078 train_loss= 0.61964 train_acc= 0.77934 train_auc= 0.86140 val_loss= 0.69314 val_acc= 0.68966 val_auc= 0.74628 time= 0.40699\n",
            "Epoch: 0077 train_loss= 0.62951 train_acc= 0.73852 train_auc= 0.84088 val_loss= 0.68541 val_acc= 0.67816 val_auc= 0.73670 time= 0.45003\n",
            "Epoch: 0079 train_loss= 0.63141 train_acc= 0.76403 train_auc= 0.83995 val_loss= 0.66984 val_acc= 0.70115 val_auc= 0.78511 time= 0.40723\n",
            "Epoch: 0078 train_loss= 0.61995 train_acc= 0.76786 train_auc= 0.86580 val_loss= 0.69606 val_acc= 0.63218 val_auc= 0.73913 time= 0.43528\n",
            "Epoch: 0079 train_loss= 0.62815 train_acc= 0.76913 train_auc= 0.85047 val_loss= 0.66051 val_acc= 0.68966 val_auc= 0.79202 time= 0.45427\n",
            "Epoch: 0079 train_loss= 0.62236 train_acc= 0.78444 train_auc= 0.86003 val_loss= 0.70253 val_acc= 0.63218 val_auc= 0.67922 time= 0.40894\n",
            "Epoch: 0078 train_loss= 0.63143 train_acc= 0.74107 train_auc= 0.84618 val_loss= 0.67580 val_acc= 0.68966 val_auc= 0.74362 time= 0.40102\n",
            "Epoch: 0079 train_loss= 0.62356 train_acc= 0.77423 train_auc= 0.85895 val_loss= 0.70788 val_acc= 0.63218 val_auc= 0.69309 time= 0.40446\n",
            "Epoch: 0079 train_loss= 0.62554 train_acc= 0.76756 train_auc= 0.85242 val_loss= 0.68235 val_acc= 0.67045 val_auc= 0.73638 time= 0.36105\n",
            "Epoch: 0078 train_loss= 0.62098 train_acc= 0.76786 train_auc= 0.86503 val_loss= 0.67415 val_acc= 0.73563 val_auc= 0.79468 time= 0.42259\n",
            "Epoch: 0079 train_loss= 0.62522 train_acc= 0.77551 train_auc= 0.84998 val_loss= 0.69312 val_acc= 0.68966 val_auc= 0.74840 time= 0.48323\n",
            "Epoch: 0078 train_loss= 0.62728 train_acc= 0.73342 train_auc= 0.84226 val_loss= 0.68505 val_acc= 0.66667 val_auc= 0.73989 time= 0.43334\n",
            "Epoch: 0079 train_loss= 0.62268 train_acc= 0.75765 train_auc= 0.85683 val_loss= 0.69600 val_acc= 0.65517 val_auc= 0.73542 time= 0.48621\n",
            "Epoch: 0080 train_loss= 0.62836 train_acc= 0.77041 train_auc= 0.84927 val_loss= 0.66995 val_acc= 0.70115 val_auc= 0.78511 time= 0.51465\n",
            "Epoch: 0080 train_loss= 0.62789 train_acc= 0.77168 train_auc= 0.84945 val_loss= 0.66018 val_acc= 0.65517 val_auc= 0.78989 time= 0.48356\n",
            "Epoch: 0080 train_loss= 0.62125 train_acc= 0.77296 train_auc= 0.86368 val_loss= 0.70319 val_acc= 0.63218 val_auc= 0.68028 time= 0.45908\n",
            "Epoch: 0080 train_loss= 0.62314 train_acc= 0.78699 train_auc= 0.86683 val_loss= 0.70844 val_acc= 0.62069 val_auc= 0.69309 time= 0.52452\n",
            "Epoch: 0079 train_loss= 0.62950 train_acc= 0.76020 train_auc= 0.84404 val_loss= 0.67537 val_acc= 0.70115 val_auc= 0.74468 time= 0.50268\n",
            "Epoch: 0079 train_loss= 0.62840 train_acc= 0.75383 train_auc= 0.84589 val_loss= 0.67345 val_acc= 0.74713 val_auc= 0.79521 time= 0.45908\n",
            "Epoch: 0080 train_loss= 0.62941 train_acc= 0.77267 train_auc= 0.84967 val_loss= 0.68208 val_acc= 0.68182 val_auc= 0.73586 time= 0.51273\n",
            "Epoch: 0079 train_loss= 0.63060 train_acc= 0.76531 train_auc= 0.84023 val_loss= 0.68487 val_acc= 0.66667 val_auc= 0.74043 time= 0.38618\n",
            "Epoch: 0080 train_loss= 0.62334 train_acc= 0.78954 train_auc= 0.86332 val_loss= 0.69302 val_acc= 0.68966 val_auc= 0.74734 time= 0.42375\n",
            "Epoch: 0081 train_loss= 0.63013 train_acc= 0.75765 train_auc= 0.84489 val_loss= 0.65998 val_acc= 0.67816 val_auc= 0.78670 time= 0.38726\n",
            "Epoch: 0081 train_loss= 0.61970 train_acc= 0.79847 train_auc= 0.87722 val_loss= 0.66973 val_acc= 0.72414 val_auc= 0.78617 time= 0.42584\n",
            "Epoch: 0081 train_loss= 0.62350 train_acc= 0.76531 train_auc= 0.85798 val_loss= 0.70383 val_acc= 0.63218 val_auc= 0.68293 time= 0.40376\n",
            "Epoch: 0080 train_loss= 0.62789 train_acc= 0.77168 train_auc= 0.85156 val_loss= 0.67508 val_acc= 0.70115 val_auc= 0.74734 time= 0.41436\n",
            "Epoch: 0080 train_loss= 0.62557 train_acc= 0.76658 train_auc= 0.84657 val_loss= 0.69540 val_acc= 0.66667 val_auc= 0.73595 time= 0.58835\n",
            "Epoch: 0080 train_loss= 0.62710 train_acc= 0.76020 train_auc= 0.85001 val_loss= 0.67229 val_acc= 0.73563 val_auc= 0.79521 time= 0.40279\n",
            "Epoch: 0081 train_loss= 0.62635 train_acc= 0.77296 train_auc= 0.85446 val_loss= 0.70892 val_acc= 0.63218 val_auc= 0.69362 time= 0.44902\n",
            "Epoch: 0081 train_loss= 0.62693 train_acc= 0.76756 train_auc= 0.84902 val_loss= 0.68183 val_acc= 0.68182 val_auc= 0.73638 time= 0.40428\n",
            "Epoch: 0080 train_loss= 0.63177 train_acc= 0.74872 train_auc= 0.83634 val_loss= 0.68510 val_acc= 0.67816 val_auc= 0.74043 time= 0.40638\n",
            "Epoch: 0081 train_loss= 0.62249 train_acc= 0.76913 train_auc= 0.85990 val_loss= 0.69287 val_acc= 0.68966 val_auc= 0.74681 time= 0.41236\n",
            "Epoch: 0082 train_loss= 0.62038 train_acc= 0.78699 train_auc= 0.87690 val_loss= 0.66892 val_acc= 0.72414 val_auc= 0.78617 time= 0.38202\n",
            "Epoch: 0082 train_loss= 0.62400 train_acc= 0.77806 train_auc= 0.86163 val_loss= 0.65940 val_acc= 0.67816 val_auc= 0.78723 time= 0.41181\n",
            "Epoch: 0082 train_loss= 0.61883 train_acc= 0.78571 train_auc= 0.88003 val_loss= 0.70454 val_acc= 0.63218 val_auc= 0.68028 time= 0.48549\n",
            "Epoch: 0082 train_loss= 0.62199 train_acc= 0.79082 train_auc= 0.87200 val_loss= 0.70919 val_acc= 0.63218 val_auc= 0.69255 time= 0.39643\n",
            "Epoch: 0081 train_loss= 0.62194 train_acc= 0.77679 train_auc= 0.85440 val_loss= 0.69463 val_acc= 0.66667 val_auc= 0.73595 time= 0.39519\n",
            "Epoch: 0081 train_loss= 0.62849 train_acc= 0.74617 train_auc= 0.84856 val_loss= 0.67505 val_acc= 0.70115 val_auc= 0.74628 time= 0.46974\n",
            "Epoch: 0082 train_loss= 0.62266 train_acc= 0.76501 train_auc= 0.86490 val_loss= 0.68184 val_acc= 0.69318 val_auc= 0.74001 time= 0.42079\n",
            "Epoch: 0081 train_loss= 0.62715 train_acc= 0.76020 train_auc= 0.84977 val_loss= 0.67116 val_acc= 0.74713 val_auc= 0.79734 time= 0.46758\n",
            "Epoch: 0081 train_loss= 0.62825 train_acc= 0.77679 train_auc= 0.85418 val_loss= 0.68593 val_acc= 0.66667 val_auc= 0.73936 time= 0.43259\n",
            "Epoch: 0082 train_loss= 0.61838 train_acc= 0.78571 train_auc= 0.87114 val_loss= 0.69273 val_acc= 0.68966 val_auc= 0.74840 time= 0.43819\n",
            "Epoch: 0083 train_loss= 0.62312 train_acc= 0.78954 train_auc= 0.87171 val_loss= 0.66836 val_acc= 0.72414 val_auc= 0.78723 time= 0.41443\n",
            "Epoch: 0083 train_loss= 0.62120 train_acc= 0.79082 train_auc= 0.87981 val_loss= 0.65979 val_acc= 0.66667 val_auc= 0.78830 time= 0.43429\n",
            "Epoch: 0083 train_loss= 0.61966 train_acc= 0.78699 train_auc= 0.87811 val_loss= 0.70541 val_acc= 0.63218 val_auc= 0.67975 time= 0.41637\n",
            "Epoch: 0082 train_loss= 0.62248 train_acc= 0.75893 train_auc= 0.86116 val_loss= 0.69434 val_acc= 0.66667 val_auc= 0.73701 time= 0.38172\n",
            "Epoch: 0083 train_loss= 0.62235 train_acc= 0.78827 train_auc= 0.87382 val_loss= 0.70907 val_acc= 0.63218 val_auc= 0.69202 time= 0.43643\n",
            "Epoch: 0083 train_loss= 0.62045 train_acc= 0.77267 train_auc= 0.87020 val_loss= 0.68188 val_acc= 0.68182 val_auc= 0.73949 time= 0.40228\n",
            "Epoch: 0082 train_loss= 0.62202 train_acc= 0.76276 train_auc= 0.86388 val_loss= 0.67505 val_acc= 0.71264 val_auc= 0.74574 time= 0.42136\n",
            "Epoch: 0082 train_loss= 0.62302 train_acc= 0.77806 train_auc= 0.87355 val_loss= 0.68675 val_acc= 0.65517 val_auc= 0.73936 time= 0.39656\n",
            "Epoch: 0082 train_loss= 0.62014 train_acc= 0.78699 train_auc= 0.87705 val_loss= 0.67071 val_acc= 0.75862 val_auc= 0.79787 time= 0.51267\n",
            "Epoch: 0083 train_loss= 0.62064 train_acc= 0.77168 train_auc= 0.87234 val_loss= 0.69281 val_acc= 0.68966 val_auc= 0.74681 time= 0.42442\n",
            "Epoch: 0084 train_loss= 0.62186 train_acc= 0.78827 train_auc= 0.87176 val_loss= 0.66021 val_acc= 0.65517 val_auc= 0.79043 time= 0.37932\n",
            "Epoch: 0084 train_loss= 0.61378 train_acc= 0.79592 train_auc= 0.88777 val_loss= 0.66800 val_acc= 0.72414 val_auc= 0.78777 time= 0.46939\n",
            "Epoch: 0083 train_loss= 0.61551 train_acc= 0.79209 train_auc= 0.88193 val_loss= 0.69409 val_acc= 0.65517 val_auc= 0.73807 time= 0.40558\n",
            "Epoch: 0084 train_loss= 0.60872 train_acc= 0.81378 train_auc= 0.90081 val_loss= 0.70604 val_acc= 0.63218 val_auc= 0.67975 time= 0.40255\n",
            "Epoch: 0084 train_loss= 0.61691 train_acc= 0.79082 train_auc= 0.88520 val_loss= 0.70925 val_acc= 0.65517 val_auc= 0.69202 time= 0.39356\n",
            "Epoch: 0084 train_loss= 0.61460 train_acc= 0.79949 train_auc= 0.88912 val_loss= 0.68177 val_acc= 0.68182 val_auc= 0.73845 time= 0.37255\n",
            "Epoch:Epoch:  00830083  train_loss=train_loss=  0.623350.62196  train_acc=train_acc=  0.770410.80357  train_auc=train_auc= 0.89176 val_loss= 0.68732 val_acc= 0.65517 val_auc= 0.73830 time= 0.37973\n",
            " 0.86820 val_loss= 0.67480 val_acc= 0.68966 val_auc= 0.74734 time= 0.39776\n",
            "Epoch: 0083 train_loss= 0.61996 train_acc= 0.78444 train_auc= 0.87696 val_loss= 0.67051 val_acc= 0.75862 val_auc= 0.79947 time= 0.41330\n",
            "Epoch: 0084 train_loss= 0.61484 train_acc= 0.80357 train_auc= 0.88856 val_loss= 0.69314 val_acc= 0.70115 val_auc= 0.74894 time= 0.41152\n",
            "Epoch: 0085 train_loss= 0.62605 train_acc= 0.77806 train_auc= 0.85847 val_loss= 0.65884 val_acc= 0.65517 val_auc= 0.79202 time= 0.41082\n",
            "Epoch: 0085 train_loss= 0.62091 train_acc= 0.78444 train_auc= 0.87325 val_loss= 0.66712 val_acc= 0.72414 val_auc= 0.78777 time= 0.40919\n",
            "Epoch: 0085 train_loss= 0.62074 train_acc= 0.77423 train_auc= 0.88158 val_loss= 0.70988 val_acc= 0.64368 val_auc= 0.69096 time= 0.38215\n",
            "Epoch: 0085 train_loss= 0.61876 train_acc= 0.79209 train_auc= 0.87735 val_loss= 0.70645 val_acc= 0.66667 val_auc= 0.68028 time= 0.43272\n",
            "Epoch: 0085 train_loss= 0.62254 train_acc= 0.77139 train_auc= 0.86622 val_loss= 0.68162 val_acc= 0.67045 val_auc= 0.73793 time= 0.39123\n",
            "Epoch: 0084 train_loss= 0.61778 train_acc= 0.81378 train_auc= 0.89164 val_loss= 0.68723 val_acc= 0.66667 val_auc= 0.73830 time= 0.36764\n",
            "Epoch: 0084 train_loss= 0.61603 train_acc= 0.79337 train_auc= 0.88529 val_loss= 0.69397 val_acc= 0.66667 val_auc= 0.73860 time= 0.52802\n",
            "Epoch: 0084 train_loss= 0.61971 train_acc= 0.79464 train_auc= 0.87664 val_loss= 0.66998 val_acc= 0.77011 val_auc= 0.79947 time= 0.42643\n",
            "Epoch: 0084 train_loss= 0.62152 train_acc= 0.79464 train_auc= 0.87035 val_loss= 0.67515 val_acc= 0.67816 val_auc= 0.74521 time= 0.49765\n",
            "Epoch: 0085 train_loss= 0.62265 train_acc= 0.77423 train_auc= 0.86613 val_loss= 0.69388 val_acc= 0.66667 val_auc= 0.74947 time= 0.50954\n",
            "Epoch: 0086 train_loss= 0.61551 train_acc= 0.77934 train_auc= 0.87361 val_loss= 0.65774 val_acc= 0.67816 val_auc= 0.79096 time= 0.40042\n",
            "Epoch: 0086 train_loss= 0.61162 train_acc= 0.79974 train_auc= 0.88194 val_loss= 0.71049 val_acc= 0.65517 val_auc= 0.68989 time= 0.42895\n",
            "Epoch: 0086 train_loss= 0.60597 train_acc= 0.81378 train_auc= 0.89303 val_loss= 0.70708 val_acc= 0.63218 val_auc= 0.67975 time= 0.42983\n",
            "Epoch: 0086 train_loss= 0.61176 train_acc= 0.78672 train_auc= 0.87913 val_loss= 0.68118 val_acc= 0.67045 val_auc= 0.74157 time= 0.44289\n",
            "Epoch: 0085 train_loss= 0.61996 train_acc= 0.79464 train_auc= 0.87346 val_loss= 0.69406 val_acc= 0.65517 val_auc= 0.74390 time= 0.40502\n",
            "Epoch: 0086 train_loss= 0.60980 train_acc= 0.78699 train_auc= 0.88877 val_loss= 0.66639 val_acc= 0.70115 val_auc= 0.78936 time= 0.56894\n",
            "Epoch: 0085 train_loss= 0.62536 train_acc= 0.78189 train_auc= 0.86881 val_loss= 0.68699 val_acc= 0.66667 val_auc= 0.73936 time= 0.47230\n",
            "Epoch: 0085 train_loss= 0.62545 train_acc= 0.78061 train_auc= 0.86467 val_loss= 0.67469 val_acc= 0.67816 val_auc= 0.74681 time= 0.40061\n",
            "Epoch: 0085 train_loss= 0.61562 train_acc= 0.80357 train_auc= 0.88980 val_loss= 0.66964 val_acc= 0.75862 val_auc= 0.79894 time= 0.46757\n",
            "Epoch: 0086 train_loss= 0.60691 train_acc= 0.80230 train_auc= 0.89078 val_loss= 0.69364 val_acc= 0.68966 val_auc= 0.74787 time= 0.43102\n",
            "Epoch: 0087 train_loss= 0.62540 train_acc= 0.75893 train_auc= 0.85084 val_loss= 0.65737 val_acc= 0.70115 val_auc= 0.78883 time= 0.43712\n",
            "Epoch: 0087 train_loss= 0.61961 train_acc= 0.77296 train_auc= 0.86253 val_loss= 0.71094 val_acc= 0.63218 val_auc= 0.68723 time= 0.44192\n",
            "Epoch: 0087 train_loss= 0.61957 train_acc= 0.77168 train_auc= 0.85568 val_loss= 0.70759 val_acc= 0.64368 val_auc= 0.68081 time= 0.42069\n",
            "Epoch: 0086 train_loss= 0.61239 train_acc= 0.79592 train_auc= 0.88387 val_loss= 0.69416 val_acc= 0.66667 val_auc= 0.74549 time= 0.44069\n",
            "Epoch: 0087 train_loss= 0.61532 train_acc= 0.79592 train_auc= 0.86423 val_loss= 0.66640 val_acc= 0.70115 val_auc= 0.78830 time= 0.41452\n",
            "Epoch: 0087 train_loss= 0.61930 train_acc= 0.76884 train_auc= 0.85975 val_loss= 0.68089 val_acc= 0.69318 val_auc= 0.74105 time= 0.51348\n",
            "Epoch: 0086 train_loss= 0.61881 train_acc= 0.78699 train_auc= 0.86566 val_loss= 0.67412 val_acc= 0.67816 val_auc= 0.74681 time= 0.39534\n",
            "Epoch: 0086 train_loss= 0.61470 train_acc= 0.78316 train_auc= 0.87484 val_loss= 0.68713 val_acc= 0.65517 val_auc= 0.73989 time= 0.46005\n",
            "Epoch: 0086 train_loss= 0.61050 train_acc= 0.80612 train_auc= 0.89019 val_loss= 0.66899 val_acc= 0.74713 val_auc= 0.80106 time= 0.42390\n",
            "Epoch: 0087 train_loss= 0.61179 train_acc= 0.79082 train_auc= 0.87660 val_loss= 0.69292 val_acc= 0.67816 val_auc= 0.75053 time= 0.40587\n",
            "Epoch: 0088 train_loss= 0.60538 train_acc= 0.79209 train_auc= 0.88272 val_loss= 0.70811 val_acc= 0.65517 val_auc= 0.68081 time= 0.40438\n",
            "Epoch: 0088 train_loss= 0.61440 train_acc= 0.76913 train_auc= 0.86856 val_loss= 0.65691 val_acc= 0.68966 val_auc= 0.78830 time= 0.47199\n",
            "Epoch: 0088 train_loss= 0.61810 train_acc= 0.77296 train_auc= 0.86389 val_loss= 0.71128 val_acc= 0.63218 val_auc= 0.68617 time= 0.44228\n",
            "Epoch: 0088 train_loss= 0.61355 train_acc= 0.78189 train_auc= 0.87597 val_loss= 0.66629 val_acc= 0.68966 val_auc= 0.78936 time= 0.41654\n",
            "Epoch: 0087 train_loss= 0.61729 train_acc= 0.77806 train_auc= 0.86245 val_loss= 0.69465 val_acc= 0.66667 val_auc= 0.74231 time= 0.44134\n",
            "Epoch: 0088 train_loss= 0.60969 train_acc= 0.78033 train_auc= 0.88027 val_loss= 0.68083 val_acc= 0.67045 val_auc= 0.74105 time= 0.45422\n",
            "Epoch: 0087 train_loss= 0.62371 train_acc= 0.76403 train_auc= 0.85539 val_loss= 0.68687 val_acc= 0.67816 val_auc= 0.73883 time= 0.43094\n",
            "Epoch: 0087 train_loss= 0.61952 train_acc= 0.77296 train_auc= 0.85990 val_loss= 0.67347 val_acc= 0.70115 val_auc= 0.75000 time= 0.47508\n",
            "Epoch: 0087 train_loss= 0.61593 train_acc= 0.76913 train_auc= 0.87066 val_loss= 0.66850 val_acc= 0.75862 val_auc= 0.80053 time= 0.42519\n",
            "Epoch: 0088 train_loss= 0.61178 train_acc= 0.79974 train_auc= 0.88152 val_loss= 0.69254 val_acc= 0.68966 val_auc= 0.75213 time= 0.42571\n",
            "Epoch: 0089 train_loss= 0.61368 train_acc= 0.79337 train_auc= 0.87690 val_loss= 0.70852 val_acc= 0.65517 val_auc= 0.68028 time= 0.38819\n",
            "Epoch: 0089 train_loss= 0.62086 train_acc= 0.77041 train_auc= 0.86632 val_loss= 0.71143 val_acc= 0.64368 val_auc= 0.68723 time= 0.40425\n",
            "Epoch: 0089 train_loss= 0.62328 train_acc= 0.75893 train_auc= 0.84849 val_loss= 0.65648 val_acc= 0.68966 val_auc= 0.78883 time= 0.45637\n",
            "Epoch: 0089 train_loss= 0.61688 train_acc= 0.78827 train_auc= 0.86479 val_loss= 0.66627 val_acc= 0.70115 val_auc= 0.78777 time= 0.39538\n",
            "Epoch: 0088 train_loss= 0.61387 train_acc= 0.78571 train_auc= 0.87140 val_loss= 0.69518 val_acc= 0.65517 val_auc= 0.73754 time= 0.42987\n",
            "Epoch: 0088 train_loss= 0.61604 train_acc= 0.78061 train_auc= 0.86830 val_loss= 0.67287 val_acc= 0.71264 val_auc= 0.75160 time= 0.37608\n",
            "Epoch: 0089 train_loss= 0.61959 train_acc= 0.77906 train_auc= 0.86253 val_loss= 0.68100 val_acc= 0.67045 val_auc= 0.74105 time= 0.39340\n",
            "Epoch: 0088 train_loss= 0.61215 train_acc= 0.78444 train_auc= 0.87205 val_loss= 0.68683 val_acc= 0.67816 val_auc= 0.73936 time= 0.43201\n",
            "Epoch: 0088 train_loss= 0.61310 train_acc= 0.79337 train_auc= 0.87720 val_loss= 0.66778 val_acc= 0.74713 val_auc= 0.80106 time= 0.40615\n",
            "Epoch: 0089 train_loss= 0.61743 train_acc= 0.75383 train_auc= 0.86124 val_loss= 0.69240 val_acc= 0.67816 val_auc= 0.75106 time= 0.40458\n",
            "Epoch: 0090 train_loss= 0.61215 train_acc= 0.78061 train_auc= 0.87104 val_loss= 0.70879 val_acc= 0.65517 val_auc= 0.68081 time= 0.40731\n",
            "Epoch: 0090 train_loss= 0.61147 train_acc= 0.77551 train_auc= 0.87513 val_loss= 0.71167 val_acc= 0.65517 val_auc= 0.68883 time= 0.39259\n",
            "Epoch: 0090 train_loss= 0.61777 train_acc= 0.75255 train_auc= 0.85644 val_loss= 0.65636 val_acc= 0.68966 val_auc= 0.78830 time= 0.43524\n",
            "Epoch: 0090 train_loss= 0.61048 train_acc= 0.78444 train_auc= 0.87758 val_loss= 0.66709 val_acc= 0.70115 val_auc= 0.78936 time= 0.40696\n",
            "Epoch: 0089 train_loss= 0.61836 train_acc= 0.77168 train_auc= 0.85795 val_loss= 0.67264 val_acc= 0.71264 val_auc= 0.75426 time= 0.40556\n",
            "Epoch: 0089 train_loss= 0.60888 train_acc= 0.78061 train_auc= 0.88332 val_loss= 0.69501 val_acc= 0.65517 val_auc= 0.73913 time= 0.39164\n",
            "Epoch: 0090 train_loss= 0.60912 train_acc= 0.79694 train_auc= 0.87724 val_loss= 0.68141 val_acc= 0.67045 val_auc= 0.74157 time= 0.43433\n",
            "Epoch: 0089 train_loss= 0.61735 train_acc= 0.77679 train_auc= 0.86799 val_loss= 0.66715 val_acc= 0.73563 val_auc= 0.80426 time= 0.39155\n",
            "Epoch: 0089 train_loss= 0.61763 train_acc= 0.77296 train_auc= 0.86080 val_loss= 0.68701 val_acc= 0.67816 val_auc= 0.73989 time= 0.45199\n",
            "Epoch: 0090 train_loss= 0.60891 train_acc= 0.77423 train_auc= 0.87472 val_loss= 0.69235 val_acc= 0.68966 val_auc= 0.75053 time= 0.40298\n",
            "Epoch: 0091 train_loss= 0.60296 train_acc= 0.80357 train_auc= 0.88586 val_loss= 0.70908 val_acc= 0.63218 val_auc= 0.68187 time= 0.36552\n",
            "Epoch: 0091 train_loss= 0.61084 train_acc= 0.79082 train_auc= 0.87961 val_loss= 0.71160 val_acc= 0.65517 val_auc= 0.68883 time= 0.39664\n",
            "Epoch: 0091 train_loss= 0.61143 train_acc= 0.78316 train_auc= 0.87300 val_loss= 0.65713 val_acc= 0.68966 val_auc= 0.78723 time= 0.40066\n",
            "Epoch: 0091 train_loss= 0.61038 train_acc= 0.79974 train_auc= 0.88194 val_loss= 0.66613 val_acc= 0.71264 val_auc= 0.79096 time= 0.39803\n",
            "Epoch: 0090 train_loss= 0.61602 train_acc= 0.77934 train_auc= 0.86303 val_loss= 0.67264 val_acc= 0.71264 val_auc= 0.75426 time= 0.39903\n",
            "Epoch: 0090 train_loss= 0.61034 train_acc= 0.79464 train_auc= 0.87888 val_loss= 0.69456 val_acc= 0.65517 val_auc= 0.74178 time= 0.42491\n",
            "Epoch: 0091 train_loss= 0.61088 train_acc= 0.78161 train_auc= 0.87201 val_loss= 0.68160 val_acc= 0.67045 val_auc= 0.74001 time= 0.43128\n",
            "Epoch: 0090 train_loss= 0.61176 train_acc= 0.77423 train_auc= 0.86875 val_loss= 0.66668 val_acc= 0.74713 val_auc= 0.80691 time= 0.41131\n",
            "Epoch: 0090 train_loss= 0.61868 train_acc= 0.76913 train_auc= 0.85293 val_loss= 0.68739 val_acc= 0.66667 val_auc= 0.74043 time= 0.55873\n",
            "Epoch: 0091 train_loss= 0.60858 train_acc= 0.79719 train_auc= 0.87788 val_loss= 0.69237 val_acc= 0.68966 val_auc= 0.75160 time= 0.47947\n",
            "Epoch: 0092 train_loss= 0.59770 train_acc= 0.81760 train_auc= 0.90470 val_loss= 0.70970 val_acc= 0.64368 val_auc= 0.68187 time= 0.48020\n",
            "Epoch: 0092 train_loss= 0.60997 train_acc= 0.78061 train_auc= 0.87734 val_loss= 0.71156 val_acc= 0.65517 val_auc= 0.68883 time= 0.44821\n",
            "Epoch: 0092 train_loss= 0.61278 train_acc= 0.80867 train_auc= 0.87603 val_loss= 0.65687 val_acc= 0.68966 val_auc= 0.78617 time= 0.44677\n",
            "Epoch: 0092 train_loss= 0.60920 train_acc= 0.80230 train_auc= 0.88464 val_loss= 0.66503 val_acc= 0.71264 val_auc= 0.78989 time= 0.43059\n",
            "Epoch: 0091 train_loss= 0.60782 train_acc= 0.78699 train_auc= 0.88072 val_loss= 0.69425 val_acc= 0.65517 val_auc= 0.74337 time= 0.42166\n",
            "Epoch: 0091 train_loss= 0.61739 train_acc= 0.77168 train_auc= 0.85767 val_loss= 0.67222 val_acc= 0.71264 val_auc= 0.75319 time= 0.50307\n",
            "Epoch: 0092 train_loss= 0.60537 train_acc= 0.80077 train_auc= 0.89181 val_loss= 0.68180 val_acc= 0.67045 val_auc= 0.73897 time= 0.47404\n",
            "Epoch: 0091 train_loss= 0.60921 train_acc= 0.80230 train_auc= 0.87573 val_loss= 0.66657 val_acc= 0.74713 val_auc= 0.80638 time= 0.50083\n",
            "Epoch: 0092 train_loss= 0.60849 train_acc= 0.78316 train_auc= 0.88186 val_loss= 0.69253 val_acc= 0.67816 val_auc= 0.75106 time= 0.39430\n",
            "Epoch: 0091 train_loss= 0.61223 train_acc= 0.78571 train_auc= 0.87152 val_loss= 0.68709 val_acc= 0.66667 val_auc= 0.74202 time= 0.45400\n",
            "Epoch: 0093 train_loss= 0.61080 train_acc= 0.79337 train_auc= 0.88331 val_loss= 0.71016 val_acc= 0.64368 val_auc= 0.68293 time= 0.42342\n",
            "Epoch: 0093 train_loss= 0.61024 train_acc= 0.78699 train_auc= 0.88261 val_loss= 0.71165 val_acc= 0.65517 val_auc= 0.69096 time= 0.38215\n",
            "Epoch: 0093 train_loss= 0.60794 train_acc= 0.80102 train_auc= 0.88430 val_loss= 0.66418 val_acc= 0.72414 val_auc= 0.79149 time= 0.42247\n",
            "Epoch: 0093 train_loss= 0.61355 train_acc= 0.78827 train_auc= 0.87041 val_loss= 0.65635 val_acc= 0.70115 val_auc= 0.78511 time= 0.45934\n",
            "Epoch: 0092 train_loss= 0.59961 train_acc= 0.80230 train_auc= 0.89397 val_loss= 0.69380 val_acc= 0.66667 val_auc= 0.74602 time= 0.44557\n",
            "Epoch: 0093 train_loss= 0.60753 train_acc= 0.79566 train_auc= 0.88375 val_loss= 0.68219 val_acc= 0.69318 val_auc= 0.73690 time= 0.42790\n",
            "Epoch: 0092 train_loss= 0.60760 train_acc= 0.78189 train_auc= 0.87936 val_loss= 0.67171 val_acc= 0.71264 val_auc= 0.75426 time= 0.51696\n",
            "Epoch: 0093 train_loss= 0.61164 train_acc= 0.78954 train_auc= 0.87684 val_loss= 0.69337 val_acc= 0.67816 val_auc= 0.74787 time= 0.39526\n",
            "Epoch: 0092 train_loss= 0.60314 train_acc= 0.79974 train_auc= 0.89634 val_loss= 0.66612 val_acc= 0.74713 val_auc= 0.80532 time= 0.45101\n",
            "Epoch: 0092 train_loss= 0.60926 train_acc= 0.79209 train_auc= 0.88474 val_loss= 0.68631 val_acc= 0.67816 val_auc= 0.74096 time= 0.41922\n",
            "Epoch: 0094 train_loss= 0.60883 train_acc= 0.77041 train_auc= 0.87982 val_loss= 0.71199 val_acc= 0.66667 val_auc= 0.69202 time= 0.40481\n",
            "Epoch: 0094 train_loss= 0.60802 train_acc= 0.79719 train_auc= 0.88394 val_loss= 0.71036 val_acc= 0.65517 val_auc= 0.68134 time= 0.42955\n",
            "Epoch: 0094 train_loss= 0.61224 train_acc= 0.77679 train_auc= 0.87338 val_loss= 0.66403 val_acc= 0.73563 val_auc= 0.79309 time= 0.41230\n",
            "Epoch: 0093 train_loss= 0.60964 train_acc= 0.78444 train_auc= 0.87433 val_loss= 0.69325 val_acc= 0.66667 val_auc= 0.74655 time= 0.40060\n",
            "Epoch: 0094 train_loss= 0.60592 train_acc= 0.79974 train_auc= 0.88706 val_loss= 0.65560 val_acc= 0.68966 val_auc= 0.78670 time= 0.48156\n",
            "Epoch: 0093 train_loss= 0.61239 train_acc= 0.76913 train_auc= 0.86610 val_loss= 0.67196 val_acc= 0.71264 val_auc= 0.75372 time= 0.40085\n",
            "Epoch: 0094 train_loss= 0.60986 train_acc= 0.79183 train_auc= 0.87576 val_loss= 0.68293 val_acc= 0.68182 val_auc= 0.73326 time= 0.46043\n",
            "Epoch: 0094 train_loss= 0.61249 train_acc= 0.77423 train_auc= 0.86820 val_loss= 0.69414 val_acc= 0.68966 val_auc= 0.74681 time= 0.44838\n",
            "Epoch: 0093 train_loss= 0.61508 train_acc= 0.78571 train_auc= 0.86648 val_loss= 0.68568 val_acc= 0.67816 val_auc= 0.74309 time= 0.42988\n",
            "Epoch: 0093 train_loss= 0.61069 train_acc= 0.77934 train_auc= 0.87907 val_loss= 0.66626 val_acc= 0.74713 val_auc= 0.80319 time= 0.42168\n",
            "Epoch: 0095 train_loss= 0.59224 train_acc= 0.82270 train_auc= 0.91015 val_loss= 0.71092 val_acc= 0.65517 val_auc= 0.68134 time= 0.42045\n",
            "Epoch: 0095 train_loss= 0.60465 train_acc= 0.79974 train_auc= 0.89034 val_loss= 0.71220 val_acc= 0.67816 val_auc= 0.69415 time= 0.46254\n",
            "Epoch: 0095 train_loss= 0.60024 train_acc= 0.82015 train_auc= 0.89538 val_loss= 0.66342 val_acc= 0.73563 val_auc= 0.79255 time= 0.46014\n",
            "Epoch: 0094 train_loss= 0.60922 train_acc= 0.77806 train_auc= 0.87472 val_loss= 0.69209 val_acc= 0.66667 val_auc= 0.74814 time= 0.40867\n",
            "Epoch: 0095 train_loss= 0.59975 train_acc= 0.82781 train_auc= 0.89981 val_loss= 0.65526 val_acc= 0.67816 val_auc= 0.78617 time= 0.42636\n",
            "Epoch: 0094 train_loss= 0.61078 train_acc= 0.79209 train_auc= 0.87500 val_loss= 0.67304 val_acc= 0.70115 val_auc= 0.75000 time= 0.43581\n",
            "Epoch: 0095 train_loss= 0.59277 train_acc= 0.83163 train_auc= 0.91398 val_loss= 0.69470 val_acc= 0.67816 val_auc= 0.74521 time= 0.39447\n",
            "Epoch: 0094 train_loss= 0.60980 train_acc= 0.80230 train_auc= 0.88112 val_loss= 0.66590 val_acc= 0.74713 val_auc= 0.80106 time= 0.39358\n",
            "Epoch: 0094 train_loss= 0.60587 train_acc= 0.79337 train_auc= 0.89267 val_loss= 0.68520 val_acc= 0.66667 val_auc= 0.74681 time= 0.40057\n",
            "Epoch: 0095 train_loss= 0.59732 train_acc= 0.81992 train_auc= 0.90749 val_loss= 0.68315 val_acc= 0.68182 val_auc= 0.73171 time= 0.50183\n",
            "Epoch: 0096 train_loss= 0.60915 train_acc= 0.80230 train_auc= 0.87699 val_loss= 0.71145 val_acc= 0.65517 val_auc= 0.68187 time= 0.38715\n",
            "Epoch: 0096 train_loss= 0.62002 train_acc= 0.76786 train_auc= 0.85056 val_loss= 0.71252 val_acc= 0.66667 val_auc= 0.69362 time= 0.41638\n",
            "Epoch: 0096 train_loss= 0.61963 train_acc= 0.76531 train_auc= 0.84748 val_loss= 0.66351 val_acc= 0.73563 val_auc= 0.79255 time= 0.40886\n",
            "Epoch: 0095 train_loss= 0.59577 train_acc= 0.81122 train_auc= 0.89647 val_loss= 0.69184 val_acc= 0.66667 val_auc= 0.75027 time= 0.44371\n",
            "Epoch: 0096 train_loss= 0.61902 train_acc= 0.77551 train_auc= 0.86089 val_loss= 0.65496 val_acc= 0.68966 val_auc= 0.78617 time= 0.43174\n",
            "Epoch: 0095 train_loss= 0.60977 train_acc= 0.79974 train_auc= 0.88940 val_loss= 0.67244 val_acc= 0.72414 val_auc= 0.74894 time= 0.40518\n",
            "Epoch: 0096 train_loss= 0.61928 train_acc= 0.77168 train_auc= 0.85558 val_loss= 0.69507 val_acc= 0.67816 val_auc= 0.74734 time= 0.40726\n",
            "Epoch: 0095 train_loss= 0.59474 train_acc= 0.82781 train_auc= 0.91562 val_loss= 0.66623 val_acc= 0.74713 val_auc= 0.80000 time= 0.43688\n",
            "Epoch: 0095 train_loss= 0.60007 train_acc= 0.83036 train_auc= 0.89914 val_loss= 0.68512 val_acc= 0.67816 val_auc= 0.75106 time= 0.43671\n",
            "Epoch: 0096 train_loss= 0.61526 train_acc= 0.78161 train_auc= 0.86321 val_loss= 0.68319 val_acc= 0.67045 val_auc= 0.72963 time= 0.40033\n",
            "Epoch: 0097 train_loss= 0.59842 train_acc= 0.80485 train_auc= 0.89293 val_loss= 0.71204 val_acc= 0.65517 val_auc= 0.68240 time= 0.39886\n",
            "Epoch: 0097 train_loss= 0.60063 train_acc= 0.79082 train_auc= 0.88407 val_loss= 0.71294 val_acc= 0.66667 val_auc= 0.69468 time= 0.40364\n",
            "Epoch: 0096 train_loss= 0.61645 train_acc= 0.76658 train_auc= 0.85716 val_loss= 0.69161 val_acc= 0.66667 val_auc= 0.75133 time= 0.42446\n",
            "Epoch: 0097 train_loss= 0.59912 train_acc= 0.78699 train_auc= 0.88135 val_loss= 0.66436 val_acc= 0.72414 val_auc= 0.79202 time= 0.47024\n",
            "Epoch: 0097 train_loss= 0.59975 train_acc= 0.81122 train_auc= 0.88816 val_loss= 0.65461 val_acc= 0.68966 val_auc= 0.78564 time= 0.40260\n",
            "Epoch: 0096 train_loss= 0.62206 train_acc= 0.76276 train_auc= 0.85293 val_loss= 0.67198 val_acc= 0.68966 val_auc= 0.74787 time= 0.41450\n",
            "Epoch: 0096 train_loss= 0.61663 train_acc= 0.77423 train_auc= 0.86459 val_loss= 0.66619 val_acc= 0.74713 val_auc= 0.80213 time= 0.43895\n",
            "Epoch: 0097 train_loss= 0.59730 train_acc= 0.79949 train_auc= 0.88856 val_loss= 0.68317 val_acc= 0.68182 val_auc= 0.72859 time= 0.37849\n",
            "Epoch: 0097 train_loss= 0.59226 train_acc= 0.81378 train_auc= 0.89765 val_loss= 0.69517 val_acc= 0.67816 val_auc= 0.74628 time= 0.52426\n",
            "Epoch: 0096 train_loss= 0.61897 train_acc= 0.75638 train_auc= 0.85464 val_loss= 0.68474 val_acc= 0.68966 val_auc= 0.75160 time= 0.53751\n",
            "Epoch: 0098 train_loss= 0.58977 train_acc= 0.80740 train_auc= 0.89969 val_loss= 0.71251 val_acc= 0.64368 val_auc= 0.68081 time= 0.49973\n",
            "Epoch: 0098 train_loss= 0.59395 train_acc= 0.80485 train_auc= 0.89511 val_loss= 0.71320 val_acc= 0.65517 val_auc= 0.69628 time= 0.49114\n",
            "Epoch: 0098 train_loss= 0.60317 train_acc= 0.78827 train_auc= 0.87548 val_loss= 0.66312 val_acc= 0.72414 val_auc= 0.79202 time= 0.42270\n",
            "Epoch: 0097 train_loss= 0.59998 train_acc= 0.79847 train_auc= 0.88816 val_loss= 0.69143 val_acc= 0.66667 val_auc= 0.75239 time= 0.43903\n",
            "Epoch: 0098 train_loss= 0.60288 train_acc= 0.79082 train_auc= 0.87887 val_loss= 0.65412 val_acc= 0.68966 val_auc= 0.78670 time= 0.46453\n",
            "Epoch: 0097 train_loss= 0.59643 train_acc= 0.81760 train_auc= 0.90484 val_loss= 0.66644 val_acc= 0.74713 val_auc= 0.79947 time= 0.40868\n",
            "Epoch: 0097 train_loss= 0.59823 train_acc= 0.81250 train_auc= 0.89707 val_loss= 0.67208 val_acc= 0.67816 val_auc= 0.74628 time= 0.45800\n",
            "Epoch: 0098 train_loss= 0.59761 train_acc= 0.79566 train_auc= 0.89169 val_loss= 0.68307 val_acc= 0.68182 val_auc= 0.72756 time= 0.41074\n",
            "Epoch: 0098 train_loss= 0.60101 train_acc= 0.80357 train_auc= 0.88418 val_loss= 0.69508 val_acc= 0.67816 val_auc= 0.74415 time= 0.43138\n",
            "Epoch: 0097 train_loss= 0.60046 train_acc= 0.78827 train_auc= 0.88845 val_loss= 0.68477 val_acc= 0.68966 val_auc= 0.75266 time= 0.39227\n",
            "Epoch: 0099 train_loss= 0.60953 train_acc= 0.76531 train_auc= 0.85650 val_loss= 0.71294 val_acc= 0.62069 val_auc= 0.67975 time= 0.49878\n",
            "Epoch: 0099 train_loss= 0.60919 train_acc= 0.79082 train_auc= 0.87100 val_loss= 0.71331 val_acc= 0.65517 val_auc= 0.69787 time= 0.44928\n",
            "Epoch: 0099 train_loss= 0.61071 train_acc= 0.76786 train_auc= 0.86036 val_loss= 0.66263 val_acc= 0.72414 val_auc= 0.79202 time= 0.45424\n",
            "Epoch: 0098 train_loss= 0.59831 train_acc= 0.81122 train_auc= 0.89165 val_loss= 0.69175 val_acc= 0.66667 val_auc= 0.75186 time= 0.43494\n",
            "Epoch: 0099 train_loss= 0.61240 train_acc= 0.78954 train_auc= 0.86021 val_loss= 0.65424 val_acc= 0.70115 val_auc= 0.78511 time= 0.43538\n",
            "Epoch: 0098 train_loss= 0.59509 train_acc= 0.81122 train_auc= 0.89495 val_loss= 0.66634 val_acc= 0.74713 val_auc= 0.79894 time= 0.43103\n",
            "Epoch: 0099 train_loss= 0.60164 train_acc= 0.79183 train_auc= 0.87301 val_loss= 0.68322 val_acc= 0.69318 val_auc= 0.72600 time= 0.41396\n",
            "Epoch: 0098 train_loss= 0.59321 train_acc= 0.80230 train_auc= 0.89678 val_loss= 0.67209 val_acc= 0.68966 val_auc= 0.74628 time= 0.44227\n",
            "Epoch: 0099 train_loss= 0.61291 train_acc= 0.76913 train_auc= 0.85252 val_loss= 0.69446 val_acc= 0.68966 val_auc= 0.74787 time= 0.40242\n",
            "Epoch: 0098 train_loss= 0.60517 train_acc= 0.78571 train_auc= 0.87766 val_loss= 0.68370 val_acc= 0.66667 val_auc= 0.75266 time= 0.38182\n",
            "Epoch: 0100 train_loss= 0.59758 train_acc= 0.78954 train_auc= 0.88426 val_loss= 0.71327 val_acc= 0.64368 val_auc= 0.69840 time= 0.43301\n",
            "Epoch: 0100 train_loss= 0.59697 train_acc= 0.79847 train_auc= 0.87792 val_loss= 0.71320 val_acc= 0.62069 val_auc= 0.67869 time= 0.45984\n",
            "Epoch: 0099 train_loss= 0.60663 train_acc= 0.79592 train_auc= 0.87493 val_loss= 0.69256 val_acc= 0.66667 val_auc= 0.74920 time= 0.43923\n",
            "Epoch: 0100 train_loss= 0.60519 train_acc= 0.78189 train_auc= 0.87069 val_loss= 0.66361 val_acc= 0.72414 val_auc= 0.79255 time= 0.49277\n",
            "Epoch: 0099 train_loss= 0.60492 train_acc= 0.79337 train_auc= 0.87711 val_loss= 0.66666 val_acc= 0.74713 val_auc= 0.79787 time= 0.40536\n",
            "Epoch: 0100 train_loss= 0.60844 train_acc= 0.78189 train_auc= 0.86993 val_loss= 0.65397 val_acc= 0.71264 val_auc= 0.78564 time= 0.49156\n",
            "Epoch: 0099 train_loss= 0.60654 train_acc= 0.78444 train_auc= 0.86919 val_loss= 0.67227 val_acc= 0.71264 val_auc= 0.74468 time= 0.43426\n",
            "Epoch: 0100 train_loss= 0.60178 train_acc= 0.78800 train_auc= 0.87966 val_loss= 0.68319 val_acc= 0.68182 val_auc= 0.72548 time= 0.45361\n",
            "Epoch: 0100 train_loss= 0.60146 train_acc= 0.78827 train_auc= 0.87377 val_loss= 0.69407 val_acc= 0.67816 val_auc= 0.74787 time= 0.48144\n",
            "Epoch: 0099 train_loss= 0.61030 train_acc= 0.78316 train_auc= 0.86378 val_loss= 0.68335 val_acc= 0.67816 val_auc= 0.75160 time= 0.45569\n",
            "Epoch: 0101 train_loss= 0.59763 train_acc= 0.79719 train_auc= 0.87514 val_loss= 0.71380 val_acc= 0.60920 val_auc= 0.67922 time= 0.41963\n",
            "Epoch: 0101 train_loss= 0.59134 train_acc= 0.80102 train_auc= 0.89800 val_loss= 0.71298 val_acc= 0.64368 val_auc= 0.69787 time= 0.43311\n",
            "Epoch: 0100 train_loss= 0.60590 train_acc= 0.78061 train_auc= 0.86314 val_loss= 0.69341 val_acc= 0.68966 val_auc= 0.74655 time= 0.38967\n",
            "Epoch: 0101 train_loss= 0.60422 train_acc= 0.78061 train_auc= 0.86796 val_loss= 0.66457 val_acc= 0.72414 val_auc= 0.79149 time= 0.43872\n",
            "Epoch: 0100 train_loss= 0.60518 train_acc= 0.79209 train_auc= 0.87677 val_loss= 0.66701 val_acc= 0.74713 val_auc= 0.79787 time= 0.47007\n",
            "Epoch: 0101 train_loss= 0.60484 train_acc= 0.79719 train_auc= 0.87360 val_loss= 0.65409 val_acc= 0.67816 val_auc= 0.78777 time= 0.40047\n",
            "Epoch: 0100 train_loss= 0.61188 train_acc= 0.77041 train_auc= 0.85543 val_loss= 0.67242 val_acc= 0.72414 val_auc= 0.74468 time= 0.42690\n",
            "Epoch: 0101 train_loss= 0.59822 train_acc= 0.78672 train_auc= 0.87898 val_loss= 0.68352 val_acc= 0.69318 val_auc= 0.72392 time= 0.43282\n",
            "Epoch: 0101 train_loss= 0.59909 train_acc= 0.79719 train_auc= 0.88211 val_loss= 0.69342 val_acc= 0.66667 val_auc= 0.74787 time= 0.37178\n",
            "Epoch: 0100 train_loss= 0.61014 train_acc= 0.77296 train_auc= 0.85913 val_loss= 0.68327 val_acc= 0.68966 val_auc= 0.75000 time= 0.43768\n",
            "Epoch: 0102 train_loss= 0.60622 train_acc= 0.78954 train_auc= 0.87287 val_loss= 0.71324 val_acc= 0.64368 val_auc= 0.69734 time= 0.43272\n",
            "Epoch: 0101 train_loss= 0.60284 train_acc= 0.79464 train_auc= 0.87686 val_loss= 0.69415 val_acc= 0.67816 val_auc= 0.74178 time= 0.43433\n",
            "Epoch: 0102 train_loss= 0.60058 train_acc= 0.79592 train_auc= 0.88142 val_loss= 0.71404 val_acc= 0.62069 val_auc= 0.67762 time= 0.46558\n",
            "Epoch: 0101 train_loss= 0.60153 train_acc= 0.79592 train_auc= 0.87674 val_loss= 0.66744 val_acc= 0.74713 val_auc= 0.79734 time= 0.39095\n",
            "Epoch: 0102 train_loss= 0.60731 train_acc= 0.78954 train_auc= 0.87457 val_loss= 0.66349 val_acc= 0.72414 val_auc= 0.79202 time= 0.42244\n",
            "Epoch: 0102 train_loss= 0.60330 train_acc= 0.80332 train_auc= 0.88371 val_loss= 0.68433 val_acc= 0.69318 val_auc= 0.72237 time= 0.42408\n",
            "Epoch: 0102 train_loss= 0.59891 train_acc= 0.81122 train_auc= 0.89688 val_loss= 0.65364 val_acc= 0.67816 val_auc= 0.78936 time= 0.47073\n",
            "Epoch: 0101 train_loss= 0.60419 train_acc= 0.78571 train_auc= 0.86693 val_loss= 0.67263 val_acc= 0.72414 val_auc= 0.74521 time= 0.44543\n",
            "Epoch: 0102 train_loss= 0.59859 train_acc= 0.80102 train_auc= 0.88851 val_loss= 0.69330 val_acc= 0.66667 val_auc= 0.74787 time= 0.40023\n",
            "Epoch: 0101 train_loss= 0.60553 train_acc= 0.79974 train_auc= 0.87109 val_loss= 0.68395 val_acc= 0.70115 val_auc= 0.74840 time= 0.41433\n",
            "Epoch: 0103 train_loss= 0.59602 train_acc= 0.80995 train_auc= 0.90080 val_loss= 0.71390 val_acc= 0.64368 val_auc= 0.69681 time= 0.43488\n",
            "Epoch: 0102 train_loss= 0.60480 train_acc= 0.79464 train_auc= 0.88590 val_loss= 0.69428 val_acc= 0.67816 val_auc= 0.74072 time= 0.40725\n",
            "Epoch: 0103 train_loss= 0.59224 train_acc= 0.79974 train_auc= 0.89354 val_loss= 0.71438 val_acc= 0.63218 val_auc= 0.67391 time= 0.40586\n",
            "Epoch: 0102 train_loss= 0.60903 train_acc= 0.78699 train_auc= 0.87824 val_loss= 0.66790 val_acc= 0.74713 val_auc= 0.79681 time= 0.42877\n",
            "Epoch: 0103 train_loss= 0.59496 train_acc= 0.80740 train_auc= 0.89918 val_loss= 0.66190 val_acc= 0.72414 val_auc= 0.79149 time= 0.42429\n",
            "Epoch: 0103 train_loss= 0.59665 train_acc= 0.80332 train_auc= 0.89217 val_loss= 0.68462 val_acc= 0.69318 val_auc= 0.72081 time= 0.40249\n",
            "Epoch: 0103 train_loss= 0.59652 train_acc= 0.80230 train_auc= 0.90107 val_loss= 0.65281 val_acc= 0.67816 val_auc= 0.78883 time= 0.38801\n",
            "Epoch: 0102 train_loss= 0.60555 train_acc= 0.77806 train_auc= 0.87147 val_loss= 0.67334 val_acc= 0.71264 val_auc= 0.74309 time= 0.42437\n",
            "Epoch: 0103 train_loss= 0.59229 train_acc= 0.81633 train_auc= 0.90471 val_loss= 0.69279 val_acc= 0.66667 val_auc= 0.74787 time= 0.44184\n",
            "Epoch: 0102 train_loss= 0.60713 train_acc= 0.78571 train_auc= 0.87359 val_loss= 0.68382 val_acc= 0.68966 val_auc= 0.74734 time= 0.51439\n",
            "Epoch: 0104 train_loss= 0.59617 train_acc= 0.79464 train_auc= 0.89069 val_loss= 0.71466 val_acc= 0.65517 val_auc= 0.69681 time= 0.46993\n",
            "Epoch: 0103 train_loss= 0.59110 train_acc= 0.81122 train_auc= 0.90547 val_loss= 0.69462 val_acc= 0.67816 val_auc= 0.73701 time= 0.47074\n",
            "Epoch: 0104 train_loss= 0.59439 train_acc= 0.79464 train_auc= 0.88868 val_loss= 0.71489 val_acc= 0.64368 val_auc= 0.67232 time= 0.47223\n",
            "Epoch: 0103 train_loss= 0.59569 train_acc= 0.80485 train_auc= 0.89247 val_loss= 0.66788 val_acc= 0.75862 val_auc= 0.79734 time= 0.47204\n",
            "Epoch: 0104 train_loss= 0.60747 train_acc= 0.77934 train_auc= 0.87139 val_loss= 0.65201 val_acc= 0.71264 val_auc= 0.78883 time= 0.44998\n",
            "Epoch: 0104 train_loss= 0.60176 train_acc= 0.80740 train_auc= 0.88576 val_loss= 0.66122 val_acc= 0.72414 val_auc= 0.79096 time= 0.46700\n",
            "Epoch: 0103 train_loss= 0.60153 train_acc= 0.78061 train_auc= 0.87698 val_loss= 0.67277 val_acc= 0.70115 val_auc= 0.74628 time= 0.45095\n",
            "Epoch: 0104 train_loss= 0.59975 train_acc= 0.79055 train_auc= 0.88157 val_loss= 0.68403 val_acc= 0.69318 val_auc= 0.72029 time= 0.50485\n",
            "Epoch: 0104 train_loss= 0.59566 train_acc= 0.79337 train_auc= 0.88866 val_loss= 0.69316 val_acc= 0.65517 val_auc= 0.74854 time= 0.44774\n",
            "Epoch: 0103 train_loss= 0.60084 train_acc= 0.78954 train_auc= 0.89375 val_loss= 0.68270 val_acc= 0.68966 val_auc= 0.74894 time= 0.41572\n",
            "Epoch: 0105 train_loss= 0.59327 train_acc= 0.81505 train_auc= 0.90097 val_loss= 0.71541 val_acc= 0.66667 val_auc= 0.69415 time= 0.39716\n",
            "Epoch: 0105 train_loss= 0.58800 train_acc= 0.81633 train_auc= 0.90243 val_loss= 0.71538 val_acc= 0.66667 val_auc= 0.67232 time= 0.37956\n",
            "Epoch: 0104 train_loss= 0.59339 train_acc= 0.79464 train_auc= 0.89812 val_loss= 0.69368 val_acc= 0.68966 val_auc= 0.73966 time= 0.41814\n",
            "Epoch:Epoch:  01040105  train_loss=train_loss=  0.600720.60107  train_acc=train_acc=  0.811220.79592 train_auc= 0.87520 val_loss= 0.65168 val_acc= 0.70115 val_auc=  train_auc= 0.89104 val_loss= 0.66719 val_acc= 0.75862 val_auc= 0.79734 time= 0.41181\n",
            "0.78883 time= 0.42055\n",
            "Epoch: 0105 train_loss= 0.59433 train_acc= 0.79974 train_auc= 0.89032 val_loss= 0.66164 val_acc= 0.73563 val_auc= 0.79255 time= 0.43881\n",
            "Epoch: 0105 train_loss= 0.58912 train_acc= 0.80843 train_auc= 0.89404 val_loss= 0.68331 val_acc= 0.70455 val_auc= 0.72237 time= 0.43089\n",
            "Epoch: 0104 train_loss= 0.60082 train_acc= 0.78827 train_auc= 0.87498 val_loss= 0.67308 val_acc= 0.70115 val_auc= 0.74468 time= 0.47176\n",
            "Epoch: 0105 train_loss= 0.59055 train_acc= 0.81505 train_auc= 0.89633 val_loss= 0.69380 val_acc= 0.68966 val_auc= 0.74734 time= 0.44800\n",
            "Epoch: 0104 train_loss= 0.60181 train_acc= 0.78189 train_auc= 0.87895 val_loss= 0.68123 val_acc= 0.67816 val_auc= 0.75372 time= 0.42514\n",
            "Epoch: 0106 train_loss= 0.60216 train_acc= 0.79464 train_auc= 0.87860 val_loss= 0.71628 val_acc= 0.66667 val_auc= 0.69202 time= 0.41393\n",
            "Epoch: 0106 train_loss= 0.59811 train_acc= 0.79847 train_auc= 0.88108 val_loss= 0.71579 val_acc= 0.66667 val_auc= 0.67232 time= 0.39489\n",
            "Epoch: 0106 train_loss= 0.60232 train_acc= 0.78189 train_auc= 0.87961 val_loss= 0.65110 val_acc= 0.70115 val_auc= 0.78777 time= 0.41173\n",
            "Epoch: 0105 train_loss= 0.58569 train_acc= 0.79847 train_auc= 0.90145 val_loss= 0.69275 val_acc= 0.68966 val_auc= 0.74337 time= 0.48061\n",
            "Epoch: 0105 train_loss= 0.58952 train_acc= 0.80357 train_auc= 0.90338 val_loss= 0.66645 val_acc= 0.75862 val_auc= 0.79840 time= 0.46735\n",
            "Epoch: 0106 train_loss= 0.60407 train_acc= 0.78061 train_auc= 0.87306 val_loss= 0.66097 val_acc= 0.73563 val_auc= 0.79149 time= 0.42847\n",
            "Epoch: 0106 train_loss= 0.60035 train_acc= 0.78800 train_auc= 0.87797 val_loss= 0.68273 val_acc= 0.70455 val_auc= 0.72444 time= 0.44758\n",
            "Epoch: 0105 train_loss= 0.59237 train_acc= 0.79082 train_auc= 0.88715 val_loss= 0.67336 val_acc= 0.70115 val_auc= 0.74468 time= 0.46181\n",
            "Epoch: 0106 train_loss= 0.59685 train_acc= 0.78827 train_auc= 0.88465 val_loss= 0.69417 val_acc= 0.67816 val_auc= 0.74787 time= 0.44984\n",
            "Epoch: 0105 train_loss= 0.59946 train_acc= 0.79082 train_auc= 0.88345 val_loss= 0.68078 val_acc= 0.67816 val_auc= 0.75745 time= 0.38505\n",
            "Epoch: 0107 train_loss= 0.59179 train_acc= 0.79592 train_auc= 0.89585 val_loss= 0.71732 val_acc= 0.67816 val_auc= 0.69043 time= 0.39472\n",
            "Epoch: 0107 train_loss= 0.58491 train_acc= 0.82526 train_auc= 0.90415 val_loss= 0.71604 val_acc= 0.65517 val_auc= 0.67391 time= 0.47048\n",
            "Epoch: 0107 train_loss= 0.59114 train_acc= 0.81122 train_auc= 0.89193 val_loss= 0.65097 val_acc= 0.70115 val_auc= 0.78777 time= 0.42014\n",
            "Epoch: 0106 train_loss= 0.60203 train_acc= 0.77551 train_auc= 0.87129 val_loss= 0.69251 val_acc= 0.68966 val_auc= 0.74443 time= 0.49026\n",
            "Epoch: 0107 train_loss= 0.58803 train_acc= 0.80230 train_auc= 0.89533 val_loss= 0.66048 val_acc= 0.72414 val_auc= 0.79096 time= 0.43336\n",
            "Epoch: 0106 train_loss= 0.60607 train_acc= 0.77423 train_auc= 0.86882 val_loss= 0.66625 val_acc= 0.75862 val_auc= 0.79734 time= 0.50990\n",
            "Epoch: 0107 train_loss= 0.59181 train_acc= 0.79438 train_auc= 0.88900 val_loss= 0.68271 val_acc= 0.69318 val_auc= 0.72133 time= 0.42816\n",
            "Epoch: 0106 train_loss= 0.60395 train_acc= 0.76020 train_auc= 0.86631 val_loss= 0.67332 val_acc= 0.70115 val_auc= 0.74309 time= 0.40153\n",
            "Epoch: 0107 train_loss= 0.59750 train_acc= 0.80230 train_auc= 0.87946 val_loss= 0.69464 val_acc= 0.68966 val_auc= 0.75053 time= 0.42863\n",
            "Epoch: 0106 train_loss= 0.60666 train_acc= 0.76531 train_auc= 0.86132 val_loss= 0.68068 val_acc= 0.68966 val_auc= 0.76170 time= 0.40608\n",
            "Epoch: 0108 train_loss= 0.59778 train_acc= 0.79974 train_auc= 0.87957 val_loss= 0.71774 val_acc= 0.66667 val_auc= 0.68936 time= 0.42410\n",
            "Epoch: 0108 train_loss= 0.59513 train_acc= 0.80995 train_auc= 0.88703 val_loss= 0.65140 val_acc= 0.71264 val_auc= 0.78777 time= 0.42900\n",
            "Epoch: 0108 train_loss= 0.59830 train_acc= 0.79974 train_auc= 0.88699 val_loss= 0.71601 val_acc= 0.62069 val_auc= 0.67656 time= 0.57367\n",
            "Epoch: 0107 train_loss= 0.58425 train_acc= 0.82781 train_auc= 0.89922 val_loss= 0.66592 val_acc= 0.75862 val_auc= 0.79840 time= 0.42696\n",
            "Epoch: 0108 train_loss= 0.60212 train_acc= 0.78444 train_auc= 0.86940 val_loss= 0.66016 val_acc= 0.72414 val_auc= 0.79043 time= 0.44854\n",
            "Epoch: 0107 train_loss= 0.58817 train_acc= 0.81250 train_auc= 0.88777 val_loss= 0.69217 val_acc= 0.66667 val_auc= 0.74655 time= 0.47116\n",
            "Epoch: 0107 train_loss= 0.59141 train_acc= 0.80485 train_auc= 0.88608 val_loss= 0.67329 val_acc= 0.70115 val_auc= 0.74309 time= 0.39535\n",
            "Epoch: 0108 train_loss= 0.59615 train_acc= 0.80971 train_auc= 0.88574 val_loss= 0.68277 val_acc= 0.69318 val_auc= 0.72133 time= 0.48562\n",
            "Epoch: 0108 train_loss= 0.59199 train_acc= 0.80867 train_auc= 0.89399 val_loss= 0.69418 val_acc= 0.68966 val_auc= 0.75053 time= 0.45080\n",
            "Epoch: 0107 train_loss= 0.59853 train_acc= 0.79337 train_auc= 0.88188 val_loss= 0.68045 val_acc= 0.68966 val_auc= 0.76383 time= 0.45228\n",
            "Epoch: 0109 train_loss= 0.59260 train_acc= 0.80740 train_auc= 0.88851 val_loss= 0.71820 val_acc= 0.64368 val_auc= 0.68670 time= 0.42579\n",
            "Epoch: 0109 train_loss= 0.59475 train_acc= 0.79719 train_auc= 0.88171 val_loss= 0.65194 val_acc= 0.72414 val_auc= 0.78670 time= 0.47638\n",
            "Epoch: 0109 train_loss= 0.59258 train_acc= 0.79719 train_auc= 0.88709 val_loss= 0.71576 val_acc= 0.62069 val_auc= 0.67762 time= 0.41447\n",
            "Epoch: 0108 train_loss= 0.60167 train_acc= 0.78444 train_auc= 0.87513 val_loss= 0.67331 val_acc= 0.70115 val_auc= 0.74309 time= 0.45700\n",
            "Epoch: 0109 train_loss= 0.59928 train_acc= 0.77423 train_auc= 0.86804 val_loss= 0.66061 val_acc= 0.72414 val_auc= 0.79096 time= 0.50031\n",
            "Epoch: 0108 train_loss= 0.60283 train_acc= 0.78827 train_auc= 0.87704 val_loss= 0.66574 val_acc= 0.75862 val_auc= 0.79681 time= 0.49411\n",
            "Epoch: 0108 train_loss= 0.59858 train_acc= 0.79719 train_auc= 0.87303 val_loss= 0.69229 val_acc= 0.67816 val_auc= 0.74443 time= 0.49842\n",
            "Epoch: 0109 train_loss= 0.59271 train_acc= 0.80867 train_auc= 0.89176 val_loss= 0.69373 val_acc= 0.67816 val_auc= 0.74840 time= 0.46611\n",
            "Epoch: 0109 train_loss= 0.60226 train_acc= 0.78416 train_auc= 0.86312 val_loss= 0.68301 val_acc= 0.69318 val_auc= 0.72081 time= 0.49312\n",
            "Epoch: 0108 train_loss= 0.59723 train_acc= 0.79974 train_auc= 0.88134 val_loss= 0.68020 val_acc= 0.66667 val_auc= 0.76223 time= 0.46693\n",
            "Epoch: 0110 train_loss= 0.58948 train_acc= 0.80485 train_auc= 0.89453 val_loss= 0.71852 val_acc= 0.64368 val_auc= 0.68564 time= 0.41934\n",
            "Epoch: 0110 train_loss= 0.59703 train_acc= 0.79592 train_auc= 0.88811 val_loss= 0.65200 val_acc= 0.70115 val_auc= 0.78511 time= 0.43672\n",
            "Epoch: 0109 train_loss= 0.60027 train_acc= 0.78316 train_auc= 0.87614 val_loss= 0.67343 val_acc= 0.70115 val_auc= 0.74521 time= 0.40654\n",
            "Epoch: 0110 train_loss= 0.59468 train_acc= 0.79082 train_auc= 0.88155 val_loss= 0.71555 val_acc= 0.62069 val_auc= 0.67975 time= 0.49079\n",
            "Epoch: 0109 train_loss= 0.59601 train_acc= 0.79337 train_auc= 0.88194 val_loss= 0.66556 val_acc= 0.75862 val_auc= 0.79628 time= 0.44034\n",
            "Epoch: 0109 train_loss= 0.59550 train_acc= 0.78316 train_auc= 0.86888 val_loss= 0.69222 val_acc= 0.68966 val_auc= 0.74496 time= 0.42324\n",
            "Epoch: 0110 train_loss= 0.59342 train_acc= 0.77934 train_auc= 0.87897 val_loss= 0.66063 val_acc= 0.71264 val_auc= 0.78936 time= 0.49914\n",
            "Epoch: 0110 train_loss= 0.59332 train_acc= 0.80612 train_auc= 0.88966 val_loss= 0.69363 val_acc= 0.67816 val_auc= 0.74840 time= 0.43089\n",
            "Epoch: 0110 train_loss= 0.59038 train_acc= 0.79949 train_auc= 0.89625 val_loss= 0.68285 val_acc= 0.69318 val_auc= 0.71821 time= 0.42016\n",
            "Epoch: 0109 train_loss= 0.60427 train_acc= 0.77041 train_auc= 0.86354 val_loss= 0.68022 val_acc= 0.65517 val_auc= 0.75798 time= 0.41257\n",
            "Epoch: 0111 train_loss= 0.58772 train_acc= 0.81122 train_auc= 0.89846 val_loss= 0.71982 val_acc= 0.63218 val_auc= 0.68564 time= 0.43042\n",
            "Epoch: 0111 train_loss= 0.59514 train_acc= 0.80485 train_auc= 0.88554 val_loss= 0.65166 val_acc= 0.67816 val_auc= 0.78564 time= 0.47115\n",
            "Epoch: 0110 train_loss= 0.60050 train_acc= 0.78954 train_auc= 0.88285 val_loss= 0.67435 val_acc= 0.70115 val_auc= 0.74574 time= 0.42013\n",
            "Epoch: 0110 train_loss= 0.59361 train_acc= 0.79464 train_auc= 0.89630 val_loss= 0.66595 val_acc= 0.73563 val_auc= 0.79628 time= 0.42969\n",
            "Epoch: 0111 train_loss= 0.58595 train_acc= 0.80867 train_auc= 0.89805 val_loss= 0.71556 val_acc= 0.62069 val_auc= 0.68187 time= 0.48205\n",
            "Epoch: 0110 train_loss= 0.59057 train_acc= 0.80995 train_auc= 0.89561 val_loss= 0.69206 val_acc= 0.68966 val_auc= 0.74655 time= 0.43099\n",
            "Epoch: 0111 train_loss= 0.58890 train_acc= 0.82398 train_auc= 0.89874 val_loss= 0.66159 val_acc= 0.71264 val_auc= 0.78936 time= 0.44610\n",
            "Epoch: 0110 train_loss= 0.59464 train_acc= 0.78699 train_auc= 0.88618 val_loss= 0.68028 val_acc= 0.66667 val_auc= 0.75638 time= 0.38686\n",
            "Epoch: 0111 train_loss= 0.58863 train_acc= 0.81760 train_auc= 0.90037 val_loss= 0.69303 val_acc= 0.67816 val_auc= 0.74734 time= 0.43886\n",
            "Epoch: 0111 train_loss= 0.59059 train_acc= 0.81098 train_auc= 0.89977 val_loss= 0.68266 val_acc= 0.69318 val_auc= 0.71925 time= 0.48218\n",
            "Epoch: 0112 train_loss= 0.59207 train_acc= 0.79209 train_auc= 0.89011 val_loss= 0.71998 val_acc= 0.63218 val_auc= 0.68457 time= 0.42296\n",
            "Epoch: 0112 train_loss= 0.59840 train_acc= 0.79592 train_auc= 0.88283 val_loss= 0.65044 val_acc= 0.67816 val_auc= 0.78351 time= 0.37422\n",
            "Epoch: 0111 train_loss= 0.59350 train_acc= 0.80995 train_auc= 0.90359 val_loss= 0.67379 val_acc= 0.70115 val_auc= 0.74628 time= 0.40057\n",
            "Epoch: 0111 train_loss= 0.59282 train_acc= 0.80357 train_auc= 0.89064 val_loss= 0.69196 val_acc= 0.70115 val_auc= 0.74655 time= 0.40348\n",
            "Epoch: 0111 train_loss= 0.59327 train_acc= 0.79464 train_auc= 0.89487 val_loss= 0.66614 val_acc= 0.73563 val_auc= 0.79787 time= 0.41843\n",
            "Epoch: 0111 train_loss= 0.60009 train_acc= 0.80230 train_auc= 0.88554 val_loss= 0.68024 val_acc= 0.66667 val_auc= 0.75638 time= 0.38145\n",
            "Epoch: 0112 train_loss= 0.59454 train_acc= 0.80357 train_auc= 0.88883 val_loss= 0.71525 val_acc= 0.63218 val_auc= 0.68293 time= 0.48308\n",
            "Epoch: 0112 train_loss= 0.59375 train_acc= 0.80485 train_auc= 0.89116 val_loss= 0.66122 val_acc= 0.72414 val_auc= 0.78830 time= 0.49903\n",
            "Epoch: 0112 train_loss= 0.58731 train_acc= 0.81250 train_auc= 0.90473 val_loss= 0.69267 val_acc= 0.66667 val_auc= 0.74681 time= 0.45326\n",
            "Epoch: 0112 train_loss= 0.59226 train_acc= 0.81737 train_auc= 0.89902 val_loss= 0.68213 val_acc= 0.69318 val_auc= 0.71977 time= 0.47766\n",
            "Epoch: 0113 train_loss= 0.59919 train_acc= 0.78954 train_auc= 0.87300 val_loss= 0.72037 val_acc= 0.63218 val_auc= 0.68351 time= 0.46583\n",
            "Epoch: 0112 train_loss= 0.59885 train_acc= 0.78571 train_auc= 0.88542 val_loss= 0.67289 val_acc= 0.70115 val_auc= 0.75000 time= 0.42863\n",
            "Epoch: 0113 train_loss= 0.59760 train_acc= 0.78061 train_auc= 0.87321 val_loss= 0.64974 val_acc= 0.68966 val_auc= 0.78351 time= 0.57784\n",
            "Epoch: 0112 train_loss= 0.59165 train_acc= 0.80612 train_auc= 0.89776 val_loss= 0.69148 val_acc= 0.70115 val_auc= 0.74761 time= 0.53090\n",
            "Epoch: 0112 train_loss= 0.59215 train_acc= 0.79847 train_auc= 0.88685 val_loss= 0.66536 val_acc= 0.73563 val_auc= 0.79628 time= 0.50910\n",
            "Epoch: 0112 train_loss= 0.60338 train_acc= 0.79847 train_auc= 0.87681 val_loss= 0.67986 val_acc= 0.66667 val_auc= 0.75691 time= 0.48119\n",
            "Epoch: 0113 train_loss= 0.58673 train_acc= 0.79337 train_auc= 0.88882 val_loss= 0.71541 val_acc= 0.63218 val_auc= 0.68187 time= 0.45187\n",
            "Epoch: 0113 train_loss= 0.59698 train_acc= 0.79209 train_auc= 0.88339 val_loss= 0.65908 val_acc= 0.71264 val_auc= 0.78989 time= 0.49769\n",
            "Epoch: 0113 train_loss= 0.58792 train_acc= 0.80740 train_auc= 0.89203 val_loss= 0.69338 val_acc= 0.66667 val_auc= 0.74734 time= 0.47788\n",
            "Epoch: 0113 train_loss= 0.58775 train_acc= 0.81609 train_auc= 0.89160 val_loss= 0.68270 val_acc= 0.69318 val_auc= 0.72081 time= 0.47974\n",
            "Epoch: 0114 train_loss= 0.58482 train_acc= 0.81888 train_auc= 0.89759 val_loss= 0.72096 val_acc= 0.62069 val_auc= 0.68085 time= 0.51232\n",
            "Epoch: 0113 train_loss= 0.59272 train_acc= 0.81122 train_auc= 0.88435 val_loss= 0.67289 val_acc= 0.70115 val_auc= 0.75213 time= 0.43930\n",
            "Epoch: 0114 train_loss= 0.59305 train_acc= 0.78061 train_auc= 0.88418 val_loss= 0.64999 val_acc= 0.70115 val_auc= 0.78457 time= 0.43450\n",
            "Epoch: 0113 train_loss= 0.59119 train_acc= 0.81378 train_auc= 0.89404 val_loss= 0.69092 val_acc= 0.68966 val_auc= 0.74814 time= 0.47264\n",
            "Epoch: 0113 train_loss= 0.58658 train_acc= 0.80867 train_auc= 0.89527 val_loss= 0.66524 val_acc= 0.75862 val_auc= 0.79574 time= 0.45206\n",
            "Epoch: 0113 train_loss= 0.60169 train_acc= 0.77934 train_auc= 0.87859 val_loss= 0.67956 val_acc= 0.66667 val_auc= 0.76117 time= 0.48520\n",
            "Epoch: 0114 train_loss= 0.58100 train_acc= 0.81250 train_auc= 0.89816 val_loss= 0.71577 val_acc= 0.64368 val_auc= 0.68081 time= 0.48758\n",
            "Epoch: 0114 train_loss= 0.58123 train_acc= 0.81505 train_auc= 0.90033 val_loss= 0.69378 val_acc= 0.67816 val_auc= 0.74521 time= 0.48888\n",
            "Epoch: 0114 train_loss= 0.58360 train_acc= 0.82270 train_auc= 0.89382 val_loss= 0.65898 val_acc= 0.71264 val_auc= 0.78936 time= 0.52726\n",
            "Epoch: 0114 train_loss= 0.57917 train_acc= 0.81737 train_auc= 0.91081 val_loss= 0.68239 val_acc= 0.69318 val_auc= 0.72237 time= 0.62625\n",
            "Epoch: 0115 train_loss= 0.58598 train_acc= 0.81122 train_auc= 0.89225 val_loss= 0.72183 val_acc= 0.63218 val_auc= 0.68085 time= 0.59528\n",
            "Epoch: 0114 train_loss= 0.57977 train_acc= 0.82781 train_auc= 0.90515 val_loss= 0.67334 val_acc= 0.70115 val_auc= 0.75106 time= 0.59318\n",
            "Epoch: 0115 train_loss= 0.58976 train_acc= 0.80867 train_auc= 0.89531 val_loss= 0.65003 val_acc= 0.70115 val_auc= 0.78723 time= 0.54947\n",
            "Epoch: 0114 train_loss= 0.58830 train_acc= 0.79974 train_auc= 0.89492 val_loss= 0.66516 val_acc= 0.75862 val_auc= 0.79468 time= 0.52869\n",
            "Epoch: 0114 train_loss= 0.58838 train_acc= 0.79974 train_auc= 0.88771 val_loss= 0.69032 val_acc= 0.68966 val_auc= 0.74814 time= 0.53161\n",
            "Epoch: 0114 train_loss= 0.59058 train_acc= 0.79974 train_auc= 0.89407 val_loss= 0.68009 val_acc= 0.70115 val_auc= 0.76383 time= 0.52303\n",
            "Epoch: 0115 train_loss= 0.58708 train_acc= 0.80102 train_auc= 0.89208 val_loss= 0.71641 val_acc= 0.65517 val_auc= 0.67975 time= 0.55993\n",
            "Epoch: 0115 train_loss= 0.58177 train_acc= 0.82526 train_auc= 0.90523 val_loss= 0.69390 val_acc= 0.68966 val_auc= 0.74415 time= 0.52518\n",
            "Epoch: 0115 train_loss= 0.58787 train_acc= 0.80230 train_auc= 0.89072 val_loss= 0.65895 val_acc= 0.73563 val_auc= 0.79096 time= 0.57435\n",
            "Epoch: 0115 train_loss= 0.58653 train_acc= 0.80077 train_auc= 0.89664 val_loss= 0.68228 val_acc= 0.67045 val_auc= 0.72392 time= 0.52284\n",
            "Epoch: 0116 train_loss= 0.59624 train_acc= 0.80357 train_auc= 0.88012 val_loss= 0.64959 val_acc= 0.71264 val_auc= 0.78723 time= 0.43667\n",
            "Epoch: 0116 train_loss= 0.58804 train_acc= 0.80740 train_auc= 0.89200 val_loss= 0.72193 val_acc= 0.63218 val_auc= 0.68032 time= 0.51840\n",
            "Epoch: 0115 train_loss= 0.59292 train_acc= 0.77806 train_auc= 0.88541 val_loss= 0.67361 val_acc= 0.70115 val_auc= 0.74894 time= 0.50689\n",
            "Epoch: 0115 train_loss= 0.58710 train_acc= 0.80995 train_auc= 0.90030 val_loss= 0.66481 val_acc= 0.75862 val_auc= 0.79521 time= 0.44987\n",
            "Epoch: 0115 train_loss= 0.58444 train_acc= 0.82270 train_auc= 0.90426 val_loss= 0.69023 val_acc= 0.68966 val_auc= 0.74920 time= 0.48173\n",
            "Epoch: 0115 train_loss= 0.59958 train_acc= 0.79719 train_auc= 0.87787 val_loss= 0.68019 val_acc= 0.71264 val_auc= 0.76436 time= 0.49694\n",
            "Epoch: 0116 train_loss= 0.59301 train_acc= 0.78571 train_auc= 0.87600 val_loss= 0.71708 val_acc= 0.64368 val_auc= 0.68028 time= 0.44707\n",
            "Epoch: 0116 train_loss= 0.59142 train_acc= 0.80102 train_auc= 0.88362 val_loss= 0.69421 val_acc= 0.66667 val_auc= 0.74521 time= 0.46058\n",
            "Epoch: 0116 train_loss= 0.59956 train_acc= 0.77551 train_auc= 0.87154 val_loss= 0.65799 val_acc= 0.71264 val_auc= 0.79096 time= 0.49615\n",
            "Epoch: 0117 train_loss= 0.59563 train_acc= 0.79847 train_auc= 0.87588 val_loss= 0.64919 val_acc= 0.70115 val_auc= 0.78936 time= 0.40292\n",
            "Epoch: 0116 train_loss= 0.59420 train_acc= 0.79592 train_auc= 0.87198 val_loss= 0.67386 val_acc= 0.68966 val_auc= 0.74840 time= 0.44093\n",
            "Epoch: 0116 train_loss= 0.58668 train_acc= 0.80460 train_auc= 0.88878 val_loss= 0.68239 val_acc= 0.63636 val_auc= 0.72652 time= 0.52024\n",
            "Epoch: 0117 train_loss= 0.59209 train_acc= 0.79847 train_auc= 0.88457 val_loss= 0.72173 val_acc= 0.62069 val_auc= 0.68032 time= 0.48818\n",
            "Epoch: 0116 train_loss= 0.58874 train_acc= 0.79209 train_auc= 0.88864 val_loss= 0.66456 val_acc= 0.75862 val_auc= 0.79415 time= 0.45269\n",
            "Epoch: 0116 train_loss= 0.59125 train_acc= 0.80357 train_auc= 0.88446 val_loss= 0.69042 val_acc= 0.68966 val_auc= 0.74761 time= 0.46970\n",
            "Epoch: 0116 train_loss= 0.58877 train_acc= 0.81633 train_auc= 0.89420 val_loss= 0.68017 val_acc= 0.70115 val_auc= 0.76330 time= 0.43993\n",
            "Epoch: 0117 train_loss= 0.58861 train_acc= 0.79209 train_auc= 0.88277 val_loss= 0.71737 val_acc= 0.64368 val_auc= 0.67922 time= 0.44433\n",
            "Epoch: 0117 train_loss= 0.58694 train_acc= 0.80740 train_auc= 0.88697 val_loss= 0.69391 val_acc= 0.67816 val_auc= 0.74840 time= 0.43494\n",
            "Epoch: 0117 train_loss= 0.58967 train_acc= 0.80102 train_auc= 0.88688 val_loss= 0.65651 val_acc= 0.72414 val_auc= 0.79043 time= 0.41821\n",
            "Epoch: 0118 train_loss= 0.58402 train_acc= 0.80995 train_auc= 0.88773 val_loss= 0.64921 val_acc= 0.71264 val_auc= 0.78723 time= 0.45089\n",
            "Epoch: 0117 train_loss= 0.59566 train_acc= 0.78827 train_auc= 0.87338 val_loss= 0.67429 val_acc= 0.70115 val_auc= 0.74894 time= 0.43150\n",
            "Epoch: 0118 train_loss= 0.58894 train_acc= 0.79847 train_auc= 0.88249 val_loss= 0.72138 val_acc= 0.63218 val_auc= 0.68191 time= 0.39592\n",
            "Epoch: 0117 train_loss= 0.59236 train_acc= 0.79438 train_auc= 0.87786 val_loss= 0.68235 val_acc= 0.63636 val_auc= 0.72600 time= 0.42596\n",
            "Epoch: 0117 train_loss= 0.58620 train_acc= 0.79464 train_auc= 0.88939 val_loss= 0.66451 val_acc= 0.75862 val_auc= 0.79415 time= 0.46109\n",
            "Epoch: 0117 train_loss= 0.59118 train_acc= 0.79592 train_auc= 0.88075 val_loss= 0.69112 val_acc= 0.68966 val_auc= 0.74655 time= 0.49096\n",
            "Epoch: 0117 train_loss= 0.59048 train_acc= 0.81760 train_auc= 0.88252 val_loss= 0.68033 val_acc= 0.67816 val_auc= 0.76117 time= 0.48518\n",
            "Epoch: 0118 train_loss= 0.58008 train_acc= 0.81505 train_auc= 0.89303 val_loss= 0.69391 val_acc= 0.70115 val_auc= 0.74840 time= 0.42029\n",
            "Epoch: 0118 train_loss= 0.57202 train_acc= 0.81633 train_auc= 0.90726 val_loss= 0.71761 val_acc= 0.65517 val_auc= 0.67603 time= 0.44607\n",
            "Epoch: 0119 train_loss= 0.58463 train_acc= 0.80485 train_auc= 0.89222 val_loss= 0.64834 val_acc= 0.70115 val_auc= 0.78617 time= 0.40917\n",
            "Epoch: 0118 train_loss= 0.58169 train_acc= 0.80612 train_auc= 0.88715 val_loss= 0.65646 val_acc= 0.71264 val_auc= 0.78989 time= 0.42990\n",
            "Epoch: 0118 train_loss= 0.58423 train_acc= 0.80867 train_auc= 0.88582 val_loss= 0.67479 val_acc= 0.70115 val_auc= 0.74787 time= 0.45045\n",
            "Epoch: 0119 train_loss= 0.58555 train_acc= 0.80485 train_auc= 0.89089 val_loss= 0.72148 val_acc= 0.62069 val_auc= 0.68138 time= 0.47187\n",
            "Epoch: 0118 train_loss= 0.58398 train_acc= 0.80588 train_auc= 0.88670 val_loss= 0.68230 val_acc= 0.67045 val_auc= 0.72756 time= 0.46613\n",
            "Epoch: 0118 train_loss= 0.58623 train_acc= 0.80995 train_auc= 0.88837 val_loss= 0.66504 val_acc= 0.74713 val_auc= 0.79362 time= 0.50386\n",
            "Epoch: 0118 train_loss= 0.58102 train_acc= 0.81378 train_auc= 0.89769 val_loss= 0.69192 val_acc= 0.68966 val_auc= 0.74390 time= 0.42066\n",
            "Epoch: 0119 train_loss= 0.57708 train_acc= 0.83291 train_auc= 0.91035 val_loss= 0.69304 val_acc= 0.66667 val_auc= 0.74840 time= 0.50354\n",
            "Epoch: 0118 train_loss= 0.58784 train_acc= 0.80740 train_auc= 0.88283 val_loss= 0.68020 val_acc= 0.68966 val_auc= 0.75904 time= 0.52546\n",
            "Epoch: 0119 train_loss= 0.57943 train_acc= 0.81505 train_auc= 0.89803 val_loss= 0.71753 val_acc= 0.63218 val_auc= 0.67656 time= 0.51964\n",
            "Epoch: 0120 train_loss= 0.58760 train_acc= 0.79082 train_auc= 0.87691 val_loss= 0.64757 val_acc= 0.67816 val_auc= 0.78777 time= 0.43805\n",
            "Epoch: 0119 train_loss= 0.58526 train_acc= 0.79847 train_auc= 0.88344 val_loss= 0.65542 val_acc= 0.71264 val_auc= 0.79149 time= 0.49973\n",
            "Epoch: 0120 train_loss= 0.59114 train_acc= 0.78699 train_auc= 0.88273 val_loss= 0.72142 val_acc= 0.62069 val_auc= 0.68191 time= 0.43229\n",
            "Epoch: 0119 train_loss= 0.58688 train_acc= 0.80843 train_auc= 0.88818 val_loss= 0.68171 val_acc= 0.68182 val_auc= 0.73067 time= 0.44316\n",
            "Epoch: 0119 train_loss= 0.58379 train_acc= 0.80612 train_auc= 0.89192 val_loss= 0.67460 val_acc= 0.70115 val_auc= 0.75053 time= 0.48171\n",
            "Epoch: 0119 train_loss= 0.58239 train_acc= 0.82653 train_auc= 0.90412 val_loss= 0.66523 val_acc= 0.72414 val_auc= 0.79468 time= 0.44060\n",
            "Epoch: 0119 train_loss= 0.58615 train_acc= 0.80357 train_auc= 0.88436 val_loss= 0.69305 val_acc= 0.66667 val_auc= 0.74125 time= 0.44964\n",
            "Epoch: 0120 train_loss= 0.57818 train_acc= 0.83291 train_auc= 0.90343 val_loss= 0.69212 val_acc= 0.65517 val_auc= 0.74521 time= 0.45675\n",
            "Epoch: 0120 train_loss= 0.58217 train_acc= 0.80357 train_auc= 0.89200 val_loss= 0.71709 val_acc= 0.60920 val_auc= 0.67709 time= 0.42973\n",
            "Epoch: 0119 train_loss= 0.58719 train_acc= 0.78827 train_auc= 0.89190 val_loss= 0.68043 val_acc= 0.66667 val_auc= 0.75532 time= 0.45568\n",
            "Epoch: 0121 train_loss= 0.58046 train_acc= 0.79847 train_auc= 0.89525 val_loss= 0.64808 val_acc= 0.67816 val_auc= 0.78670 time= 0.48269\n",
            "Epoch: 0120 train_loss= 0.58665 train_acc= 0.78571 train_auc= 0.88112 val_loss= 0.67422 val_acc= 0.71264 val_auc= 0.75053 time= 0.44727\n",
            "Epoch: 0120 train_loss= 0.58191 train_acc= 0.81098 train_auc= 0.89396 val_loss= 0.68086 val_acc= 0.68182 val_auc= 0.72911 time= 0.46257\n",
            "Epoch: 0120 train_loss= 0.58349 train_acc= 0.78061 train_auc= 0.88195 val_loss= 0.65536 val_acc= 0.71264 val_auc= 0.79202 time= 0.51733\n",
            "Epoch: 0121 train_loss= 0.57829 train_acc= 0.81122 train_auc= 0.90152 val_loss= 0.72130 val_acc= 0.62069 val_auc= 0.68245 time= 0.52464\n",
            "Epoch: 0120 train_loss= 0.58075 train_acc= 0.79719 train_auc= 0.89370 val_loss= 0.66498 val_acc= 0.72414 val_auc= 0.79149 time= 0.43622\n",
            "Epoch: 0120 train_loss= 0.58451 train_acc= 0.79974 train_auc= 0.88263 val_loss= 0.69340 val_acc= 0.66667 val_auc= 0.74019 time= 0.45588\n",
            "Epoch: 0121 train_loss= 0.58009 train_acc= 0.80357 train_auc= 0.89941 val_loss= 0.71725 val_acc= 0.60920 val_auc= 0.67922 time= 0.40647\n",
            "Epoch: 0121 train_loss= 0.58003 train_acc= 0.80485 train_auc= 0.89669 val_loss= 0.69193 val_acc= 0.65517 val_auc= 0.74468 time= 0.44530\n",
            "Epoch: 0120 train_loss= 0.58924 train_acc= 0.79209 train_auc= 0.88215 val_loss= 0.68066 val_acc= 0.66667 val_auc= 0.75638 time= 0.47034\n",
            "Epoch: 0122 train_loss= 0.59338 train_acc= 0.79974 train_auc= 0.88563 val_loss= 0.64943 val_acc= 0.68966 val_auc= 0.78404 time= 0.43700\n",
            "Epoch: 0121 train_loss= 0.57217 train_acc= 0.81098 train_auc= 0.90887 val_loss= 0.68092 val_acc= 0.68182 val_auc= 0.73171 time= 0.41704\n",
            "Epoch: 0121 train_loss= 0.57635 train_acc= 0.81505 train_auc= 0.89996 val_loss= 0.67461 val_acc= 0.71264 val_auc= 0.74894 time= 0.43275\n",
            "Epoch: 0121 train_loss= 0.58434 train_acc= 0.82270 train_auc= 0.89335 val_loss= 0.65595 val_acc= 0.71264 val_auc= 0.79202 time= 0.43491\n",
            "Epoch: 0122 train_loss= 0.57921 train_acc= 0.82270 train_auc= 0.89903 val_loss= 0.72159 val_acc= 0.62069 val_auc= 0.68191 time= 0.44793\n",
            "Epoch: 0121 train_loss= 0.58428 train_acc= 0.81633 train_auc= 0.89642 val_loss= 0.66483 val_acc= 0.72414 val_auc= 0.79202 time= 0.49067\n",
            "Epoch: 0121 train_loss= 0.57625 train_acc= 0.82908 train_auc= 0.89926 val_loss= 0.69363 val_acc= 0.66667 val_auc= 0.73807 time= 0.47008\n",
            "Epoch: 0122 train_loss= 0.57583 train_acc= 0.80230 train_auc= 0.90051 val_loss= 0.71681 val_acc= 0.60920 val_auc= 0.67869 time= 0.42901\n",
            "Epoch: 0122 train_loss= 0.57730 train_acc= 0.81505 train_auc= 0.90404 val_loss= 0.69257 val_acc= 0.65517 val_auc= 0.74628 time= 0.50171\n",
            "Epoch: 0121 train_loss= 0.58098 train_acc= 0.81122 train_auc= 0.89735 val_loss= 0.68133 val_acc= 0.67816 val_auc= 0.75904 time= 0.47911\n",
            "Epoch: 0123 train_loss= 0.58359 train_acc= 0.82143 train_auc= 0.90115 val_loss= 0.64861 val_acc= 0.68966 val_auc= 0.78351 time= 0.45442\n",
            "Epoch: 0122 train_loss= 0.57627 train_acc= 0.82886 train_auc= 0.90839 val_loss= 0.68086 val_acc= 0.68182 val_auc= 0.73275 time= 0.46659\n",
            "Epoch: 0122 train_loss= 0.58955 train_acc= 0.79337 train_auc= 0.88831 val_loss= 0.67515 val_acc= 0.68966 val_auc= 0.74734 time= 0.44982\n",
            "Epoch: 0122 train_loss= 0.57458 train_acc= 0.82781 train_auc= 0.90677 val_loss= 0.65551 val_acc= 0.71264 val_auc= 0.79255 time= 0.41160\n",
            "Epoch: 0123 train_loss= 0.58638 train_acc= 0.79847 train_auc= 0.89019 val_loss= 0.72221 val_acc= 0.64368 val_auc= 0.68511 time= 0.51529\n",
            "Epoch: 0122 train_loss= 0.58362 train_acc= 0.80485 train_auc= 0.89549 val_loss= 0.66427 val_acc= 0.74713 val_auc= 0.79202 time= 0.51017\n",
            "Epoch: 0122 train_loss= 0.58688 train_acc= 0.82015 train_auc= 0.88797 val_loss= 0.69304 val_acc= 0.67816 val_auc= 0.73966 time= 0.55614\n",
            "Epoch: 0123 train_loss= 0.57620 train_acc= 0.79974 train_auc= 0.89551 val_loss= 0.71640 val_acc= 0.62069 val_auc= 0.68240 time= 0.58028\n",
            "Epoch: 0123 train_loss= 0.58191 train_acc= 0.80995 train_auc= 0.90087 val_loss= 0.69268 val_acc= 0.64368 val_auc= 0.74734 time= 0.51101\n",
            "Epoch: 0122 train_loss= 0.58535 train_acc= 0.81250 train_auc= 0.89457 val_loss= 0.68177 val_acc= 0.67816 val_auc= 0.76064 time= 0.51471\n",
            "Epoch: 0124 train_loss= 0.57888 train_acc= 0.80867 train_auc= 0.89500 val_loss= 0.64876 val_acc= 0.68966 val_auc= 0.78351 time= 0.51830\n",
            "Epoch: 0123 train_loss= 0.58040 train_acc= 0.80332 train_auc= 0.89906 val_loss= 0.68059 val_acc= 0.68182 val_auc= 0.73326 time= 0.51609\n",
            "Epoch: 0123 train_loss= 0.57652 train_acc= 0.82015 train_auc= 0.90602 val_loss= 0.67628 val_acc= 0.70115 val_auc= 0.74840 time= 0.51064\n",
            "Epoch: 0123 train_loss= 0.58230 train_acc= 0.80230 train_auc= 0.89936 val_loss= 0.65371 val_acc= 0.71264 val_auc= 0.79255 time= 0.55471\n",
            "Epoch: 0124 train_loss= 0.57731 train_acc= 0.80867 train_auc= 0.89724 val_loss= 0.72200 val_acc= 0.63218 val_auc= 0.68936 time= 0.46020\n",
            "Epoch: 0123 train_loss= 0.58203 train_acc= 0.81505 train_auc= 0.89899 val_loss= 0.66374 val_acc= 0.74713 val_auc= 0.79415 time= 0.43504\n",
            "Epoch: 0123 train_loss= 0.58417 train_acc= 0.80357 train_auc= 0.88937 val_loss= 0.69223 val_acc= 0.68966 val_auc= 0.74072 time= 0.47283\n",
            "Epoch: 0124 train_loss= 0.57136 train_acc= 0.82398 train_auc= 0.90586 val_loss= 0.69277 val_acc= 0.64368 val_auc= 0.74628 time= 0.52164\n",
            "Epoch: 0123 train_loss= 0.58744 train_acc= 0.79464 train_auc= 0.88654 val_loss= 0.68276 val_acc= 0.68966 val_auc= 0.76011 time= 0.50934\n",
            "Epoch: 0124 train_loss= 0.56482 train_acc= 0.81888 train_auc= 0.90682 val_loss= 0.71642 val_acc= 0.63218 val_auc= 0.68346 time= 0.53822\n",
            "Epoch: 0125 train_loss= 0.58636 train_acc= 0.79719 train_auc= 0.88860 val_loss= 0.64934 val_acc= 0.68966 val_auc= 0.78457 time= 0.46635\n",
            "Epoch: 0124 train_loss= 0.57319 train_acc= 0.80102 train_auc= 0.89882 val_loss= 0.65310 val_acc= 0.72414 val_auc= 0.79362 time= 0.41266\n",
            "Epoch: 0124 train_loss= 0.57334 train_acc= 0.81865 train_auc= 0.90640 val_loss= 0.68009 val_acc= 0.68182 val_auc= 0.73378 time= 0.47873\n",
            "Epoch: 0124 train_loss= 0.57408 train_acc= 0.82143 train_auc= 0.90914 val_loss= 0.67564 val_acc= 0.68966 val_auc= 0.74734 time= 0.48255\n",
            "Epoch: 0125 train_loss= 0.57938 train_acc= 0.78699 train_auc= 0.88714 val_loss= 0.72133 val_acc= 0.63218 val_auc= 0.68883 time= 0.46143\n",
            "Epoch: 0124 train_loss= 0.57632 train_acc= 0.81250 train_auc= 0.90128 val_loss= 0.66268 val_acc= 0.75862 val_auc= 0.79468 time= 0.42811\n",
            "Epoch: 0124 train_loss= 0.56946 train_acc= 0.80102 train_auc= 0.89941 val_loss= 0.69162 val_acc= 0.67816 val_auc= 0.74284 time= 0.45593\n",
            "Epoch: 0124 train_loss= 0.57384 train_acc= 0.81250 train_auc= 0.90081 val_loss= 0.68391 val_acc= 0.70115 val_auc= 0.75585 time= 0.44048\n",
            "Epoch: 0125 train_loss= 0.57993 train_acc= 0.81250 train_auc= 0.88792 val_loss= 0.69347 val_acc= 0.66667 val_auc= 0.74574 time= 0.48651\n",
            "Epoch: 0125 train_loss= 0.58300 train_acc= 0.78571 train_auc= 0.87686 val_loss= 0.71680 val_acc= 0.64368 val_auc= 0.68505 time= 0.47650\n",
            "Epoch: 0126 train_loss= 0.58913 train_acc= 0.78571 train_auc= 0.87247 val_loss= 0.64922 val_acc= 0.68966 val_auc= 0.78457 time= 0.49668\n",
            "Epoch: 0125 train_loss= 0.58911 train_acc= 0.77423 train_auc= 0.87084 val_loss= 0.65355 val_acc= 0.72414 val_auc= 0.79309 time= 0.48460\n",
            "Epoch: 0125 train_loss= 0.57696 train_acc= 0.81505 train_auc= 0.89122 val_loss= 0.67555 val_acc= 0.68966 val_auc= 0.74840 time= 0.45330\n",
            "Epoch: 0125 train_loss= 0.57661 train_acc= 0.81354 train_auc= 0.89405 val_loss= 0.68017 val_acc= 0.68182 val_auc= 0.73378 time= 0.49482\n",
            "Epoch: 0126 train_loss= 0.57864 train_acc= 0.80740 train_auc= 0.89155 val_loss= 0.71984 val_acc= 0.64368 val_auc= 0.69043 time= 0.47430\n",
            "Epoch: 0125 train_loss= 0.58252 train_acc= 0.80230 train_auc= 0.89142 val_loss= 0.66127 val_acc= 0.75862 val_auc= 0.79681 time= 0.45029\n",
            "Epoch: 0125 train_loss= 0.57964 train_acc= 0.80612 train_auc= 0.88797 val_loss= 0.69104 val_acc= 0.68966 val_auc= 0.74549 time= 0.50664\n",
            "Epoch: 0126 train_loss= 0.58186 train_acc= 0.79974 train_auc= 0.89015 val_loss= 0.69426 val_acc= 0.67816 val_auc= 0.74521 time= 0.43997\n",
            "Epoch: 0125 train_loss= 0.58842 train_acc= 0.78699 train_auc= 0.87813 val_loss= 0.68456 val_acc= 0.71264 val_auc= 0.75585 time= 0.49292\n",
            "Epoch: 0126 train_loss= 0.57553 train_acc= 0.79337 train_auc= 0.88974 val_loss= 0.71727 val_acc= 0.65517 val_auc= 0.68346 time= 0.47578\n",
            "Epoch: 0126 train_loss= 0.58592 train_acc= 0.80102 train_auc= 0.87974 val_loss= 0.65364 val_acc= 0.72414 val_auc= 0.79202 time= 0.43958\n",
            "Epoch: 0127 train_loss= 0.57820 train_acc= 0.80995 train_auc= 0.89369 val_loss= 0.64882 val_acc= 0.68966 val_auc= 0.78511 time= 0.45252\n",
            "Epoch: 0126 train_loss= 0.58877 train_acc= 0.78444 train_auc= 0.87740 val_loss= 0.67506 val_acc= 0.70115 val_auc= 0.75000 time= 0.43867\n",
            "Epoch: 0127 train_loss= 0.56945 train_acc= 0.81760 train_auc= 0.90212 val_loss= 0.71913 val_acc= 0.64368 val_auc= 0.68989 time= 0.42868\n",
            "Epoch: 0126 train_loss= 0.57584 train_acc= 0.79183 train_auc= 0.89373 val_loss= 0.68109 val_acc= 0.65909 val_auc= 0.73223 time= 0.46753\n",
            "Epoch: 0126 train_loss= 0.58289 train_acc= 0.80612 train_auc= 0.89032 val_loss= 0.66039 val_acc= 0.75862 val_auc= 0.79681 time= 0.44501\n",
            "Epoch: 0126 train_loss= 0.58142 train_acc= 0.80102 train_auc= 0.88710 val_loss= 0.69072 val_acc= 0.67816 val_auc= 0.74549 time= 0.43136\n",
            "Epoch: 0126 train_loss= 0.58381 train_acc= 0.79337 train_auc= 0.88388 val_loss= 0.68451 val_acc= 0.71264 val_auc= 0.75372 time= 0.41215\n",
            "Epoch: 0127 train_loss= 0.57121 train_acc= 0.82908 train_auc= 0.90197 val_loss= 0.69438 val_acc= 0.66667 val_auc= 0.74415 time= 0.46331\n",
            "Epoch: 0127 train_loss= 0.57168 train_acc= 0.82398 train_auc= 0.90669 val_loss= 0.71777 val_acc= 0.64368 val_auc= 0.68134 time= 0.46738\n",
            "Epoch: 0128 train_loss= 0.57252 train_acc= 0.82398 train_auc= 0.90267 val_loss= 0.64842 val_acc= 0.67816 val_auc= 0.78564 time= 0.42333\n",
            "Epoch: 0127 train_loss= 0.57375 train_acc= 0.82781 train_auc= 0.90464 val_loss= 0.67476 val_acc= 0.70115 val_auc= 0.74947 time= 0.43962\n",
            "Epoch: 0127 train_loss= 0.57768 train_acc= 0.80357 train_auc= 0.89531 val_loss= 0.65274 val_acc= 0.72414 val_auc= 0.79096 time= 0.49863\n",
            "Epoch: 0128 train_loss= 0.57372 train_acc= 0.80867 train_auc= 0.90153 val_loss= 0.71924 val_acc= 0.65517 val_auc= 0.69202 time= 0.45706\n",
            "Epoch: 0127 train_loss= 0.56624 train_acc= 0.83525 train_auc= 0.90848 val_loss= 0.68157 val_acc= 0.62500 val_auc= 0.73223 time= 0.46193\n",
            "Epoch: 0127 train_loss= 0.57786 train_acc= 0.81250 train_auc= 0.89474 val_loss= 0.65978 val_acc= 0.74713 val_auc= 0.79521 time= 0.47283\n",
            "Epoch: 0127 train_loss= 0.57696 train_acc= 0.79082 train_auc= 0.89242 val_loss= 0.69044 val_acc= 0.67816 val_auc= 0.74496 time= 0.44753\n",
            "Epoch: 0127 train_loss= 0.57119 train_acc= 0.82526 train_auc= 0.90807 val_loss= 0.68423 val_acc= 0.66667 val_auc= 0.75160 time= 0.43592\n",
            "Epoch: 0128 train_loss= 0.56704 train_acc= 0.83673 train_auc= 0.91213 val_loss= 0.71851 val_acc= 0.64368 val_auc= 0.67922 time= 0.42159\n",
            "Epoch: 0128 train_loss= 0.56954 train_acc= 0.81378 train_auc= 0.90298 val_loss= 0.69396 val_acc= 0.65517 val_auc= 0.74468 time= 0.47216\n",
            "Epoch: 0129 train_loss= 0.57493 train_acc= 0.82526 train_auc= 0.89406 val_loss= 0.64824 val_acc= 0.67816 val_auc= 0.78511 time= 0.46956\n",
            "Epoch: 0128 train_loss= 0.56901 train_acc= 0.84056 train_auc= 0.91270 val_loss= 0.67515 val_acc= 0.70115 val_auc= 0.74734 time= 0.45817\n",
            "Epoch: 0128 train_loss= 0.57670 train_acc= 0.80740 train_auc= 0.89601 val_loss= 0.65156 val_acc= 0.72414 val_auc= 0.79149 time= 0.41045\n",
            "Epoch: 0129 train_loss= 0.58160 train_acc= 0.79974 train_auc= 0.88593 val_loss= 0.71914 val_acc= 0.65517 val_auc= 0.69255 time= 0.40055\n",
            "Epoch: 0128 train_loss= 0.56850 train_acc= 0.83014 train_auc= 0.90885 val_loss= 0.68128 val_acc= 0.62500 val_auc= 0.73275 time= 0.51665\n",
            "Epoch: 0128 train_loss= 0.56451 train_acc= 0.82781 train_auc= 0.91235 val_loss= 0.66010 val_acc= 0.74713 val_auc= 0.79415 time= 0.48787\n",
            "Epoch: 0128 train_loss= 0.57847 train_acc= 0.81250 train_auc= 0.89695 val_loss= 0.69016 val_acc= 0.67816 val_auc= 0.74867 time= 0.47582\n",
            "Epoch: 0128 train_loss= 0.57958 train_acc= 0.80102 train_auc= 0.89713 val_loss= 0.68400 val_acc= 0.67816 val_auc= 0.75000 time= 0.50693\n",
            "Epoch: 0129 train_loss= 0.57270 train_acc= 0.81633 train_auc= 0.89917 val_loss= 0.69409 val_acc= 0.64368 val_auc= 0.74096 time= 0.41472\n",
            "Epoch: 0129 train_loss= 0.56540 train_acc= 0.83291 train_auc= 0.91076 val_loss= 0.71909 val_acc= 0.64368 val_auc= 0.67762 time= 0.47296\n",
            "Epoch: 0129 train_loss= 0.57565 train_acc= 0.80740 train_auc= 0.89918 val_loss= 0.65094 val_acc= 0.72414 val_auc= 0.79202 time= 0.47037\n",
            "Epoch: 0130 train_loss= 0.57651 train_acc= 0.80740 train_auc= 0.89354 val_loss= 0.64735 val_acc= 0.66667 val_auc= 0.78511 time= 0.50823\n",
            "Epoch: 0129 train_loss= 0.57080 train_acc= 0.80485 train_auc= 0.90198 val_loss= 0.67524 val_acc= 0.70115 val_auc= 0.74734 time= 0.49672\n",
            "Epoch: 0130 train_loss= 0.57613 train_acc= 0.81505 train_auc= 0.89319 val_loss= 0.71907 val_acc= 0.64368 val_auc= 0.69096 time= 0.51181\n",
            "Epoch: 0129 train_loss= 0.57209 train_acc= 0.82781 train_auc= 0.90044 val_loss= 0.66024 val_acc= 0.74713 val_auc= 0.79309 time= 0.39141\n",
            "Epoch: 0129 train_loss= 0.57292 train_acc= 0.83525 train_auc= 0.90339 val_loss= 0.68077 val_acc= 0.67045 val_auc= 0.73482 time= 0.47438\n",
            "Epoch: 0129 train_loss= 0.57813 train_acc= 0.79592 train_auc= 0.89058 val_loss= 0.68977 val_acc= 0.68966 val_auc= 0.75027 time= 0.40091\n",
            "Epoch: 0130 train_loss= 0.57243 train_acc= 0.81760 train_auc= 0.90119 val_loss= 0.71972 val_acc= 0.65517 val_auc= 0.67444 time= 0.40651\n",
            "Epoch: 0129 train_loss= 0.58535 train_acc= 0.80612 train_auc= 0.88504 val_loss= 0.68362 val_acc= 0.67816 val_auc= 0.75213 time= 0.42879\n",
            "Epoch: 0130 train_loss= 0.57472 train_acc= 0.80612 train_auc= 0.88851 val_loss= 0.69394 val_acc= 0.64368 val_auc= 0.74415 time= 0.45191\n",
            "Epoch: 0131 train_loss= 0.57394 train_acc= 0.81250 train_auc= 0.89490 val_loss= 0.64696 val_acc= 0.66667 val_auc= 0.78457 time= 0.40782\n",
            "Epoch: 0130 train_loss= 0.56980 train_acc= 0.83418 train_auc= 0.90562 val_loss= 0.67556 val_acc= 0.70115 val_auc= 0.74521 time= 0.42469\n",
            "Epoch: 0130 train_loss= 0.58152 train_acc= 0.81122 train_auc= 0.88688 val_loss= 0.65053 val_acc= 0.72414 val_auc= 0.79149 time= 0.45240\n",
            "Epoch: 0131 train_loss= 0.57007 train_acc= 0.81505 train_auc= 0.90110 val_loss= 0.71925 val_acc= 0.63218 val_auc= 0.69096 time= 0.43621\n",
            "Epoch: 0130 train_loss= 0.57647 train_acc= 0.80077 train_auc= 0.89618 val_loss= 0.68019 val_acc= 0.68182 val_auc= 0.73275 time= 0.43667\n",
            "Epoch: 0130 train_loss= 0.56865 train_acc= 0.80995 train_auc= 0.89903 val_loss= 0.65995 val_acc= 0.75862 val_auc= 0.79574 time= 0.48954\n",
            "Epoch: 0130 train_loss= 0.56731 train_acc= 0.82398 train_auc= 0.90580 val_loss= 0.68964 val_acc= 0.68966 val_auc= 0.75133 time= 0.41758\n",
            "Epoch: 0131 train_loss= 0.57034 train_acc= 0.81888 train_auc= 0.90294 val_loss= 0.72032 val_acc= 0.65517 val_auc= 0.67179 time= 0.43704\n",
            "Epoch: 0131 train_loss= 0.56768 train_acc= 0.82143 train_auc= 0.90622 val_loss= 0.69436 val_acc= 0.66667 val_auc= 0.74149 time= 0.42000\n",
            "Epoch: 0130 train_loss= 0.57994 train_acc= 0.78954 train_auc= 0.89031 val_loss= 0.68241 val_acc= 0.66667 val_auc= 0.75266 time= 0.47108\n",
            "Epoch: 0131 train_loss= 0.58330 train_acc= 0.79974 train_auc= 0.87963 val_loss= 0.67595 val_acc= 0.70115 val_auc= 0.74574 time= 0.36152\n",
            "Epoch: 0132 train_loss= 0.57590 train_acc= 0.82781 train_auc= 0.89657 val_loss= 0.64707 val_acc= 0.66667 val_auc= 0.78457 time= 0.47184\n",
            "Epoch: 0131 train_loss= 0.57959 train_acc= 0.80485 train_auc= 0.89021 val_loss= 0.65171 val_acc= 0.72414 val_auc= 0.79255 time= 0.43377\n",
            "Epoch: 0132 train_loss= 0.57434 train_acc= 0.79337 train_auc= 0.89665 val_loss= 0.71942 val_acc= 0.63218 val_auc= 0.69255 time= 0.41432\n",
            "Epoch: 0131 train_loss= 0.57095 train_acc= 0.82886 train_auc= 0.90253 val_loss= 0.68022 val_acc= 0.69318 val_auc= 0.73482 time= 0.41112\n",
            "Epoch: 0131 train_loss= 0.57420 train_acc= 0.80740 train_auc= 0.90100 val_loss= 0.65932 val_acc= 0.75862 val_auc= 0.79521 time= 0.43104\n",
            "Epoch: 0131 train_loss= 0.56844 train_acc= 0.82398 train_auc= 0.90523 val_loss= 0.68914 val_acc= 0.68966 val_auc= 0.75610 time= 0.42844\n",
            "Epoch: 0132 train_loss= 0.58154 train_acc= 0.80612 train_auc= 0.88870 val_loss= 0.72083 val_acc= 0.64368 val_auc= 0.67020 time= 0.42503\n",
            "Epoch: 0132 train_loss= 0.57744 train_acc= 0.80485 train_auc= 0.89062 val_loss= 0.67675 val_acc= 0.71264 val_auc= 0.74521 time= 0.40323\n",
            "Epoch: 0132 train_loss= 0.56633 train_acc= 0.83036 train_auc= 0.91115 val_loss= 0.69436 val_acc= 0.66667 val_auc= 0.74309 time= 0.45570\n",
            "Epoch: 0131 train_loss= 0.57884 train_acc= 0.80485 train_auc= 0.89374 val_loss= 0.68187 val_acc= 0.66667 val_auc= 0.75106 time= 0.46541\n",
            "Epoch: 0133 train_loss= 0.57844 train_acc= 0.79974 train_auc= 0.89537 val_loss= 0.64808 val_acc= 0.67816 val_auc= 0.78617 time= 0.41024\n",
            "Epoch: 0133 train_loss= 0.57363 train_acc= 0.82653 train_auc= 0.90598 val_loss= 0.71919 val_acc= 0.62069 val_auc= 0.69415 time= 0.41684\n",
            "Epoch: 0132 train_loss= 0.56925 train_acc= 0.82781 train_auc= 0.90463 val_loss= 0.65113 val_acc= 0.72414 val_auc= 0.79309 time= 0.43932\n",
            "Epoch: 0132 train_loss= 0.56987 train_acc= 0.82248 train_auc= 0.90223 val_loss= 0.68019 val_acc= 0.69318 val_auc= 0.73586 time= 0.41118\n",
            "Epoch: 0132 train_loss= 0.57842 train_acc= 0.80357 train_auc= 0.89426 val_loss= 0.65880 val_acc= 0.75862 val_auc= 0.79681 time= 0.38889\n",
            "Epoch: 0132 train_loss= 0.57151 train_acc= 0.80867 train_auc= 0.89895 val_loss= 0.68834 val_acc= 0.68966 val_auc= 0.75557 time= 0.44542\n",
            "Epoch: 0133 train_loss= 0.57401 train_acc= 0.81378 train_auc= 0.89311 val_loss= 0.67642 val_acc= 0.71264 val_auc= 0.74202 time= 0.40007\n",
            "Epoch: 0133 train_loss= 0.57458 train_acc= 0.81760 train_auc= 0.89701 val_loss= 0.72140 val_acc= 0.64368 val_auc= 0.67073 time= 0.43360\n",
            "Epoch: 0133 train_loss= 0.57886 train_acc= 0.81250 train_auc= 0.89366 val_loss= 0.69525 val_acc= 0.68966 val_auc= 0.74255 time= 0.39845\n",
            "Epoch: 0132 train_loss= 0.57357 train_acc= 0.81633 train_auc= 0.89911 val_loss= 0.68210 val_acc= 0.66667 val_auc= 0.74947 time= 0.44943\n",
            "Epoch: 0134 train_loss= 0.57298 train_acc= 0.81760 train_auc= 0.90990 val_loss= 0.71896 val_acc= 0.63218 val_auc= 0.69628 time= 0.46435\n",
            "Epoch: 0134 train_loss= 0.57393 train_acc= 0.82015 train_auc= 0.90736 val_loss= 0.64716 val_acc= 0.68966 val_auc= 0.78723 time= 0.51436\n",
            "Epoch: 0133 train_loss= 0.58220 train_acc= 0.80357 train_auc= 0.88486 val_loss= 0.65221 val_acc= 0.72414 val_auc= 0.79521 time= 0.48078\n",
            "Epoch: 0133 train_loss= 0.57335 train_acc= 0.82653 train_auc= 0.90549 val_loss= 0.65849 val_acc= 0.73563 val_auc= 0.79681 time= 0.41329\n",
            "Epoch: 0133 train_loss= 0.57868 train_acc= 0.80204 train_auc= 0.89369 val_loss= 0.68034 val_acc= 0.69318 val_auc= 0.73378 time= 0.44836\n",
            "Epoch: 0133 train_loss= 0.57334 train_acc= 0.80867 train_auc= 0.89722 val_loss= 0.68712 val_acc= 0.68966 val_auc= 0.75928 time= 0.43810\n",
            "Epoch: 0134 train_loss= 0.57163 train_acc= 0.81760 train_auc= 0.90543 val_loss= 0.69626 val_acc= 0.67816 val_auc= 0.73936 time= 0.42674\n",
            "Epoch: 0134 train_loss= 0.56815 train_acc= 0.82015 train_auc= 0.91018 val_loss= 0.67621 val_acc= 0.68966 val_auc= 0.74096 time= 0.49925\n",
            "Epoch: 0134 train_loss= 0.56751 train_acc= 0.81888 train_auc= 0.91016 val_loss= 0.72203 val_acc= 0.65517 val_auc= 0.66861 time= 0.46936\n",
            "Epoch: 0133 train_loss= 0.56992 train_acc= 0.82398 train_auc= 0.90666 val_loss= 0.68216 val_acc= 0.67816 val_auc= 0.75106 time= 0.43093\n",
            "Epoch: 0135 train_loss= 0.57367 train_acc= 0.81888 train_auc= 0.90331 val_loss= 0.64653 val_acc= 0.70115 val_auc= 0.78830 time= 0.39174\n",
            "Epoch: 0135 train_loss= 0.58286 train_acc= 0.79464 train_auc= 0.88785 val_loss= 0.71908 val_acc= 0.64368 val_auc= 0.69681 time= 0.44157\n",
            "Epoch: 0134 train_loss= 0.57124 train_acc= 0.83291 train_auc= 0.90582 val_loss= 0.65130 val_acc= 0.72414 val_auc= 0.79681 time= 0.41181\n",
            "Epoch: 0134 train_loss= 0.56745 train_acc= 0.83163 train_auc= 0.91518 val_loss= 0.65873 val_acc= 0.73563 val_auc= 0.79734 time= 0.39882\n",
            "Epoch: 0134 train_loss= 0.57805 train_acc= 0.79310 train_auc= 0.89165 val_loss= 0.68038 val_acc= 0.69318 val_auc= 0.73326 time= 0.41262\n",
            "Epoch: 0135 train_loss= 0.56920 train_acc= 0.82653 train_auc= 0.90749 val_loss= 0.69670 val_acc= 0.66667 val_auc= 0.73989 time= 0.36779\n",
            "Epoch: 0135 train_loss= 0.57569 train_acc= 0.81250 train_auc= 0.90148 val_loss= 0.67631 val_acc= 0.68966 val_auc= 0.73883 time= 0.38256\n",
            "Epoch: 0135 train_loss= 0.56419 train_acc= 0.83291 train_auc= 0.90987 val_loss= 0.72302 val_acc= 0.65517 val_auc= 0.66596 time= 0.38499\n",
            "Epoch: 0134 train_loss= 0.57866 train_acc= 0.81250 train_auc= 0.89844 val_loss= 0.68245 val_acc= 0.68966 val_auc= 0.75213 time= 0.38481\n",
            "Epoch: 0134 train_loss= 0.56303 train_acc= 0.83673 train_auc= 0.91854 val_loss= 0.68613 val_acc= 0.68966 val_auc= 0.76087 time= 0.50830\n",
            "Epoch: 0136 train_loss= 0.57451 train_acc= 0.82015 train_auc= 0.89857 val_loss= 0.64649 val_acc= 0.70115 val_auc= 0.79043 time= 0.41069\n",
            "Epoch: 0135 train_loss= 0.57729 train_acc= 0.80740 train_auc= 0.88968 val_loss= 0.65019 val_acc= 0.72414 val_auc= 0.79681 time= 0.39745\n",
            "Epoch: 0136 train_loss= 0.57309 train_acc= 0.81633 train_auc= 0.89850 val_loss= 0.71937 val_acc= 0.63218 val_auc= 0.69734 time= 0.40902\n",
            "Epoch: 0135 train_loss= 0.56861 train_acc= 0.82270 train_auc= 0.91444 val_loss= 0.65941 val_acc= 0.74713 val_auc= 0.79574 time= 0.40280\n",
            "Epoch: 0135 train_loss= 0.57567 train_acc= 0.79694 train_auc= 0.89575 val_loss= 0.68044 val_acc= 0.65909 val_auc= 0.73430 time= 0.52473\n",
            "Epoch: 0136 train_loss= 0.56883 train_acc= 0.82908 train_auc= 0.90583 val_loss= 0.69777 val_acc= 0.66667 val_auc= 0.73670 time= 0.44813\n",
            "Epoch: 0136 train_loss= 0.57112 train_acc= 0.80357 train_auc= 0.89437 val_loss= 0.72426 val_acc= 0.64368 val_auc= 0.66755 time= 0.40482\n",
            "Epoch: 0135 train_loss= 0.57150 train_acc= 0.81250 train_auc= 0.90456 val_loss= 0.68241 val_acc= 0.68966 val_auc= 0.75372 time= 0.46564\n",
            "Epoch: 0136 train_loss= 0.56756 train_acc= 0.82143 train_auc= 0.91426 val_loss= 0.67691 val_acc= 0.68966 val_auc= 0.73883 time= 0.45421\n",
            "Epoch: 0135 train_loss= 0.56920 train_acc= 0.81250 train_auc= 0.90684 val_loss= 0.68544 val_acc= 0.67816 val_auc= 0.76140 time= 0.50559\n",
            "Epoch: 0137 train_loss= 0.56794 train_acc= 0.82270 train_auc= 0.90447 val_loss= 0.64689 val_acc= 0.71264 val_auc= 0.79043 time= 0.47061\n",
            "Epoch: 0137 train_loss= 0.56145 train_acc= 0.83163 train_auc= 0.90620 val_loss= 0.72019 val_acc= 0.63218 val_auc= 0.69681 time= 0.43352\n",
            "Epoch: 0136 train_loss= 0.56577 train_acc= 0.83036 train_auc= 0.91016 val_loss= 0.66058 val_acc= 0.74713 val_auc= 0.79468 time= 0.38066\n",
            "Epoch: 0136 train_loss= 0.57946 train_acc= 0.80612 train_auc= 0.88733 val_loss= 0.64937 val_acc= 0.72414 val_auc= 0.80053 time= 0.45997\n",
            "Epoch: 0136 train_loss= 0.57311 train_acc= 0.81992 train_auc= 0.89975 val_loss= 0.68100 val_acc= 0.64773 val_auc= 0.73326 time= 0.35895\n",
            "Epoch: 0137 train_loss= 0.56569 train_acc= 0.80612 train_auc= 0.89830 val_loss= 0.72526 val_acc= 0.64368 val_auc= 0.66755 time= 0.40472\n",
            "Epoch: 0137 train_loss= 0.56296 train_acc= 0.82015 train_auc= 0.90664 val_loss= 0.69863 val_acc= 0.66667 val_auc= 0.73564 time= 0.44095\n",
            "Epoch: 0136 train_loss= 0.57510 train_acc= 0.81888 train_auc= 0.89533 val_loss= 0.68226 val_acc= 0.71264 val_auc= 0.75479 time= 0.44310\n",
            "Epoch: 0136 train_loss= 0.57599 train_acc= 0.82398 train_auc= 0.90079 val_loss= 0.68535 val_acc= 0.68966 val_auc= 0.76405 time= 0.44531\n",
            "Epoch: 0137 train_loss= 0.57437 train_acc= 0.80995 train_auc= 0.89405 val_loss= 0.67748 val_acc= 0.68966 val_auc= 0.73883 time= 0.48255\n",
            "Epoch: 0138 train_loss= 0.58010 train_acc= 0.80230 train_auc= 0.89423 val_loss= 0.64756 val_acc= 0.70115 val_auc= 0.79149 time= 0.41666\n",
            "Epoch: 0137 train_loss= 0.56415 train_acc= 0.81888 train_auc= 0.90824 val_loss= 0.66147 val_acc= 0.73563 val_auc= 0.79521 time= 0.37137\n",
            "Epoch: 0138 train_loss= 0.57379 train_acc= 0.80612 train_auc= 0.89103 val_loss= 0.72099 val_acc= 0.63218 val_auc= 0.69468 time= 0.42898\n",
            "Epoch: 0137 train_loss= 0.57604 train_acc= 0.79209 train_auc= 0.88904 val_loss= 0.64841 val_acc= 0.73563 val_auc= 0.80372 time= 0.41312\n",
            "Epoch: 0137 train_loss= 0.56771 train_acc= 0.83653 train_auc= 0.90429 val_loss= 0.68205 val_acc= 0.64773 val_auc= 0.73067 time= 0.45170\n",
            "Epoch: 0138 train_loss= 0.56618 train_acc= 0.81250 train_auc= 0.90672 val_loss= 0.69856 val_acc= 0.66667 val_auc= 0.73245 time= 0.43422\n",
            "Epoch: 0138 train_loss= 0.56064 train_acc= 0.80740 train_auc= 0.90504 val_loss= 0.72532 val_acc= 0.64368 val_auc= 0.66755 time= 0.54428\n",
            "Epoch: 0137 train_loss= 0.56570 train_acc= 0.81633 train_auc= 0.90634 val_loss= 0.68241 val_acc= 0.70115 val_auc= 0.75372 time= 0.45541\n",
            "Epoch: 0137 train_loss= 0.57018 train_acc= 0.80230 train_auc= 0.89918 val_loss= 0.68550 val_acc= 0.68966 val_auc= 0.76299 time= 0.45335\n",
            "Epoch: 0139 train_loss= 0.56519 train_acc= 0.84056 train_auc= 0.91647 val_loss= 0.64565 val_acc= 0.70115 val_auc= 0.79202 time= 0.50105\n",
            "Epoch: 0138 train_loss= 0.57497 train_acc= 0.79337 train_auc= 0.89193 val_loss= 0.67736 val_acc= 0.70115 val_auc= 0.74096 time= 0.53599\n",
            "Epoch: 0138 train_loss= 0.56877 train_acc= 0.82143 train_auc= 0.90636 val_loss= 0.66213 val_acc= 0.73563 val_auc= 0.79415 time= 0.58514\n",
            "Epoch: 0139 train_loss= 0.57566 train_acc= 0.81760 train_auc= 0.89532 val_loss= 0.72147 val_acc= 0.63218 val_auc= 0.69415 time= 0.53255\n",
            "Epoch: 0138 train_loss= 0.57525 train_acc= 0.81122 train_auc= 0.89449 val_loss= 0.64866 val_acc= 0.72414 val_auc= 0.80266 time= 0.50647\n",
            "Epoch: 0138 train_loss= 0.57907 train_acc= 0.79694 train_auc= 0.88501 val_loss= 0.68198 val_acc= 0.63636 val_auc= 0.73015 time= 0.52225\n",
            "Epoch: 0139 train_loss= 0.56318 train_acc= 0.82781 train_auc= 0.91228 val_loss= 0.69749 val_acc= 0.67816 val_auc= 0.73298 time= 0.44596\n",
            "Epoch: 0138 train_loss= 0.57056 train_acc= 0.81633 train_auc= 0.89963 val_loss= 0.68583 val_acc= 0.68966 val_auc= 0.76034 time= 0.38373\n",
            "Epoch: 0139 train_loss= 0.55948 train_acc= 0.81633 train_auc= 0.90453 val_loss= 0.72560 val_acc= 0.65517 val_auc= 0.66755 time= 0.41498\n",
            "Epoch: 0138 train_loss= 0.58186 train_acc= 0.79464 train_auc= 0.88216 val_loss= 0.68289 val_acc= 0.67816 val_auc= 0.75426 time= 0.42519\n",
            "Epoch: 0139 train_loss= 0.56327 train_acc= 0.82015 train_auc= 0.91061 val_loss= 0.67679 val_acc= 0.70115 val_auc= 0.74149 time= 0.43002\n",
            "Epoch: 0140 train_loss= 0.58404 train_acc= 0.79847 train_auc= 0.87811 val_loss= 0.64438 val_acc= 0.70115 val_auc= 0.79362 time= 0.46910\n",
            "Epoch: 0139 train_loss= 0.57226 train_acc= 0.81760 train_auc= 0.90303 val_loss= 0.66229 val_acc= 0.73563 val_auc= 0.79468 time= 0.46163\n",
            "Epoch: 0140 train_loss= 0.57652 train_acc= 0.80102 train_auc= 0.88885 val_loss= 0.72182 val_acc= 0.63218 val_auc= 0.69574 time= 0.51275\n",
            "Epoch: 0139 train_loss= 0.56715 train_acc= 0.81122 train_auc= 0.90526 val_loss= 0.64653 val_acc= 0.72414 val_auc= 0.80213 time= 0.47413\n",
            "Epoch: 0140 train_loss= 0.57508 train_acc= 0.80102 train_auc= 0.88836 val_loss= 0.69705 val_acc= 0.67816 val_auc= 0.73351 time= 0.41711\n",
            "Epoch: 0139 train_loss= 0.56189 train_acc= 0.83014 train_auc= 0.92017 val_loss= 0.68135 val_acc= 0.64773 val_auc= 0.73171 time= 0.47888\n",
            "Epoch: 0140 train_loss= 0.56969 train_acc= 0.81505 train_auc= 0.89632 val_loss= 0.72524 val_acc= 0.63218 val_auc= 0.66967 time= 0.42724\n",
            "Epoch: 0139 train_loss= 0.56043 train_acc= 0.82015 train_auc= 0.91440 val_loss= 0.68657 val_acc= 0.68966 val_auc= 0.75875 time= 0.47011\n",
            "Epoch: 0139 train_loss= 0.56870 train_acc= 0.82398 train_auc= 0.90632 val_loss= 0.68233 val_acc= 0.67816 val_auc= 0.75372 time= 0.46609\n",
            "Epoch: 0141 train_loss= 0.56692 train_acc= 0.81633 train_auc= 0.90485 val_loss= 0.64402 val_acc= 0.68966 val_auc= 0.79468 time= 0.40871\n",
            "Epoch: 0140 train_loss= 0.57337 train_acc= 0.82015 train_auc= 0.89401 val_loss= 0.67630 val_acc= 0.70115 val_auc= 0.74309 time= 0.46748\n",
            "Epoch: 0140 train_loss= 0.56921 train_acc= 0.83163 train_auc= 0.91012 val_loss= 0.66271 val_acc= 0.74713 val_auc= 0.79149 time= 0.47266\n",
            "Epoch: 0140 train_loss= 0.57992 train_acc= 0.79974 train_auc= 0.88657 val_loss= 0.64649 val_acc= 0.73563 val_auc= 0.80266 time= 0.45749\n",
            "Epoch: 0141 train_loss= 0.56150 train_acc= 0.80740 train_auc= 0.91357 val_loss= 0.72329 val_acc= 0.64368 val_auc= 0.69415 time= 0.49917\n",
            "Epoch: 0141 train_loss= 0.56079 train_acc= 0.82781 train_auc= 0.91449 val_loss= 0.69704 val_acc= 0.66667 val_auc= 0.73245 time= 0.48256\n",
            "Epoch: 0140 train_loss= 0.57495 train_acc= 0.81098 train_auc= 0.88982 val_loss= 0.68118 val_acc= 0.67045 val_auc= 0.73067 time= 0.44514\n",
            "Epoch: 0140 train_loss= 0.57672 train_acc= 0.80740 train_auc= 0.89130 val_loss= 0.68817 val_acc= 0.68966 val_auc= 0.75345 time= 0.42263\n",
            "Epoch: 0141 train_loss= 0.55526 train_acc= 0.82270 train_auc= 0.91324 val_loss= 0.72571 val_acc= 0.64368 val_auc= 0.67179 time= 0.44411\n",
            "Epoch: 0140 train_loss= 0.57736 train_acc= 0.81250 train_auc= 0.89789 val_loss= 0.68180 val_acc= 0.66667 val_auc= 0.75426 time= 0.41683\n",
            "Epoch: 0142 train_loss= 0.56538 train_acc= 0.83801 train_auc= 0.90748 val_loss= 0.64364 val_acc= 0.68966 val_auc= 0.79415 time= 0.41792\n",
            "Epoch: 0141 train_loss= 0.56570 train_acc= 0.81250 train_auc= 0.90833 val_loss= 0.67697 val_acc= 0.70115 val_auc= 0.74309 time= 0.44635\n",
            "Epoch: 0141 train_loss= 0.56407 train_acc= 0.83036 train_auc= 0.90943 val_loss= 0.66340 val_acc= 0.73563 val_auc= 0.79202 time= 0.44129\n",
            "Epoch: 0142 train_loss= 0.57111 train_acc= 0.81505 train_auc= 0.89242 val_loss= 0.72459 val_acc= 0.66667 val_auc= 0.69468 time= 0.41747\n",
            "Epoch: 0141 train_loss= 0.55912 train_acc= 0.82270 train_auc= 0.90734 val_loss= 0.64664 val_acc= 0.72414 val_auc= 0.80266 time= 0.46019\n",
            "Epoch: 0142 train_loss= 0.56446 train_acc= 0.81633 train_auc= 0.90189 val_loss= 0.69757 val_acc= 0.66667 val_auc= 0.73191 time= 0.41188\n",
            "Epoch: 0141 train_loss= 0.56548 train_acc= 0.81760 train_auc= 0.90571 val_loss= 0.68979 val_acc= 0.68966 val_auc= 0.74920 time= 0.42517\n",
            "Epoch: 0142 train_loss= 0.55902 train_acc= 0.81888 train_auc= 0.90981 val_loss= 0.72563 val_acc= 0.63218 val_auc= 0.67020 time= 0.42348\n",
            "Epoch: 0141 train_loss= 0.56250 train_acc= 0.81992 train_auc= 0.90928 val_loss= 0.68131 val_acc= 0.68182 val_auc= 0.73223 time= 0.48769\n",
            "Epoch: 0141 train_loss= 0.57253 train_acc= 0.80357 train_auc= 0.89705 val_loss= 0.68149 val_acc= 0.66667 val_auc= 0.75372 time= 0.43846\n",
            "Epoch: 0143 train_loss= 0.57399 train_acc= 0.81633 train_auc= 0.89292 val_loss= 0.64352 val_acc= 0.68966 val_auc= 0.79574 time= 0.42407\n",
            "Epoch: 0142 train_loss= 0.57288 train_acc= 0.81250 train_auc= 0.89278 val_loss= 0.67683 val_acc= 0.70115 val_auc= 0.74149 time= 0.45194\n",
            "Epoch: 0142 train_loss= 0.57761 train_acc= 0.81888 train_auc= 0.89520 val_loss= 0.66394 val_acc= 0.73563 val_auc= 0.78989 time= 0.41289\n",
            "Epoch: 0142 train_loss= 0.57698 train_acc= 0.79337 train_auc= 0.88486 val_loss= 0.64726 val_acc= 0.72414 val_auc= 0.80319 time= 0.38756\n",
            "Epoch: 0143 train_loss= 0.56810 train_acc= 0.80357 train_auc= 0.89814 val_loss= 0.72589 val_acc= 0.65517 val_auc= 0.69309 time= 0.46979\n",
            "Epoch: 0143 train_loss= 0.56825 train_acc= 0.81633 train_auc= 0.90489 val_loss= 0.72575 val_acc= 0.63218 val_auc= 0.66861 time= 0.44856\n",
            "Epoch: 0143 train_loss= 0.56777 train_acc= 0.80867 train_auc= 0.89349 val_loss= 0.69854 val_acc= 0.66667 val_auc= 0.73191 time= 0.48417\n",
            "Epoch: 0142 train_loss= 0.55977 train_acc= 0.83163 train_auc= 0.91325 val_loss= 0.68188 val_acc= 0.66667 val_auc= 0.75266 time= 0.40284\n",
            "Epoch: 0142 train_loss= 0.56478 train_acc= 0.81633 train_auc= 0.90267 val_loss= 0.69077 val_acc= 0.70115 val_auc= 0.74920 time= 0.43953\n",
            "Epoch: 0142 train_loss= 0.57760 train_acc= 0.80588 train_auc= 0.89089 val_loss= 0.68152 val_acc= 0.68182 val_auc= 0.72963 time= 0.49470\n",
            "Epoch: 0144 train_loss= 0.56275 train_acc= 0.81888 train_auc= 0.90693 val_loss= 0.64371 val_acc= 0.68966 val_auc= 0.79734 time= 0.48519\n",
            "Epoch: 0143 train_loss= 0.56952 train_acc= 0.80230 train_auc= 0.89451 val_loss= 0.67682 val_acc= 0.70115 val_auc= 0.74149 time= 0.46913\n",
            "Epoch: 0143 train_loss= 0.57002 train_acc= 0.79974 train_auc= 0.89967 val_loss= 0.66299 val_acc= 0.73563 val_auc= 0.78936 time= 0.45177\n",
            "Epoch: 0143 train_loss= 0.56797 train_acc= 0.79719 train_auc= 0.89515 val_loss= 0.64829 val_acc= 0.72414 val_auc= 0.80532 time= 0.45250\n",
            "Epoch: 0144 train_loss= 0.56765 train_acc= 0.80995 train_auc= 0.89835 val_loss= 0.72694 val_acc= 0.64368 val_auc= 0.69255 time= 0.45348\n",
            "Epoch: 0144 train_loss= 0.55565 train_acc= 0.83546 train_auc= 0.92142 val_loss= 0.72614 val_acc= 0.63218 val_auc= 0.66543 time= 0.43742\n",
            "Epoch: 0144 train_loss= 0.56650 train_acc= 0.83163 train_auc= 0.90941 val_loss= 0.69993 val_acc= 0.67816 val_auc= 0.72979 time= 0.45883\n",
            "Epoch: 0143 train_loss= 0.56580 train_acc= 0.80357 train_auc= 0.90095 val_loss= 0.68189 val_acc= 0.68966 val_auc= 0.75160 time= 0.50091\n",
            "Epoch: 0143 train_loss= 0.56656 train_acc= 0.80995 train_auc= 0.89926 val_loss= 0.68930 val_acc= 0.70115 val_auc= 0.75186 time= 0.49945\n",
            "Epoch: 0143 train_loss= 0.56347 train_acc= 0.80332 train_auc= 0.90333 val_loss= 0.68142 val_acc= 0.67045 val_auc= 0.72963 time= 0.43446\n",
            "Epoch: 0145 train_loss= 0.57990 train_acc= 0.80357 train_auc= 0.88514 val_loss= 0.64404 val_acc= 0.71264 val_auc= 0.79468 time= 0.40603\n",
            "Epoch: 0144 train_loss= 0.56632 train_acc= 0.82653 train_auc= 0.90795 val_loss= 0.67690 val_acc= 0.67816 val_auc= 0.74149 time= 0.42557\n",
            "Epoch: 0144 train_loss= 0.56496 train_acc= 0.82781 train_auc= 0.91192 val_loss= 0.64675 val_acc= 0.73563 val_auc= 0.80691 time= 0.38799\n",
            "Epoch: 0144 train_loss= 0.55968 train_acc= 0.83036 train_auc= 0.91453 val_loss= 0.66114 val_acc= 0.74713 val_auc= 0.79309 time= 0.42262\n",
            "Epoch: 0145 train_loss= 0.57077 train_acc= 0.78444 train_auc= 0.88986 val_loss= 0.72743 val_acc= 0.64368 val_auc= 0.69149 time= 0.40341\n",
            "Epoch: 0145 train_loss= 0.56976 train_acc= 0.82653 train_auc= 0.90259 val_loss= 0.72672 val_acc= 0.63218 val_auc= 0.66649 time= 0.41384\n",
            "Epoch: 0145 train_loss= 0.57087 train_acc= 0.80867 train_auc= 0.89797 val_loss= 0.70189 val_acc= 0.65517 val_auc= 0.72819 time= 0.42784\n",
            "Epoch: 0144 train_loss= 0.55722 train_acc= 0.82653 train_auc= 0.92027 val_loss= 0.68799 val_acc= 0.67816 val_auc= 0.75292 time= 0.43260\n",
            "Epoch: 0144 train_loss= 0.56549 train_acc= 0.81737 train_auc= 0.90641 val_loss= 0.68176 val_acc= 0.64773 val_auc= 0.73067 time= 0.45679\n",
            "Epoch: 0146 train_loss= 0.57290 train_acc= 0.82015 train_auc= 0.89667 val_loss= 0.64533 val_acc= 0.71264 val_auc= 0.79468 time= 0.44205\n",
            "Epoch: 0145 train_loss= 0.56964 train_acc= 0.81888 train_auc= 0.90257 val_loss= 0.67672 val_acc= 0.70115 val_auc= 0.74255 time= 0.42875\n",
            "Epoch: 0144 train_loss= 0.56704 train_acc= 0.81378 train_auc= 0.90320 val_loss= 0.68186 val_acc= 0.68966 val_auc= 0.75479 time= 0.50221\n",
            "Epoch: 0145 train_loss= 0.57689 train_acc= 0.79847 train_auc= 0.88726 val_loss= 0.64674 val_acc= 0.75862 val_auc= 0.80585 time= 0.38814\n",
            "Epoch: 0145 train_loss= 0.56939 train_acc= 0.82015 train_auc= 0.90630 val_loss= 0.65959 val_acc= 0.73563 val_auc= 0.79362 time= 0.42902\n",
            "Epoch: 0146 train_loss= 0.56412 train_acc= 0.82908 train_auc= 0.90575 val_loss= 0.72718 val_acc= 0.63218 val_auc= 0.69043 time= 0.44938\n",
            "Epoch: 0146 train_loss= 0.55764 train_acc= 0.83163 train_auc= 0.91338 val_loss= 0.70265 val_acc= 0.66667 val_auc= 0.72713 time= 0.41386\n",
            "Epoch: 0146 train_loss= 0.55769 train_acc= 0.82908 train_auc= 0.91351 val_loss= 0.72694 val_acc= 0.63218 val_auc= 0.66649 time= 0.45554\n",
            "Epoch: 0145 train_loss= 0.56147 train_acc= 0.82398 train_auc= 0.90834 val_loss= 0.68700 val_acc= 0.66667 val_auc= 0.75769 time= 0.45183\n",
            "Epoch: 0145 train_loss= 0.57453 train_acc= 0.81098 train_auc= 0.89769 val_loss= 0.68263 val_acc= 0.62500 val_auc= 0.72911 time= 0.44183\n",
            "Epoch: 0147 train_loss= 0.56578 train_acc= 0.82398 train_auc= 0.90696 val_loss= 0.64534 val_acc= 0.71264 val_auc= 0.79521 time= 0.45755\n",
            "Epoch: 0146 train_loss= 0.56961 train_acc= 0.80867 train_auc= 0.90071 val_loss= 0.67616 val_acc= 0.70115 val_auc= 0.74468 time= 0.43695\n",
            "Epoch: 0145 train_loss= 0.57249 train_acc= 0.79974 train_auc= 0.89476 val_loss= 0.68154 val_acc= 0.70115 val_auc= 0.75638 time= 0.42781\n",
            "Epoch: 0146 train_loss= 0.57494 train_acc= 0.80230 train_auc= 0.89332 val_loss= 0.64594 val_acc= 0.75862 val_auc= 0.80585 time= 0.42435\n",
            "Epoch: 0146 train_loss= 0.56239 train_acc= 0.83418 train_auc= 0.91118 val_loss= 0.65942 val_acc= 0.74713 val_auc= 0.79309 time= 0.39395\n",
            "Epoch: 0147 train_loss= 0.55916 train_acc= 0.83546 train_auc= 0.92074 val_loss= 0.72669 val_acc= 0.64368 val_auc= 0.68936 time= 0.43764\n",
            "Epoch: 0147 train_loss= 0.55954 train_acc= 0.82526 train_auc= 0.91071 val_loss= 0.72702 val_acc= 0.63218 val_auc= 0.66596 time= 0.40002\n",
            "Epoch: 0147 train_loss= 0.56205 train_acc= 0.82653 train_auc= 0.90836 val_loss= 0.70286 val_acc= 0.65517 val_auc= 0.72713 time= 0.48710\n",
            "Epoch: 0146 train_loss= 0.56501 train_acc= 0.83142 train_auc= 0.90213 val_loss= 0.68356 val_acc= 0.62500 val_auc= 0.72756 time= 0.38072\n",
            "Epoch: 0148 train_loss= 0.56160 train_acc= 0.81633 train_auc= 0.91082 val_loss= 0.64459 val_acc= 0.71264 val_auc= 0.79574 time= 0.38296\n",
            "Epoch: 0146 train_loss= 0.57581 train_acc= 0.79209 train_auc= 0.89139 val_loss= 0.68119 val_acc= 0.71264 val_auc= 0.75638 time= 0.40604\n",
            "Epoch: 0146 train_loss= 0.56555 train_acc= 0.80867 train_auc= 0.90472 val_loss= 0.68655 val_acc= 0.67816 val_auc= 0.75769 time= 0.47874\n",
            "Epoch: 0147 train_loss= 0.56293 train_acc= 0.80740 train_auc= 0.91064 val_loss= 0.67580 val_acc= 0.68966 val_auc= 0.74574 time= 0.42574\n",
            "Epoch: 0147 train_loss= 0.57698 train_acc= 0.78827 train_auc= 0.88934 val_loss= 0.64403 val_acc= 0.74713 val_auc= 0.80745 time= 0.41370\n",
            "Epoch: 0147 train_loss= 0.56730 train_acc= 0.82015 train_auc= 0.90115 val_loss= 0.65896 val_acc= 0.73563 val_auc= 0.79574 time= 0.41318\n",
            "Epoch: 0148 train_loss= 0.56120 train_acc= 0.82143 train_auc= 0.91047 val_loss= 0.72684 val_acc= 0.63218 val_auc= 0.68989 time= 0.43440\n",
            "Epoch: 0148 train_loss= 0.56105 train_acc= 0.82270 train_auc= 0.91169 val_loss= 0.72691 val_acc= 0.63218 val_auc= 0.66490 time= 0.38516\n",
            "Epoch: 0147 train_loss= 0.56694 train_acc= 0.81226 train_auc= 0.90548 val_loss= 0.68431 val_acc= 0.62500 val_auc= 0.72652 time= 0.48020\n",
            "Epoch: 0148 train_loss= 0.56193 train_acc= 0.82781 train_auc= 0.90415 val_loss= 0.70308 val_acc= 0.66667 val_auc= 0.72819 time= 0.52373\n",
            "Epoch: 0149 train_loss= 0.56068 train_acc= 0.82781 train_auc= 0.91308 val_loss= 0.64336 val_acc= 0.71264 val_auc= 0.79521 time= 0.50798\n",
            "Epoch: 0147 train_loss= 0.57834 train_acc= 0.80612 train_auc= 0.88724 val_loss= 0.68127 val_acc= 0.70115 val_auc= 0.75585 time= 0.45874\n",
            "Epoch: 0147 train_loss= 0.57460 train_acc= 0.80740 train_auc= 0.89295 val_loss= 0.68686 val_acc= 0.68966 val_auc= 0.75663 time= 0.48352\n",
            "Epoch: 0148 train_loss= 0.56758 train_acc= 0.80102 train_auc= 0.89302 val_loss= 0.64256 val_acc= 0.74713 val_auc= 0.80638 time= 0.44794\n",
            "Epoch: 0148 train_loss= 0.56582 train_acc= 0.81250 train_auc= 0.89989 val_loss= 0.67572 val_acc= 0.70115 val_auc= 0.74574 time= 0.54592\n",
            "Epoch: 0148 train_loss= 0.55706 train_acc= 0.83163 train_auc= 0.91966 val_loss= 0.65823 val_acc= 0.73563 val_auc= 0.79468 time= 0.46718\n",
            "Epoch: 0149 train_loss= 0.57362 train_acc= 0.82143 train_auc= 0.89936 val_loss= 0.72709 val_acc= 0.66667 val_auc= 0.68936 time= 0.42938\n",
            "Epoch: 0149 train_loss= 0.56009 train_acc= 0.83036 train_auc= 0.90765 val_loss= 0.72676 val_acc= 0.60920 val_auc= 0.66596 time= 0.45075\n",
            "Epoch: 0148 train_loss= 0.56076 train_acc= 0.81737 train_auc= 0.90837 val_loss= 0.68376 val_acc= 0.62500 val_auc= 0.72600 time= 0.41574\n",
            "Epoch: 0148 train_loss= 0.56228 train_acc= 0.83418 train_auc= 0.90855 val_loss= 0.68083 val_acc= 0.68966 val_auc= 0.75691 time= 0.38515\n",
            "Epoch: 0149 train_loss= 0.56151 train_acc= 0.83163 train_auc= 0.91279 val_loss= 0.70202 val_acc= 0.67816 val_auc= 0.73085 time= 0.46279\n",
            "Epoch: 0150 train_loss= 0.55978 train_acc= 0.81633 train_auc= 0.90946 val_loss= 0.64321 val_acc= 0.70115 val_auc= 0.79255 time= 0.47178\n",
            "Optimization Finished!\n",
            "Epoch: 0149 train_loss= 0.56186 train_acc= 0.82398 train_auc= 0.90700 val_loss= 0.64269 val_acc= 0.73563 val_auc= 0.80638 time= 0.41774\n",
            "Epoch: 0148 train_loss= 0.56180 train_acc= 0.82398 train_auc= 0.90653 val_loss= 0.68796 val_acc= 0.67816 val_auc= 0.75133 time= 0.50802\n",
            "Epoch: 0149 train_loss= 0.57207 train_acc= 0.81378 train_auc= 0.89755 val_loss= 0.67505 val_acc= 0.68966 val_auc= 0.74734 time= 0.45714\n",
            "Epoch: 0149 train_loss= 0.56544 train_acc= 0.82143 train_auc= 0.90619 val_loss= 0.65830 val_acc= 0.73563 val_auc= 0.79521 time= 0.46345\n",
            "Test set results: cost= 0.64321 accuracy= 0.70115 auc= 0.79255\n",
            "Epoch: 0150 train_loss= 0.56372 train_acc= 0.83036 train_auc= 0.91778 val_loss= 0.72771 val_acc= 0.66667 val_auc= 0.68989 time= 0.43986\n",
            "Optimization Finished!\n",
            "Epoch: 0150 train_loss= 0.55761 train_acc= 0.83036 train_auc= 0.92046 val_loss= 0.72709 val_acc= 0.62069 val_auc= 0.66808 time= 0.45619\n",
            "Optimization Finished!\n",
            "Epoch: 0149 train_loss= 0.56477 train_acc= 0.83546 train_auc= 0.90950 val_loss= 0.68064 val_acc= 0.68966 val_auc= 0.75798 time= 0.39951\n",
            "Epoch: 0149 train_loss= 0.56448 train_acc= 0.81481 train_auc= 0.90257 val_loss= 0.68264 val_acc= 0.62500 val_auc= 0.72963 time= 0.42341\n",
            "Test set results: cost= 0.72771 accuracy= 0.66667 auc= 0.68989\n",
            "Epoch: 0150 train_loss= 0.57540 train_acc= 0.80612 train_auc= 0.89447 val_loss= 0.64344 val_acc= 0.72414 val_auc= 0.80691 time= 0.39534\n",
            "Optimization Finished!\n",
            "Test set results: cost= 0.72709 accuracy= 0.62069 auc= 0.66808\n",
            "Epoch: 0150 train_loss= 0.56253 train_acc= 0.82270 train_auc= 0.90654 val_loss= 0.70124 val_acc= 0.67816 val_auc= 0.72926 time= 0.46537\n",
            "Optimization Finished!\n",
            "Epoch: 0150 train_loss= 0.56051 train_acc= 0.83929 train_auc= 0.91560 val_loss= 0.67426 val_acc= 0.70115 val_auc= 0.74894 time= 0.37675\n",
            "Optimization Finished!\n",
            "Epoch: 0149 train_loss= 0.56883 train_acc= 0.81250 train_auc= 0.90149 val_loss= 0.68936 val_acc= 0.67816 val_auc= 0.74920 time= 0.41454\n",
            "Epoch: 0150 train_loss= 0.56395 train_acc= 0.81760 train_auc= 0.91215 val_loss= 0.65848 val_acc= 0.73563 val_auc= 0.79681 time= 0.39258\n",
            "Optimization Finished!\n",
            "0.70114946\n",
            "Test set results: cost= 0.64344 accuracy= 0.72414 auc= 0.80691\n",
            "Test set results: cost= 0.70124 accuracy= 0.67816 auc= 0.72926\n",
            "Test set results: cost= 0.67426 accuracy= 0.70115 auc= 0.74894\n",
            "Test set results: cost= 0.65848 accuracy= 0.73563 auc= 0.79681\n",
            "Epoch: 0150 train_loss= 0.55485Epoch: 0150 train_loss= 0.56235 train_acc= 0.81888 train_auc= 0.91670 val_loss= 0.68132  val_acc= 0.68966 val_auc= 0.75691 time= 0.38588\n",
            "train_acc= Optimization Finished!\n",
            "0.85185 train_auc= 0.92124 val_loss= 0.68197 val_acc= 0.68182 val_auc= 0.73015 time= 0.37276\n",
            "Optimization Finished!\n",
            "0.6666666\n",
            "Test set results: cost= 0.68197 accuracy= 0.68182 auc= 0.73015\n",
            "Test set results: cost= 0.68132 accuracy= 0.68966 auc= 0.75691\n",
            "0.6206897\n",
            "Epoch: 0150 train_loss= 0.56114 train_acc= 0.83418 train_auc= 0.91677 val_loss= 0.69066 val_acc= 0.70115 val_auc= 0.74655 time= 0.35476\n",
            "Optimization Finished!\n",
            "0.7241379\n",
            "0.67816097\n",
            "0.73563224\n",
            "Test set results: cost= 0.69066 accuracy= 0.70115 auc= 0.74655\n",
            "0.70114946\n",
            "0.6818182\n",
            "0.68965524\n",
            "0.70114946\n",
            "[(58, 0.6898936170212766, 52, 0.5925531914893616, 87), (61, 0.7489361702127659, 59, 0.7074468085106381, 87), (59, 0.7292553191489362, 56, 0.6611702127659574, 87), (61, 0.7925531914893617, 60, 0.7393617021276595, 87), (60, 0.7569148936170214, 58, 0.7074468085106382, 87), (64, 0.7968085106382979, 59, 0.6781914893617021, 87), (63, 0.8069148936170213, 69, 0.8861702127659574, 87), (61, 0.7465535524920467, 55, 0.689289501590668, 87), (54, 0.6680805938494168, 51, 0.6129374337221632, 87), (60, 0.7301504929942917, 57, 0.69901401141671, 88)]\n",
            "overall linear accuracy %f0.661308840413318\n",
            "overall linear AUC %f0.6973581372261457\n",
            "overall accuracy %f0.6900114810562572\n",
            "overall AUC %f0.7466061235080436\n",
            "Saving results to: /Users/camillekoczo/Desktop/MVA/Semestre_1/GEOMETRIC DATA ANALYSIS/population-gcn/results/ABIDE_classification_run1.mat\n"
          ]
        }
      ],
      "source": [
        "!python main_ABIDE.py --folds 11 --epochs 150 --suffix run1"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyOX919wfqXxXfjqVb0Fuy7V",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "gcn-env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.20"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
